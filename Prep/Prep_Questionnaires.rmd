---
title: "Prep_Questionnaires"
author: "Pauliina Vuorinen"
date: "28/06/2022-"
output: html_document
library: "~/Extra/RPackages.bib"
---

## Introduction

The purpose of this script is to prepare questionnaire data for use in the analysis.

Three different questionnaires are processed: (1) Intrinsic Motivation Inventory (IMI), (2) IMI-R: task specific Intrinsic Motivation Inventory, and (3) Questionnaire on electronic reading experience. Our intention is to use these questionnaires to create 6 different variables to use in the analysis:

From the IMI-R (1):
* A measure of contextual reading motivation
* Consists of the following subcomponents:
    * 'Interest': interest in reading as an activity, e.g. *I think recreational reading is enjoyable*
    * 'Competence': Sense of competence in one's own reading ability, e.g. *"I think I am good at reading"*

From the IMI (2):
* A measure of situational reading motivation
* Consists of the following subcomponents:
    * 'Interest': participants' interest in the short story that they read, e.g. **
    * 'Competence': Perceived competence in reading and difficulty of the read short story, e.g. **
    * 'Autonomy': Perceived autonomy in text selection and the ways in which the short story was read, e.g. **

From the questionnaire on electronic reading experience (3):
* A measure of experience using task-specific devices (smartphones, tablets, laptops, PCs, Macs) for any electronic reading
* A measure of experience reading task-specific texts (long-form fiction) on any electronic devices

## Setup

Load libraries, set working directory

```{r 'setup', results='hide', warning=FALSE}
# basic set of packages
library(tidyverse)
library(tidyr)
library(dplyr)
library(knitr)
# packages for a wordcloud
library(tm)
library(wordcloud2)
# packages for correlations, cronbach's alpha etc
library(EFA.dimensions)
library(corrplot)
library(ltm)
```

```{r 'working directory', results='hide', warning=FALSE}
setwd("C:/Users/vuori/Documents/GITHUB/Short_Story_Reading_Behaviour_Public") # temporary
getwd() # check that working directory is ~/Short_Story_Reading_Behaviour_Public
# to knit the rmarkdown,
## make sure your knitr settings working directory is the same as above
# a knitted file: Prep_Questionnaires.html in Prep
```

Save working directory so that this script can be used elsewhere, if required. The working directory should be "~/Short-Story-Reading-Behaviour-Public/". If the working directory is not correct, we save the correct path and use that in loading files. In our purposes, the foulder could be found from /Documents/GITHUB/Short_Story_Reading_Behaviour_Public.

The working directory is not changed with setwd() because this script is knit remotely in other scripts.

```{r 'working directory for my purposes'}
mypath_SSRBP <- getwd()
if (!grepl("Short_Story_Reading_Behaviour_Public", mypath_SSRBP, fixed = TRUE)) {
    # wrong working directory
    if (!grepl("GITHUB", dirname(mypath_SSRBP), fixed = TRUE)) {
        # directory name isn't GITHUB, unlike I would expect
        if (grepl("GITHUB", mypath_SSRBP, fixed = TRUE)) {
            # GITHUB is in the path
            ## use mypath_SSRBP instead of dirname()
            mypath_SSRBP <- paste0(
                mypath_SSRBP,
                "/Short_Story_Reading_Behaviour_Public"
            )
        }
    } else {
        # directory name is GITHUB
        # save correct working directory
        mypath_SSRBP <- paste0(
            dirname(mypath_SSRBP),
            "/Short_Story_Reading_Behaviour_Public"
        )
    }
}
```

## Load data

```{r 'load data'}
raw_questionnaire_data <- read.csv(
    paste0(
        mypath_SSRBP,
        "/Data/raw_questionnaire_data.csv"
    ),
    header = TRUE,
    sep = ";",
    dec = ","
)
```

```{r 'change variable types'}
str(raw_questionnaire_data)
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_VariableTypeConversion.R"
    )
)

## turn columns into factors that should be factors
names(raw_questionnaire_data)[names(raw_questionnaire_data) == "BookId"] <- "StoryId"
raw_questionnaire_data[, c(
    "UserId",
    "StoryId"
)] <- convert.magic(
    raw_questionnaire_data[, c(
        "UserId",
        "StoryId"
    )],
    "factor"
)
```

The questionnaire data has answers from participants who didn't complete the data in full, and these participants are excluded:

```{r 'remove excluded'}
raw_questionnaire_data <-
    filter(
        raw_questionnaire_data,
        UserId != "33"
    )
raw_questionnaire_data <-
    filter(
        raw_questionnaire_data,
        UserId != "61"
    )
raw_questionnaire_data <-
    filter(
        raw_questionnaire_data,
        UserId != "81"
    )
```

## Create datasets from the questions and answers

To efficiently access the questionnaire data, we create two separate datasets:
* all_answer_data
    * Contains only the following columns: UserId, Questionnaire (Ids to identify the Questionnaires), QuestionNumber, and Answer
    * Used to study the answers and group them together
    * In this dataset the answers are organised by UserId under columns of which headings show QuestionnaireId_QuestionNumber
* all_questions_data
    * Used to check which question matches the 'Questionnaire' Id and 'QuestionNumber'
    * The column headings show QuestionnaireId_QuestionNumber and the values show the corresponding 'Question'

```{r 'create answer and question datasets'}
all_answer_data <- raw_questionnaire_data %>%
    group_by(UserId) %>%
    dplyr::select(-Question, -StoryId) %>%
    pivot_wider(
        names_from = c(Questionnaire, QuestionNumber),
        values_from = Answer
    )
all_questions_data <- raw_questionnaire_data %>%
    dplyr::select(-UserId, -StoryId, -Answer) %>%
    distinct() %>%
    pivot_wider(
        names_from = c(Questionnaire, QuestionNumber),
        values_from = Question
    )
```

## Participant demographics

We check participants' demographic information, including their gender, age, and whether English is their native language or not. First, we create a new dataset with only these columns:

```{r 'create a dataset on demographics'}
demographics_data <-
    all_answer_data[, c(1, 22:24)]
names(demographics_data)[2:4] <- c("Age", "Gender", "NativeEnglish")
str(demographics_data)
demographics_data$Age <-
    as.numeric(
        demographics_data$Age
    )
```

Second, we study demographics of the sample:
- The sample includes `r sum(demographics_data$Gender == "Female")` women, `r sum(demographics_data$Gender == "Male")`  men, and `r sum(demographics_data$Gender == "Prefer not to say")` participant who preferred not to disclose their gender.
- The participants' average age is `r round(mean(demographics_data$Age), 2)` (*SD* = `r round(sd(demographics_data$Age), 2)`), and age range is `r min(demographics_data$Age)`-`r max(demographics_data$Age)`
- `r sum(demographics_data$NativeEnglish == "Yes")` of the `r nrow(demographics_data)` participants (`r round((sum(demographics_data$NativeEnglish == "Yes")/nrow(demographics_data)*100), 2)`%) are native speakers of English, whereas the remaining `r sum(demographics_data$NativeEnglish == "No")` participants are not.

```{r 'demographics', include=FALSE}
# Gender
table(demographics_data$Gender)
# Age
mean(demographics_data$Age)
sd(demographics_data$Age)
range(demographics_data$Age)
# Native language
table(demographics_data$NativeEnglish)
```

We save a dataset on demographics for usage in the analyses:

```{r 'save demographics dataset', eval=FALSE, echo=TRUE}
# write.csv2(
#    demographics_data,
#    "demographics_data.csv"
# )
```

## Revised Intrinsic Motivation Inventory (IMI-R)

IMI-R is used to measure participants' contextual reading motivation.
The questionnaire is an adapted version of the situational motivation questionnaire, IMI (included below IMI-R).
The questionnaire consists of the following subcomponents:
* 'Interest': interest in reading as an activity, e.g. *I think recreational reading is enjoyable*
* 'Competence': Sense of competence in one's own reading ability, e.g. *"I think I am good at reading"*
* 'Effort': Perceived effortufulness of reading, e.g. *"I put energy into recreational reading"*

The items are expected to load on the subcomponents as follows:

- Interest:
    - questionnaire11_1: I think that recreational reading is boring
    - questionnaire11_2: I think recreational reading is enjoyable
    - questionnaire11_4: I like recreational reading
    - questionnaire11_7: I read recreationally for the fun of it
    - questionnaire11_9: While I am reading recreationally, I think about how much I enjoy it
    - questionnaire11_12: If I could choose what to do right now, I would read recreationally
    - questionnaire11_14: I would describe recreational reading as interesting
    - questionnaire11_17: Overall, I enjoy recreational reading
    - questionnaire11_19: Recreational reading is fun to do

- Competence:
    - questionnaire11_3: I am skilled at reading
    - questionnaire11_6: After reading for a while, I feel skilled
    - questionnaire11_8: I think I am good at reading
    - questionnaire11_10: I am satisfied with how well I can read
    - questionnaire11_18: Reading is an activity that I can do well
    - questionnaire11_20: I think I read well, in comparison to others

In addition to these two components, the version of the questionnaire that we included in the study also had items on 'effort' which can tells us of how effortful an individual thinks reading is e.g. *"I put energy into recreational reading"*. These items were disregarded from the analyses due to two reasons: (1) we had not determined hypotheses concerning the 'effort' subcomponent and thus it was not considered to be an important variable, and (2) feedback from participants indicated that the items in the 'effort' subcomponent were ambiguous. Indeed, putting a lot of effort in reading a text can indicate of positive investment in the activity, or a negative burden of completing the short story. Due to these reasons, the 'effort' score was not used in analyses.

The subcomponents of contextual 'interest' and 'competence' were used in analyses. To do so, we check the internal consistency of the scale and correlations between the items. Item scores within each of the subcomponents are then summed together for use in the analysis.

First, we create a dataset with only the responses to IMI-R:

```{r 'create dataset for IMI-R'}
# select correct columns from all_answer_data
## UserId, and items in the 'interest' and 'competence' subcomponents of IMIR
IMIR_data <- all_answer_data[, c(
    1:5,
    7:11,
    13,
    15,
    18:21
)]
```

Missing responses need to be removed, and so we check if IMI-R has any missing responses.

```{r 'remove missing values IMI-R'}
any(is.na(IMIR_data))
```

IMI-R was admistered at the beginning of the study, and so there are no missing responses.
*n* = `r nrow(IMIR_data)`

Next, we remove special characters from the answers:

```{r ' remove special characters IMI-R'}
# remove special characters
IMIR_data <- as.data.frame(IMIR_data)
for (item in 2:length(IMIR_data)) {
    IMIR_data[, item] <- substr(
        IMIR_data[, item],
        1,
        1
    )
}
IMIR_data <- IMIR_data %>%
    mutate_if(is.character, as.numeric)
```

Only one IMI-R item is reverse coded. questionnaire11_1: "I think that recreational reading is boring" (interest). The score of this item is reversed:

```{r 'reverse one IMI-R question'}
IMIR_data[, "questionnaire11_1"] <- (8 - IMIR_data[, "questionnaire11_1"])
```

### IMI-R scale tests and adjustments

We then check correlations between the items.

IMI-R is measured on a 7-point Likert scale, and so the scores are ordinal. Therefore, we use polychor correlations rather than Pearson's correlations.

```{r 'correlations IMI-R', include=FALSE}
IMIR_item_polychor_correlations <- POLYCHORIC_R(
    IMIR_data[, c(2:length(IMIR_data))],
    verbose = FALSE
)
round(IMIR_item_polychor_correlations, 2)
```

Visualise correlations:

```{r 'correlation plot IMI-R', include=FALSE}
# correlationsIMIR <- corrplot(round(IMIR_item_polychor_correlations, 2), method = "circle", type = "lower")
```

Most of the items seem to be highly correlated with each other.

Next, we check the internal consistency of the IMIR overall scale and each of the subscales (interest, competence, and effort) by computing Cronbach's Alpha [XX Cronbach, 1951].

```{r 'overall scale Cronbachs alpha IMI-R'}
IMIRFullScaleCronbach <-
    cronbach.alpha(
        IMIR_data[, 2:length(IMIR_data)],
        CI = TRUE
    )
```

The full scale Cronbach's Alpha is = `r round(IMIRFullScaleCronbach$alpha, 3)` with a confidence interval of `r round(IMIRFullScaleCronbach$ci, 2)`. Similarly to IMI, IMI-R fullscale has 'good' internal consistency [XX].

Next, we check Cronbach's alpha of each subscale:

```{r 'subscale Cronbachs alpha IMI-R'}
# Interest
IMIR_interest_subscale <- dplyr::select(
    IMIR_data,
    questionnaire11_1,
    questionnaire11_2,
    questionnaire11_4,
    questionnaire11_7,
    questionnaire11_9,
    questionnaire11_12,
    questionnaire11_14,
    questionnaire11_17,
    questionnaire11_19
)
IMIRInterestCronbach <-
    cronbach.alpha(
        IMIR_interest_subscale,
        CI = TRUE
    )
# Competence
IMIR_competence_subscale <- dplyr::select(
    IMIR_data,
    questionnaire11_3,
    questionnaire11_6,
    questionnaire11_8,
    questionnaire11_10,
    questionnaire11_18,
    questionnaire11_20
)
IMIRCompetenceCronbach <-
    cronbach.alpha(
        IMIR_competence_subscale,
        CI = TRUE
    )
```

- The subscale of 'interest' has 'good' internal consistency, Cronbach's Alpha = `r round(IMIRInterestCronbach$alpha, 3)` (Confidence interval = `r round(IMIRInterestCronbach$ci, 2)`).
- The subscale of 'competence' has 'acceptable' internal consistency, Cronbach's Alpha = `r round(IMIRCompetenceCronbach$alpha, 3)` (Confidence interval = `r round(IMIRCompetenceCronbach$ci, 2)`).

Both subscales show sufficiently high internal consistency.

### IMI-R scores

Next, we calculate each participants' score on both of the IMI-R subscales by summing item scores together.

We create each participant a score on both of the subscales by summing item scores together. The IMI-R scores are named as 'CInterest' and 'CCompetence' (C: contextual motivation) to distinguish them from the IMI subscale scores.

```{r 'IMI-R scores', include=FALSE}
IMIR_scores <- IMIR_data %>%
    mutate(
        CInterest = rowSums(.[names(IMIR_interest_subscale)]),
        CCompetence = rowSums(.[names(IMIR_competence_subscale)])
    ) %>%
    dplyr::select(UserId, CInterest, CCompetence)
```

Inspect participants' IMI-R scores:

```{r 'IMI-R interest', include=FALSE}
# interest subscale
## max score on the scale: 9*7 = 63, min score 9*1 = 9
mean(IMIR_scores$CInterest)
sd(IMIR_scores$CInterest)
range(IMIR_scores$CInterest)
# visualise
CInterest <- ggplot(IMIR_scores, aes(x = UserId, y = CInterest)) +
    geom_point() +
    theme_classic()
```

The average contextual interest score is `r round(mean(IMIR_scores$CInterest), 2)` (*SD* = `r round(sd(IMIR_scores$CInterest), 2)`, *range* = `r round(range(IMIR_scores$CInterest), 2)`). Therefore, most of the participants were contextually motivated to read for fun. This makes sense as the sample was collected by convenience sampling, and individuals who enjoy reading as an activity are more likely to take part in a book reading study than people who are not motivated to read.

The mean score divided by the number of items, `r round((mean(IMIR_scores$CInterest)/9), 2)` corresponds with the third highest option on the Likert-scale from 1 (*not at all true*) to 7 (*very true*).

```{r 'IMI-R competence', include=FALSE}
# competence subscale
## max score on the scale: 6*7 = 42, min score 6*1 = 6
mean(IMIR_scores$CCompetence)
sd(IMIR_scores$CCompetence)
range(IMIR_scores$CCompetence)
# visualise
CCompetence <- ggplot(IMIR_scores, aes(x = UserId, y = CCompetence)) +
    geom_point() +
    theme_classic()
```

The average contextual competence score is `r round(mean(IMIR_scores$CCompetence), 2)` (*SD* = `r round(sd(IMIR_scores$CCompetence), 2)`, *range* = `r round(range(IMIR_scores$CCompetence), 2)`). The plot shows that participants vary more in their perceived reading competence than in their reading enjoyment. The average contextual competence score divided by the amount of items is `r round((mean(IMIR_scores$CCompetence)/6), 2)`, which indicates slightly positive sense of competence (scale: 1 (*not at all true*) to 7 (*very true*)).

We then check correlations between scores:

```{r 'IMI-R score correlations', include=FALSE}
IMIR_scores_correlations <- cor(
    IMIR_scores[, 2:length(IMIR_scores)]
)
# corrplotScoresIMIR <- corrplot(IMIR_scores_correlations, method = "number")
CInteresCCompetence <- ggplot(IMIR_scores, aes(x = CInterest, y = CCompetence)) +
    geom_point() +
    geom_smooth() +
    theme_classic() +
    ggtitle("IMI-R: contextual interest and competence scores")
```

The IMI-R subscales are positively correlated with each other. This indicates that participants who generally enjoy reading, also perceive themselves as competent in reading.

Finally, we save the IMI-R scores dataset for use in analyses. This dataset has already been saved and so the below code is not run.

```{r 'save IMI-R scores data', eval=FALSE, echo=TRUE}
write.csv2(
    IMIR_scores,
    "IMIR_scores.csv"
)
```

## Intrinsic Motivation Inventory (IMI)

IMI is used in the study to measure participants' situational reading motivation.
The questionnaire consists of multiple different components:
* Interest: participants' interest in the short story and their enjoyment of the task
* Competence: participants' perceived difficulty reading the short story
* Autonomy: participants' perceived autonomy in reading the short story

Similarly to IMI-R, we create scores for the subcomponents by summing the scores together. We also inspect cronbach's alpha to assess the internal consistency of IMI in our sample.

In addition to these three subcomponents, the version of the IMI that we used in the study had questions on 'effort' and 'pressure' subcomponents as well. Whereas effort indicates how effortful participants found reading of the short story, pressure indicates whether participants experienced feelings of pressure to read to story. These two components were not included in analyses because of three reasons: (1) we had not determined hypotheses concerning the subcomponents and thus they were not considered to be important variables, (2) feedback from participants indicated that the items in the 'effort' subcomponent were ambiguous. Indeed, putting a lot of effort in reading a text can indicate of positive investment in the activity, or a negative burden of completing the short story. (3) The subcomponents showed poor psychometric properties: the 'pressure' component had poor internal consistency (Cronbach's alpha < .5), whereas the 'effort' component was not correlated with the other elements in the questionnaire. Due to these reasons, the 'effort' and 'pressure' scores were not used in analyses,

First, we create a dataset with only the responses to IMI and tidy the answers to usable format:
```{r 'create dataset for IMI'}
# select correct columns from all_answer_data
IMI_data <- all_answer_data[, c(
    1,
    38:41,
    44:46,
    48:53,
    56:58,
    60,
    62:64
)]

# remove special characters from answers
IMI_data <- as.data.frame(IMI_data)
for (item in 2:length(IMI_data)) {
    IMI_data[, item] <- substr(
        IMI_data[, item],
        1,
        1
    )
}
IMI_data <- IMI_data %>%
    mutate_if(is.character, as.numeric)
```

Missing responses need to be removed, and so we check if IMI-R has any missing responses.

```{r 'remove missing values', include=FALSE}
any(is.na(IMI_data))
```

IMI has no missing responses, and so *n* remains `r nrow(IMI_data)`

Some of the IMI items are reverse coded. The scores of these items are reversed:

```{r 'reverse IMI questions'}
# reversed questions
ReversedItemsIMI <- c(
    "questionnaire22_9",
    "questionnaire22_10",
    "questionnaire22_11",
    "questionnaire22_16",
    "questionnaire22_20",
    "questionnaire22_26",
    "questionnaire22_27",
    "questionnaire22_30"
)
for (item in 2:length(IMI_data)) {
    if (any((colnames(IMI_data)[item]) == ReversedItemsIMI)) {
        IMI_data[, item] <- 8 - IMI_data[, item]
    }
}
```

### IMI scale tests and adjustments

We then check correlations between the items.

Similarly to IMI-R, IMI is measured on a 7-point Likert scale, and so the scores are ordinal. Therefore, we use polychor correlations rather than Pearson's correlations.

```{r, include=FALSE}
# there is an issue with polychoric correlations
## removing either column 23 and 25 fixes the issue??
### cor for now
IMI_item_polychor_correlations <- POLYCHORIC_R(
    IMI_data[, c(2:length(IMI_data))],
    verbose = FALSE
)
round(IMI_item_polychor_correlations, 2)
```

Visualise correlations:

```{r, include=FALSE}
# corrplotIMI <- corrplot(round(IMI_item_polychor_correlations, 2), method = "circle", type = "lower")
```

Next, we check the internal consistency of the IMI overall scale and each of the subscales (interest, competence, effort, pressure and autonomy) by computing Cronbach's Alpha [XX Cronbach, 1951].

```{r 'overall scale Cronbachs alpha IMI'}
IMIFullScaleCronbach <-
    cronbach.alpha(
        IMI_data[, 2:length(IMI_data)],
        CI = TRUE
    )
```

The full scale Cronbach's Alpha is = `r round(IMIFullScaleCronbach$alpha, 3)` with a confidence interval of `r round(IMIFullScaleCronbach$ci, 2)`. The fullscale therefore has 'excellent' internal consistency [XX].

Next, we check Cronbach's alpha of each subscale:

```{r 'subscale Cronbachs alpha IMI'}
# Interest
IMI_interest_subscale <- dplyr::select(
    IMI_data,
    questionnaire22_8,
    questionnaire22_11,
    questionnaire22_14,
    questionnaire22_18,
    questionnaire22_22,
    questionnaire22_26,
    questionnaire22_28
)
IMIInterestCronbach <-
    cronbach.alpha(
        IMI_interest_subscale,
        CI = TRUE
    )
# Competence
IMI_competence_subscale <- dplyr::select(
    IMI_data,
    questionnaire22_9,
    questionnaire22_15,
    questionnaire22_19,
    questionnaire22_23,
    questionnaire22_33,
    questionnaire22_34
)
IMICompetenceCronbach <-
    cronbach.alpha(
        IMI_competence_subscale,
        CI = TRUE
    )
# Autonomy
IMI_autonomy_subscale <- dplyr::select(
    IMI_data,
    questionnaire22_10,
    questionnaire22_16,
    questionnaire22_20,
    questionnaire22_21,
    questionnaire22_27,
    questionnaire22_30,
    questionnaire22_32
)
IMIAutonomyCronbach <-
    cronbach.alpha(
        IMI_autonomy_subscale,
        CI = TRUE
    )
```

- The subscale of 'interest' has 'excellent' internal consistency, Cronbach's Alpha = `r round(IMIInterestCronbach$alpha, 3)` (Confidence interval = `r round(IMIInterestCronbach$ci, 2)`).
- The subscale of 'competence' has 'good' internal consistency, Cronbach's Alpha = `r round(IMICompetenceCronbach$alpha, 3)` (Confidence interval = `r round(IMICompetenceCronbach$ci, 2)`).
- The subscale of 'autonomy' has 'good' internal consistency (Cronbach's Alpha = `r round(IMIAutonomyCronbach$alpha, 3)`, Confidence interval = `r round(IMIAutonomyCronbach$ci, 2)`)

All subcomponents show sufficient internal consistency.

### IMI scores

Next, we calculate each participants' score on each of the IMI subscales by summing item scores together.

We create each participant a score on each of the subscales by summing item scores together. The IMI scores are named as 'SInterest', 'SCompetence', and 'SAutonomy' (S: situational motivation) to distinguish them from the IMI-R subscale scores.

```{r 'IMI scores', include=FALSE}
IMI_scores <- IMI_data %>%
    mutate(
        SInterest = rowSums(.[names(IMI_interest_subscale)]),
        SCompetence = rowSums(.[names(IMI_competence_subscale)]),
        SAutonomy = rowSums(.[names(IMI_autonomy_subscale)])
    ) %>%
    dplyr::select(UserId, SInterest, SCompetence, SAutonomy)
```

Inspect participants' IMI scores:

```{r 'Interest IMI scores', include=FALSE}
# interest subscale
## max score on the scale: 7*7 = 49, min score 7*1 = 7
mean(IMI_scores$SInterest)
sd(IMI_scores$SInterest)
range(IMI_scores$SInterest)
# visualise
SInterest <- ggplot(IMI_scores, aes(x = UserId, y = SInterest)) +
    geom_point() +
    theme_classic()
```

The average situational interest score is `r round(mean(IMI_scores$SInterest), 2)` (*SD* = `r round(sd(IMI_scores$SInterest), 2)`, *range* = `r round(range(IMI_scores$SInterest), 2)`). Therefore, most of the participants were situationally motivated to read the short story.

The mean score divided by the number of questions, `r round((mean(IMI_scores$SInterest)/7), 2)` corresponds with the third highest option on the Likert-scale from 1 (*not at all true*) to 7 (*very true*).

```{r 'IMI competence score', include=FALSE}
# competence subscale
## max score on the scale: 6*7 = 42, min score 6*1 = 6
mean(IMI_scores$SCompetence)
sd(IMI_scores$SCompetence)
range(IMI_scores$SCompetence)
# visualise
SCompetence <- ggplot(IMI_scores, aes(x = UserId, y = SCompetence)) +
    geom_point() +
    theme_classic()
```

The average situational competence score is `r round(mean(IMI_scores$SCompetence), 2)` (*SD* = `r round(sd(IMI_scores$SCompetence), 2)`, *range* = `r round(range(IMI_scores$SCompetence), 2)`). The plot shows that participants varied more in how difficult they found the reading task than in their enjoyment of the short story. The average situational competence score divided by the amount of questions is `r round((mean(IMI_scores$SCompetence)/6), 2)`, which corresponds with the third highest option in the Likert scale (scale: 1 (*not at all true*) to 7 (*very true*)).

```{r, include=FALSE}
# autonomy subscale
## max score on the scale: 7*7 = 49, min score 7*1 = 7
mean(IMI_scores$SAutonomy)
sd(IMI_scores$SAutonomy)
range(IMI_scores$SAutonomy)
# visualise
SAutonomy <- ggplot(IMI_scores, aes(x = UserId, y = SAutonomy)) +
    geom_point() +
    theme_classic()
```

The average situational autonomy score is `r round(mean(IMI_scores$SAutonomy), 2)` (*SD* = `r round(sd(IMI_scores$SAutonomy), 2)`, *range* = `r round(range(IMI_scores$SAutonomy), 2)`). Most of the participants reported that they felt like they had autonomy in text selection and how they read the short story. The average situational autonomy score divided by the amount of items is `r round((mean(IMI_scores$SAutonomy)/7), 2)`, which corresponds with a slightly positive answer (scale: 1 (*not at all true*) to 7 (*very true*)).

We then check correlations between scores:

```{r 'IMI correlations between scores', include=FALSE}
IMI_scores_correlations <- cor(
    IMI_scores[, 2:length(IMI_scores)]
)
# corrplot(IMI_scores_correlations, method = "number")
ggplot(IMI_scores, aes(x = SInterest, y = SCompetence)) +
    geom_point() +
    geom_smooth() +
    theme_classic() +
    ggtitle("IMI: situational interest and competence scores")
ggplot(IMI_scores, aes(x = SInterest, y = SAutonomy)) +
    geom_point() +
    geom_smooth() +
    theme_classic() +
    ggtitle("IMI: situational interest and effort scores")
ggplot(IMI_scores, aes(x = SCompetence, y = SAutonomy)) +
    geom_point() +
    geom_smooth() +
    theme_classic() +
    ggtitle("IMI: situational competence and autonomy scores")
```

The 'interest', 'autonomy', and 'competence' IMI subscales are positively correlated with each other. This indicates that participants who enjoyed reading the short story, also found the story easier to read, and they felt autonomous in text selection and how they read the story.

Finally, we save the IMI scores dataset for use in analyses. This dataset has already been saved and so the below code is not run.

```{r, eval=FALSE, echo=TRUE}
write.csv2(
    IMI_scores,
    "IMI_scores.csv"
)
```

## Questionnaire on Electronic Reading Experience

Questionnaire on Electronic Reading Experience was developed for the purposes of our studies. The questionnaire includes 23 items that are divided in four different types of questions:

(1) The first item asks participants to report which digital devices they own. Participants are asked to select all options that apply.
(2) The following 7 items are used to determine participants' frequency of reading different text types electronically (e.g. *"How often do you read fiction books electronically?"*). These items are responded on a 5-point Likert scale from 'Never' to 'Every day'.
(3) The following 7 items (questionnaire3_9 to questionnaire3_15) ask participants to report their frequency of using different reading formats for recreational reading (e.g. *"How often do you use a tablet computer (such as an iPad) for reading recreationally?"*). Similarly to previous, these items are responded on a 5-point Likert scale from 'Never' to 'Every day'.
(4) The final 7 items ask participants to report how frequently they use different reading formats as part of work or study (e.g. *"How often do you use a laptop for reading as part of work or study?"*). These items are again measured on a 5-point Likert scale from 'Never' to 'Every day'.

Our intention was to compute measures of electronic experience for recreational reading of fictional texts, and therefore, we are interested in the frequency of reading long-form fiction electronically and the frequency of using digital devices for recreational reading that could be used to access the e-reader during the study. Indeed, our aim is to measure **task-relevant electronic reading experience** - experience of reading similar electronic texts (long-form fiction) that were available in the study, on devices that could have been used to access the e-reader.

To create these measures, we combine participants responses in the following way:

* To measure frequency of reading long-form fiction electronically, we sum together responses to items questionnaire3_4 (*"How often do you read fiction books electronically?"*) and questionnaire3_7 (*"How often do you read short stories or fanfiction electronically?"*)
* To measure frequency of using digital devices for recreational reading (only devices that could be used in the study to access the e-reader), we sum together responses to items questionnaire3_10 (*"How often do you use a desktop computer for reading recreationally?"*), questionnaire3_11 (*"How often do you use a laptop for reading recreationally?"*), questionnaire3_12 (*"How often do you use a smartphone for reading recreationally?"*), and questionnaire3_13 (*"How often do you use a tablet computer (such as an iPad) for reading recreationally?"*).

To do this, we first create a dataset with only responses to the electronic reading experience items (which is shortened as 'eexp'):

```{r 'create dataset for eexp'}
# select correct columns from all_answer_data
eexp_data <- all_answer_data[, c(1, 67:88)] # UserId, questionnaire3_1:questionnaire3_22
```

We then check if the eexp_data includes any missing values:

```{r 'any missing values eexp', include=FALSE}
any(is.na(eexp_data))
```

There are no missing values

Next, we code the answers into numeric values:

```{r 'code responses as numeric eexp'}
eexp_data <- as.data.frame(eexp_data)
nameslist <- names(eexp_data[, 3:length(eexp_data)])
for (k in nameslist) {
    eexp_data[, k] <- recode(eexp_data[, k],
        "Every day" = 5,
        "A few times a week" = 4,
        "A few times a month" = 3,
        "A few times a year" = 2,
        "Never" = 1
    )
}
```


We look into which digital devices were owned by participants:

```{r 'code and inspect access to digital devices', include=FALSE}
eexp_data$HasEInkEReader <- grepl(
    "Dedicated e-reader",
    eexp_data$questionnaire3_1
)
eexp_data$HasDesktopComputer <- grepl(
    "Desktop computer",
    eexp_data$questionnaire3_1
)
eexp_data$HasLaptop <- grepl(
    "Laptop",
    eexp_data$questionnaire3_1
)
eexp_data$HasSmartphone <- grepl(
    "Smartphone",
    eexp_data$questionnaire3_1
)
eexp_data$HasTabletComputer <- grepl(
    "Tablet computer",
    eexp_data$questionnaire3_1
)
eexp_data$HasOtherDevice <- grepl(
    "Other device",
    eexp_data$questionnaire3_1
)
eexp_data <- eexp_data %>%
    mutate(
        NumberOfDevices = rowSums(.[24:29])
    )
# inspect accessible devices:
table(eexp_data$HasEInkEReader)
table(eexp_data$HasDesktopComputer)
table(eexp_data$HasLaptop)
table(eexp_data$HasSmartphone)
table(eexp_data$HasTabletComputer)
table(eexp_data$HasOtherDevice)
table(eexp_data$NumberOfDevices)
eexp_data$NumberOfDevices <- as.factor(eexp_data$NumberOfDevices)
Devices <- ggplot(eexp_data) +
    geom_bar(aes(x = NumberOfDevices)) +
    theme_classic()
```

The bar graph shows that most of the participants have 2 different digital devices, followed by 3 and then 1 device(s). Most frequently, participants own laptops (`r round((sum(eexp_data$HasLaptop)/nrow(eexp_data))*100, 2)`%) and smartphones (`r round((sum(eexp_data$HasSmartphone)/nrow(eexp_data))*100, 2)`%). `r round((sum(eexp_data$HasTabletComputer)/nrow(eexp_data))*100, 2)`% of the participants have tablet computers, and `r round((sum(eexp_data$HasDesktopComputer)/nrow(eexp_data))*100, 2)`% of the participants own a desktop computer.

`r round((sum(eexp_data$HasEInkEReader)/nrow(eexp_data))*100, 2)`% of the participants own a dedicated e-ink e-reader (`r (sum(eexp_data$HasEInkEReader)` participants). 

Next, we compute the two electronic reading experience measures

(1) Frequency of reading long-form fictional texts electronically, and
(2) Frequency of using task-relevant digital devices for recreational reading purposes.

```{r 'compute the eexp measures'}
eexp_data <- eexp_data %>%
    mutate(
        Eexp1LongformEFreq = (questionnaire3_4
        + questionnaire3_7),
        Eexp2DeviceRecFreq = (questionnaire3_10
        + questionnaire3_11
            + questionnaire3_12
            + questionnaire3_13)
    )
```

Inspect participants' variation in the eexp measures. First we look at the frequence of reading long-form fictional texts electronically:

```{r 'visualise eexp measure 1', include=FALSE}
# frequency of reading fictional, long-form texts electronically
# possible score range: min - 2, max - 10
mean(eexp_data$Eexp1LongformEFreq)
sd(eexp_data$Eexp1LongformEFreq)
range(eexp_data$Eexp1LongformEFreq)
table(eexp_data$Eexp1LongformEFreq)
Eexp1 <- ggplot(eexp_data, aes(x = UserId, y = Eexp1LongformEFreq)) +
    geom_point() +
    theme_classic()
```

Most of the participants rarely read fiction books or short stories electronically. Indeed, on average participants' response corresponds to 'a few times a year' on the frequency Likert-scale (`r round(mean(eexp_data$Eexp1LongformEFreq/2), 2)`, 1: Never, 2: A few times a year, 3: A few times a month, 4: A few times a week, 5: Every day). Most of the participants indicated that they 'Never' read fiction books or short stories electronically (`r sum(eexp_data$Eexp1LongformEFreq == 2)`, `r round((sum(eexp_data$Eexp1LongformEFreq == 2)/nrow(eexp_data))*100, 2)`%).

Secondly, we inspect variation in the second measure, frequency of using task-relevant digital devices for recreational reading:

```{r 'visualise eexp measure 2', include=FALSE}
# frequency of using task-relevant digital devices for recreational reading
# possible score range: min - 4, max - 20
mean(eexp_data$Eexp2DeviceRecFreq)
sd(eexp_data$Eexp2DeviceRecFreq)
range(eexp_data$Eexp2DeviceRecFreq)
table(eexp_data$Eexp2DeviceRecFreq)
Eexp2 <- ggplot(eexp_data, aes(x = UserId, y = Eexp2DeviceRecFreq)) +
    geom_point() +
    theme_classic()
```

Most participants also score low in the second electronic experience measure (task relevant digital device usage for recreational reading).  The average participant response corresponds to 'a few times a year' on the Likert-scale (`r round(mean(eexp_data$Eexp2DeviceRecFreq/4), 2)`, 1: Never, 2: A few times a year, 3: A few times a month, 4: A few times a week, 5: Every day). `r sum(eexp_data$Eexp2DeviceRecFreq == 4)` participants have the lowest possible score on this measure, and so they indicated 'Never' using laptops, smartphones, desktop computers, and tablet computers for recreational reading. In contrast, only `r sum(eexp_data$Eexp2DeviceRecFreq >= 16)` participants have a score that is higher than 16, indicating that they use task-relevant digital devices for recreational reading on average 'A few times a week'.

We then inspect correlations between the two eexp measures:

```{r 'correlation between eexp measures', include=FALSE}
cor(eexp_data[, c("Eexp1LongformEFreq", "Eexp2DeviceRecFreq")])
# Eexp <- plot(eexp_data$Eexp1LongformEFreq, eexp_data$Eexp2DeviceRecFreq)
```

The two electronic reading experience measures are moderately, positively correlated. This indicates that participants who read long-form fictional texts electronically are also likely to use task-relevant digital devices for recreational reading purposes.

We then inspect associations between the two measures and ownership of digital devices:

```{r 'inspect access to digital devices and eexp measures', include=FALSE}
# access to digital devices
## eexp1: longform fictional texts electronically
Eexp1Devices <- ggplot(
    eexp_data,
    aes(x = NumberOfDevices, y = Eexp1LongformEFreq)
) +
    geom_boxplot() +
    geom_jitter() +
    theme_classic()
ANOVA_NumberOfDevices_Eexp1 <- aov(
    Eexp1LongformEFreq ~ NumberOfDevices,
    data = eexp_data
)
summary(ANOVA_NumberOfDevices_Eexp1)
## eexp2: task-relevant devices for rec reading
Eexp2Devices <- ggplot(
    eexp_data,
    aes(x = NumberOfDevices, y = Eexp2DeviceRecFreq)
) +
    geom_boxplot() +
    geom_jitter() +
    theme_classic()
ANOVA_NumberOfDevices_Eexp2 <- aov(
    Eexp2DeviceRecFreq ~ NumberOfDevices,
    data = eexp_data
)
summary(ANOVA_NumberOfDevices_Eexp2)
```

Interestingly, the number of devices that participants own is not connected to either measure on task-relevant electronic reading experience. This indicates that ownership of more devices is not linked to more frequent electronic reading on these devices in our sample.

We then create an abridged version of the dataset for use in analyses:

```{r}
eexp_scores <- dplyr::select(
    eexp_data,
    UserId,
    Eexp1LongformEFreq,
    Eexp2DeviceRecFreq,
    NumberOfDevices
)
```

Finally we save the eexp dataset. The dataset has already been saved, and so the below r code chunk is not run.

```{r 'save eexp scores', eval=FALSE, echo=TRUE}
write.csv2(
    eexp_scores,
    "eexp_scores.csv"
)
```