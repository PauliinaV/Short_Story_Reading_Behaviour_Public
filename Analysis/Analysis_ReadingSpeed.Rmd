---
title: "Analysis_ReadingSpeed"
author: "Pauliina Vuorinen"
date: "14/10/2022-"
output: html_document
library: "~/Extra/RPackages.bib"
---

## Introduction

The purpose of this script is to analyse variance in participants' reading speed.

We aim to assess how variance in reading speed fluctuated during the study, in which contexts, and whether autonomy condition, contextual reading motivation, and electronic reading experience are connected to reading speed. See the full article for details.

To answer these research questions, we use 'AbsReadingRate' (saved in Data as reading_speed_measure_data.csv) as a measure of variance in reading speed. This measure was computed in Prep_ReadingSpeedMeasure.rmd. The measure indicates the absolute variation from participants' baseline speed (baseline deducted from speed). We focus on natural reading speeds with which participants can fully comprehend the story, which we call 'deep reading', and thus the dataset only includes observations on deep reading.

In this analysis script, AbsReadingRate is used as an outcome variable in **two different multilevel models**. In the first model, we study whether reader charactersistics such as condition, contextual motivation, and electronic reading experience predict AbsReadingRate. In the second model, the independent variables include information on the timing and location of AbsReadingRate. With the latter model our intention is to study in which situation reading speed varies. Both model structures are based on selection outlined in the full article. The models are constructed using lmer() in the lme4-package by @bates_fitting_2015.

**Information on the hypotheses**

* H6a: Situational autonomous motivation is connected to lower reading speed variance when task competence is high.
* H6b: Contextual autonomous motivation is connected to lower reading speed variance when task competence is high.
* H6c: Task-relevant electronic experience is connected to lower reading speed variance when task competence is high.

## Setup

```{r, setup}
library(tidyverse)
library(dplyr)
library(psych)
library(gridExtra)
library(corrplot)
library(lme4)
library(lmerTest)
library(DHARMa)
library(performance)
library(car)
library(effects)
library(interactions)
library(jtools)
getwd() # working directory should be ~/Short_Story_Reading_Behaviour_Public
```

Save working directory so that this script can be used elsewhere, if required. The working directory should be "~/Short-Story-Reading-Behaviour-Public/". If the working directory is not correct, we save the correct path and use that in loading files. In our purposes, the foulder could be found from /Documents/GITHUB/Short_Story_Reading_Behaviour_Public.

The working directory is not changed with setwd() because this script is knit remotely in other scripts.

```{r 'working directory for my purposes'}
mypath_SSRBP <- getwd()
if (!grepl("Short_Story_Reading_Behaviour_Public", mypath_SSRBP, fixed = TRUE)) {
    # wrong working directory
    if (!grepl("GITHUB", dirname(mypath_SSRBP), fixed = TRUE)) {
        # directory name isn't GITHUB, unlike I would expect
        if (grepl("GITHUB", mypath_SSRBP, fixed = TRUE)) {
            # GITHUB is in the path
            ## use mypath_SSRBP instead of dirname()
            mypath_SSRBP <- paste0(
                mypath_SSRBP,
                "/Short_Story_Reading_Behaviour_Public"
            )
        }
    } else {
        # directory name is GITHUB
        # save correct working directory
        mypath_SSRBP <- paste0(
            dirname(mypath_SSRBP),
            "/Short_Story_Reading_Behaviour_Public"
        )
    }
}
```

## Load data and check variable types

```{r, load data}
# load data
reading_speed_measure_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/reading_speed_measure_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
reading_speed_measure_data <- dplyr::select(reading_speed_measure_data, -X) # remove row numbers
```

We then load in data from questionnaires:

```{r 'load predictors: questionnaires'}
# IMI
IMI_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/IMI_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
IMI_scores <- dplyr::select(IMI_scores, -X) # remove row numbers
# IMI-R
IMIR_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/IMIR_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
IMIR_scores <- dplyr::select(IMIR_scores, -X) # remove row numbers
# Electronic experience
eexp_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/eexp_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
eexp_scores <- dplyr::select(eexp_scores, -X) # remove row numbers
# demographic information
demographics_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/demographics_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
demographics_data <- dplyr::select(demographics_data, -X) # remove row numbers
```

Load in information about the stories that participants read:

```{r}
story_information_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/story_information_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
story_information_data <- dplyr::select(story_information_data, -X) # remove row numbers
```

We then merge these dataframes together:

```{r 'merge dfs'}
# measure and IMI
reading_speed_measure_data <- merge(
    reading_speed_measure_data,
    IMI_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and IMI-R
reading_speed_measure_data <- merge(
    reading_speed_measure_data,
    IMIR_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and EEXP
reading_speed_measure_data <- merge(
    reading_speed_measure_data,
    eexp_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and demographic information
reading_speed_measure_data <- merge(
    reading_speed_measure_data,
    demographics_data,
    all.x = TRUE,
    by = "UserId"
)
# measure and information about stories
reading_speed_measure_data <- merge(
    reading_speed_measure_data,
    story_information_data[, c(1:3, 10)],
    all.x = TRUE,
    by = "StoryId"
)
```

## Change variable types

```{r 'change variable types'}
str(reading_speed_measure_data)
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_VariableTypeConversion.R"
    )
)

## turn columns into factors that should be factors
reading_speed_measure_data[, c(
    "UserId",
    "StoryId"
)] <- convert.magic(
    reading_speed_measure_data[, c(
        "UserId",
        "StoryId"
    )],
    "factor"
)
```

## Reader characteristics model

The reader characteristics model of reading speed aims to address the following hypotheses:

* H6a: Situational autonomous motivation is connected to lower reading speed variance when task competence is high.
* H6b: Contextual autonomous motivation is connected to lower reading speed variance when task competence is high.
* H6c: Task-relevant electronic experience is connected to lower reading speed variance when task competence is high.

The reader characteristics model for linearity of reading takes the following structure:

$Y_{i} = 
    \beta_{0} +
    DUD +
    Native +
    Cond +
    SComp +
    CMot +
    CComp +
    EExp1 +
    EExp2 +
    Cond x SComp +
    CMot x SComp +
    EExp1 x EExp2 +
    EExp1 x EExp2 x SComp +
    u_{Subject} +
    u_{Story} +
    u_{ScreenSize} +
    \varepsilon_{i}$ 

where
**$Y_{i}$** is the reading behaviour measure, in this case StartsNonlinearity for reading linearity.
**$\beta_{0}$** is the intercept
**DUD** is an acronym for 'DaysUntilDeadline' which tells us how much the participant has time left to read the short story. Days until deadline is used to control for variance in reading behaviour as a result of pressure to read the short story in time. For example, participants may read the text differently closer to the deadline compared to the beginning of the study.
**Native** is a binary variable indicating whether the participant is a native speaker of English or not (responses - Yes/No, yes indicating that the participant is a native speaker).
**Cond** represents autonomy condition (as a measure of situational motivation)
**SComp** is participants' situational competence (perception of competence to read the story), measured by the subcomponent of 'competence' from the IMI questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**CMot** is participants' contextual reading motivation, measured by the subcomponent of contextual interest from the IMI-R questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**CComp** is participants' contextual competence (perception of general reading ability), measured by the subcomponent of 'competence' from the IMI-R questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**EExp1** is frequency of using any electronic devices for long-form text reading purposes
**EExp2** is frequency of using task-relevant devices for any reading purpose
**$u_{Subject}$** indicates a random intercept of subject indicator
**$u_{Story}$** indicates a random intercept of story indicator
**$u_{ScreenSize}$** indicates a random intercept of screen size, measured by window width in pixels (which is affected by device and has an effect on how much text is visible)
**$\varepsilon_{i}$** indicates the residual variance of $Y_{i}$

Whereas linearity of reading and reading speed are analysed with this reader characteristics structure, the remaining models (task switching and reading persistence) are modelled without 'Cond x SComp', 'CMot x SComp', and 'EExp1 x EExp2 x SComp'. This is because linearity and speed are expected to be connected to motivation and electronic experience via interaction with situational competence, whereas task switching and reading persistence are expected to be connected to motivation and electronic experience directly (main effects).

### Visualise outcome variable and predictors

The reading_speed_measure_data has `r nrow(reading_speed_measure_data)` observations.

We create graphs to inspect speed of reading and its connection to other variables.

AbsReadingRate is skewed towards lower values, and so it could benefit from a log-transformation:

```{r}
hist(reading_speed_measure_data$AbsRelativeSpeedDifference)
hist(log(reading_speed_measure_data$AbsRelativeSpeedDifference + 1))
```

Variance between participants:

```{r}
ggplot(reading_speed_measure_data, aes(x = UserId, y = AbsReadingRate)) +
    geom_boxplot() +
    theme_classic()
```

Variance between different stories:

```{r}
ggplot(reading_speed_measure_data, aes(x = StoryId, y = AbsReadingRate)) +
    geom_boxplot() +
    theme_classic()
```

We correct for story length to see if there are differences between different books read:

```{r}
ggplot(reading_speed_measure_data, aes(x = StoryId, y = AbsReadingRate / CharacterLength)) +
    geom_boxplot() +
    theme_classic()
```

Variance across window widths (size of device and amount of text visible):

```{r}
ggplot(reading_speed_measure_data, aes(x = WindowWidth, y = AbsReadingRate)) +
    geom_point() +
    theme_classic()
```

We then visualise the effect of DaysUntilDeadline:

```{r}
ggplot(reading_speed_measure_data, aes(x = FirstTimeUntilDeadlineDays, y = AbsReadingRate)) +
    geom_point() +
    theme_classic()
```

### Reading Speed and Demographic Information

Gender:

```{r}
ggplot(reading_speed_measure_data, aes(x = Gender, y = AbsReadingRate)) +
    geom_violin() +
    geom_jitter() +
    theme_classic()
```

Age:

```{r}
ggplot(reading_speed_measure_data, aes(x = Age, y = AbsReadingRate)) +
    geom_point() +
    theme_classic()
```

Whether participants' native language is English or not:

```{r}
ggplot(reading_speed_measure_data, aes(x = NativeEnglish, y = AbsReadingRate)) +
    geom_violin() +
    geom_jitter() +
    theme_classic()
```

### Condition

```{r}
ggplot(reading_speed_measure_data, aes(x = Condition, y = AbsReadingRate)) +
    geom_boxplot() +
    theme_classic()
```

### Motivation

Only situational competence is included in the model from the IMI variables, considering that condition is significantly connected to both situational autonomy and interest.

Situational competence:

```{r}
ggplot(reading_speed_measure_data, aes(x = SCompetence, y = AbsReadingRate)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

We then create visualisations for contextual measures of motivation: interest, and competence.

Contextual interest:

```{r}
ggplot(reading_speed_measure_data, aes(x = CInterest, y = AbsReadingRate)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

Contextual competence:

```{r}
ggplot(reading_speed_measure_data, aes(x = CCompetence, y = AbsReadingRate)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

### Reading speed and electronic reading experience

We are interested in how task-relevant electronic experience is connected to reading speed.
Electronic experience was measured by two variables: Eexp1LongFormEFreq - the frequency of reading long-form, narrative texts electronnically, and Eexp2DeviceRecFreq - the frequency of using task-relevant digital devices for recreational reading purposes.

Eexp1LongformEFreq (longfrom reading electronically):

```{r}
ggplot(reading_speed_measure_data, aes(x = Eexp1LongformEFreq, y = AbsReadingRate)) +
    geom_point() +
    theme_classic()
```

Eexp2DeviceRecFreq (task-relevant digital devices for recreational reading):

```{r}
ggplot(reading_speed_measure_data, aes(x = Eexp2DeviceRecFreq, y = AbsReadingRate)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

### Set contrasts to predictors

Predictors in the model are given contrasts. Continuous variables are scaled and centered, whereas categorical variables are given helmert contrasts. Helmert contrasts are used to control for uneven levels in categorical variables.

Load a function to create helmert contrasts for categorical variables:

```{r 'load helmert contrasts functions'}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.weighted.R"
    )
)
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.unweighted.R"
    )
)
```

Contrasts for categorical variables:

```{r 'categorical contrasts'}
# whether native language is English or not
reading_speed_measure_data$NativeEnglish <- as.factor(reading_speed_measure_data$NativeEnglish)
contrasts(reading_speed_measure_data$NativeEnglish) <- contr.helmert.unweighted(reading_speed_measure_data$NativeEnglish)
contrasts(reading_speed_measure_data$NativeEnglish)
# condition
reading_speed_measure_data$Condition <- dplyr::recode(
    reading_speed_measure_data$Condition,
    "AutonomousCondition" = "HighAutonomyCondition",
    "NonAutonomousCondition" = "LowAutonomyCondition"
)
reading_speed_measure_data$Condition <- as.factor(reading_speed_measure_data$Condition)
contrasts(reading_speed_measure_data$Condition) <- contr.helmert.unweighted(
    reading_speed_measure_data$Condition
)
contrasts(reading_speed_measure_data$Condition)
```

Next, we scale and center continuous variables. These variables names are appended with ".cs" as a reminder that the variable is scaled and centered.

```{r 'scale continuous variables'}
# Situational competence
reading_speed_measure_data$SCompetence.sc <-
    scale(reading_speed_measure_data$SCompetence,
        center = TRUE,
        scale = TRUE
    )
# Contextual motivation
reading_speed_measure_data$CInterest.sc <-
    scale(reading_speed_measure_data$CInterest,
        center = TRUE,
        scale = TRUE
    )
reading_speed_measure_data$CCompetence.sc <-
    scale(reading_speed_measure_data$CCompetence,
        center = TRUE,
        scale = TRUE
    )
# Electronic reading experience
reading_speed_measure_data$Eexp1LongformEFreq.sc <-
    scale(reading_speed_measure_data$Eexp1LongformEFreq,
        center = TRUE,
        scale = TRUE
    )
reading_speed_measure_data$Eexp2DeviceRecFreq.sc <-
    scale(reading_speed_measure_data$Eexp2DeviceRecFreq,
        center = TRUE,
        scale = TRUE
    )
# Days until reading deadline
reading_speed_measure_data$DaysUntilDeadline.sc <-
    scale(reading_speed_measure_data$FirstTimeUntilDeadlineDays,
        center = TRUE,
        scale = TRUE
    )
```

### Modelling by Reader Characteristics

First, we build the full structure. Refer to the beginning of this script to see information on the full structure. The full model includes 20 parameters (including two random effects and categorical variable levels), and `r nrow(reading_speed_measure_data)` observations. Previous research has suggested that each parameter in a model has at least 20 observations. Our sample exceeds this threshold considering that `r nrow(reading_speed_measure_data)`/20 = `r round(nrow(reading_speed_measure_data)/20, 2)`

```{r}
FullStructure_RC <- (
    "RelativeSpeedDifference ~ DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | StoryId) + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Speed_RC_FullModel <- lmer(
    FullStructure_RC,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

The full model is singular, and so it needs to be simplified.

```{r}
summary(Speed_RC_FullModel)
plot(Speed_RC_FullModel)
plot(reading_speed_measure_data$RelativeSpeedDifference, fitted(Speed_RC_FullModel))
```

Inspect model values:

```{r}
fixef(Speed_RC_FullModel)
```

### Model backward selection

The full model is singular, and thus its structure needs to be simplified. We remove predictors by backward stepwise selection, until convergence. The predictors with the lowest p-value is removed first.

Find lowest p-value:
```{r}
summary(Speed_RC_FullModel)
```

An interaction between situational competence and condition, 'Condition:SCompetence.sc', has the lowest p-value and thus contributes the least to the model. It is removed:

```{r}
SimplifiedStructure <- str_remove(FullStructure_RC, fixed("+  Condition : SCompetence.sc "))
```

re-fit the model with the simplified structure:

```{r}
Speed_RC_ModelSelection1 <- lmer(
    SimplifiedStructure,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Speed_RC_ModelSelection1)
```

An interaction between the two electronic experience measures, 'Eexp1LongformEFreq.sc:Eexp2DeviceRecFreq.sc' has the lowest p-value and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("+ Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc "))
```

re-fit the model with the simplified structure:

```{r}
Speed_RC_ModelSelection2 <- lmer(
    SimplifiedStructure,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Speed_RC_ModelSelection2)
```

A binary variable of whether English is each participants' native language or not, 'NativeEnglish', has the highest p-value (of the predictors that can be removed from the model). So it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("+ NativeEnglish "))
```

```{r}
Speed_RC_ModelSelection3 <- lmer(
    SimplifiedStructure,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Speed_RC_ModelSelection3)
```

Condition has the highest p-value of the predictors that can be removed, and so it is taken out:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("+ Condition "))
```

```{r}
Speed_RC_ModelSelection4 <- lmer(
    SimplifiedStructure,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Speed_RC_ModelSelection4)
```

Many of the remaining predictors that can be removed (interaction effects need to be removed before removing main effects involved in interactions) are significant. The highest p-value, however, is between situational competence and contextual motivation, 'SCompetence.sc:CInterest.sc', and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("+ CInterest.sc : SCompetence.sc "))
```

```{r}
Speed_RC_ModelSelection5 <- lmer(
    SimplifiedStructure,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Speed_RC_ModelSelection5)
```

Contextual motivation, 'CInterest.sc', has the highest p-value, and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("+ CInterest.sc "))
```

```{r}
Speed_RC_ModelSelection6 <- lmer(
    SimplifiedStructure,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

The model converges after the removal. Inspect model values:

```{r}
summary(Speed_RC_ModelSelection6)
plot(Speed_RC_ModelSelection6)
```

### Model Assumptions

All lmer models were tested for (1) multicollinearity, (2) non-normality of residuals, (3) heteroscedasticity, (4) influential observations, and (5) normality of random effects

#### 1. Multicollinearity

We check for multicollinearity first with a correlation matrix:

```{r}
corrplot(cor(reading_speed_measure_data[, c(31:35)]), method = "number")
```

Some of the variables show high correlations. For example, contextual reading competence is correlated with situational competence (*r* = `r round(cor(reading_speed_measure_data$SCompetence, reading_speed_measure_data$CCompetence), 2)`) and contextual motivation (*r* = `r round(cor(reading_speed_measure_data$CCompetence, reading_speed_measure_data$CInterest), 2)`).

To check if these correlations are problematic, we calculate Variance Inflation Factors (VIF):

```{r}
check_collinearity(Speed_RC_ModelSelection6)
plot(check_collinearity(Speed_RC_ModelSelection6))
```

Despite of the correlations, the model predictors do not seem to be multicollinear. All VIF < 2, and usually VIF > 5 is used as an indicator of potential multicollinearity.

#### 2. Non-normality of residuals

We first check normality of residuals by plotting model residuals and fitted values

```{r}
plot(resid(Speed_RC_ModelSelection6), fitted(Speed_RC_ModelSelection6))
```

The model residuals are not symmetrically scattered in the plot, as the model observations seem to cluster around specific values.

To further check non-normality, we use a qqplot:

```{r}
qqnorm(residuals(Speed_RC_ModelSelection6))
```

The qqplot shows slight variation from normality. To assess how severe it is, we compare to a simulated qqplot:

```{r}
set.seed(1234)
par(mfrow = c(3, 3))
for (i in 1:9) {
    x <- rnorm(1936)
    qqnorm(x, main = "Simulated Normal Q-Q Plot")
    qqline(x)
}
```

Finally, we test normality of residuals with a Shapiro Test that checks studentized residuals for normal distribution. A p-value of < .05 indicates significant deviation from normality.

```{r}
plot(check_normality(Speed_RC_ModelSelection6))
```

Finally, the Shapiro Test indicates that the residuals are indeed not normally distributed.

The nonnormality of residuals seems to be due to observations that cluster around certain reading speeds, indeed the model seems to have trouble estimating mid-values in the distribution. It is possible that the largest amount of observations were detected at the midpoint of the distribution (around baseline speed), and participants varied widely in how baseline-level speeds were used during the study.

We first assess if the nonnormality of residuals could be addressed by a log-transformation of the outcome variable.

```{r}
SimplifiedStructure_Log <- (
    "log(RelativeSpeedDifference + 1) ~ DaysUntilDeadline.sc + SCompetence.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | StoryId) + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Speed_RC_ModelSelection6 <- lmer(
    SimplifiedStructure_Log,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

```{r}
plot(check_normality(Speed_RC_ModelSelection6))
```

The normality of residuals is not improved by a log-transformation. Mainly, the mid-values are affected.

We then assess if the nonnormality could be addressed by a squareroot transformation of the outcome variable. We perform this transformation on the full model structure, and simplify the model as necessary until convergence and non-singularity before retesting the model for normality of residuals:

```{r}
FullStructure_RC_Transformed <- (
    "sqrt(RelativeSpeedDifference + 1) ~ DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | StoryId) + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Speed_RC_FullModel_Transformed <- lmer(
    FullStructure_RC_Transformed,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

```{r}
summary(Speed_RC_FullModel_Transformed)
```

The full model structure is singular and so it needs to be simplified before we can test for normality of residuals. The simplification is done off-script for brevity, but it is carried out similarly to the simplification earlier. The simplified model to reach convergence has the following form:

```{r}
SimplifiedStructure_Transformed <- (
    "sqrt(RelativeSpeedDifference + 1) ~ Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | StoryId) + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Speed_RC_ModelSelection_Transformed <- lmer(
    SimplifiedStructure_Transformed,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

```{r}
summary(Speed_RC_ModelSelection_Transformed)
```

We can then check normality of residuals:

```{r}
plot(resid(Speed_RC_ModelSelection_Transformed), fitted(Speed_RC_ModelSelection_Transformed))
plot(check_normality(Speed_RC_ModelSelection_Transformed))
qqnorm(residuals(Speed_RC_ModelSelection_Transformed))
```

The model square-root transformed outcome variable model does not align to the assumption of normality of residuals.

#### 3. Heteroscedasticity

We use the Breusch-Pagan test to check whether the variances of residuals in the model vary systematically:

```{r}
check_heteroscedasticity(TaskSwitching_FullModel)
plot(check_heteroscedasticity(TaskSwitching_FullModel))
```

The p-value is not significant, which indicates that the residuals are homoscedastic. Also, the plot line seems to be fairly flat and horizontal, indicating of homoscedasticity.

We check categorical predictors separately with a Levene's test:

```{r}
leveneTest(residuals(TaskSwitching_FullModel) ~ task_switching_data$Condition)
```

```{r}
leveneTest(residuals(TaskSwitching_FullModel) ~ task_switching_data$NativeEnglish)
```

The assumption of equal variables is met for both Condition and NativeEnglish.

#### 4. Influential Observations

We test influential observations with Cook's Distance:

```{r}
cooksd <-
    cooks.distance(TaskSwitching_FullModel)
plot(cooksd,
    pch = "*",
    cex = 2,
    main = "Influential Obs by Cooks Distance"
)
abline(h = 4 * mean(cooksd, na.rm = TRUE), col = "red")
text(
    x = 1:length(cooksd) + 1,
    y = cooksd,
    labels = ifelse(cooksd > 4 * mean(cooksd, na.rm = TRUE), names(cooksd), ""),
    col = "red"
)
```

The plot indicates that some observations may be influential. However, the Cook's Distance threshold is very conservative at `r round((4 * mean(cooksd, na.rm = TRUE)), 2)`.

We further inspect Cook's Distance with a different plot:

```{r}
check_outliers(TaskSwitching_FullModel)
plot(check_outliers(TaskSwitching_FullModel))
```

Usage of a less conservative limit (Cook's Distance > .5 for influential observations) indicates that none of the observations are influential. We trust this judgement after off-script experimentation indicated that removal of the most influential observation flagged earlier does not affect results.

#### Normality of Random Effects

Random effects should be normally distributed in multilevel models. We test this assumption by inspecting first the normality of 'UserId' and then 'StoryId' random intercepts.

```{r}
random_intercept_UserId <- ranef(TaskSwitching_FullModel)$UserId$`(Intercept)`
qqnorm(random_intercept_UserId)
qqline(random_intercept_UserId)
shapiro.test(random_intercept_UserId)
```

The qqplot slightly varies from the reference line, but considering that the variance is not extensive and the Shapiro Test is not significant, we assume that UserId random intercept is normally distributed

```{r}
random_intercept_StoryId <- ranef(TaskSwitching_FullModel)$StoryId$`(Intercept)`
qqnorm(random_intercept_StoryId)
qqline(random_intercept_StoryId)
shapiro.test(random_intercept_StoryId)
```

StoryId aligns much worse with the reference line, however, this is expected considering that StoryId only includes 9 different story groups. With the limited amount of information, achieving a visually normally distributed result is unlikely. Considering that the Shapiro Test is not significant, we can assume that StoryId random intercept is normally distributed.

The model aligns with all assumptions.

Inspection of the model indicates that StoryId accounts for no variance in the model, which may cause the singularity. According to @Bolker2022 this is a common occurrence. In this case, the random effect can be added in the model as a fixed effect to assess if this rectified the singularity issue. 

#### Fit StoryId as a fixed effect

To address singularity in the model, we attempt to fit the model with StoryId as a fixed rather than a random effect. This is because StoryId is estimated to account for zero variance in the model.

We first set contrasts to StoryId:

```{r}
reading_speed_measure_data$StoryId <- as.factor(reading_speed_measure_data$StoryId)
contrasts(reading_speed_measure_data$StoryId) <- contr.helmert.unweighted(reading_speed_measure_data$StoryId)
contrasts(reading_speed_measure_data$StoryId)
```

Create a new model structure with StoryId as a fixed effect:

```{r}
FullStructure_AdjustedStoryId_RC <- (
    "log(AbsReadingRate + 1) ~ DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + StoryId + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Speed_RC_AdjustedStoryId_Model <- lmer(
    FullStructure_AdjustedStoryId_RC,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

The full model converges and it is no longer singular. This indicates that zero variance in StoryId was likely to cause the singularity.

```{r}
summary(Speed_RC_AdjustedStoryId_Model)
plot(Speed_RC_AdjustedStoryId_Model)
```

Inspect model values:

```{r}
fixef(Speed_RC_AdjustedStoryId_Model)
```

StoryId may be collinear with other variables in the model. We assess this possibility:

```{r}
check_collinearity(Speed_RC_AdjustedStoryId_Model)
plot(check_collinearity(Speed_RC_AdjustedStoryId_Model))
vif_values <- vif(Speed_RC_AdjustedStoryId_Model)
# compare to original model structure:
plot(check_collinearity(Speed_RC_FullModel))
```

Indeed, some of the variables in the model appear collinear, whereas this is not the case for the original model. Variance Inflation Factor (VIF) is very high for StoryId at `r vif_values[9]` (usually a VIF > 10 is taken as indication of high multicollinearity).

Therefore, we assume that StoryId cannot be included in the model as random nor a fixed effect. As a random effect, StoryId accounts for zero variance which results in singularity, whereas as a fixed effect, StoryId is multicollinear with other predictors in the model.

We therefore remove StoryId from the model and attempt to fit the model without it.

#### Fit reader characteristics model without StoryId

Considering that StoryId cannot be included in the model as a random effect (zero variance results in singularity) nor a fixed effect (multicollinear), we attempt to fit the reader characteristics model of reading speed without StoryId. This means that we cannot estimate differences between the different texts read during the study.

Create a new model structure without StoryId:

```{r}
FullStructure_RemovedStoryId_RC <- (
    "log(AbsReadingRate + 1) ~ DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Speed_RC_RemovedStoryId_Model <- lmer(
    FullStructure_RemovedStoryId_RC,
    data = reading_speed_measure_data,
    control = lmerControl(optimizer = "bobyqa"),
    REML = TRUE
)
```

The full model converges and it is not singular.

```{r}
summary(Speed_RC_RemovedStoryId_Model)
plot(Speed_RC_RemovedStoryId_Model)
```

Inspect model values:

```{r}
fixef(Speed_RC_RemovedStoryId_Model)
```

We check if removing StoryId from the model resolved multicollinearity:

```{r}
check_collinearity(Speed_RC_RemovedStoryId_Model)
plot(check_collinearity(Speed_RC_RemovedStoryId_Model))
```

A quick assessment indicates that the model is no longer multicollinear.
Thus we accept this model, and subject it to assumptions testing.

