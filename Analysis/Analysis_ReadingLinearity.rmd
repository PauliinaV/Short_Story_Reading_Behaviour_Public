---
title: "Analysis_ReadingLinearity"
author: "Pauliina Vuorinen"
date: "27/06/2022-"
output: html_document
library: "~/Extra/RPackages.bib"
---

## Introduction

<!-- some notes -->
* talk about datapoint or observation k that is predicted
    * previous event is k-1, the one before that is k-2
* create an illustration from a participants' linearity across pages? look at notebook pg43
    * this could go into Prep_ReadingLinearityMeasure


The purpose of this script is to analyse how linearly participants read the short story during the study.
We aim to assess how frequently participants used nonlinearity during the study, how nonlinearity occurred, and whether autonomy condition, contextual reading motivation, and electronic reading experience are connected to this nonlinearity. See the full article for details.

To answer these research questions, we use 'StartsNonlinearity' (saved in Data as linearity_measure_data.csv) as a measure of the frequency of nonlinearity. This measure was computed in Prep_ReadingLinearityMeasure.rmd, and it tells us whether nonlinear navigation was initiated on a page-view or not. Therefore, StartsNonlinearity is TRUE when a participant starts a regression or a forward leap on a page-view and the previous page-view did not include similar nonlinearity (see Prep_TrackingDataWrangling.rmd in Prep for more detail).

In this analysis script, StartsNonlinearity is used as an outcome variable in **two different multilevel models**. In the first model, we study whether reader charactersistics such as condition, contextual motivation, and electronic reading experience predict StartsNonlinearity. In the second model, the independent variables include information on the timing and location of StartsNonlinearity. With the latter model our intention is to study in which situations nonlinearity occurs. Both model structures are based on selection outlined in the full article. The models are constructed using glmer() in the lme4-package by @bates_fitting_2015.

**Information on the hypotheses**

* H6a: Situational autonomous motivation is connected to infrequent nonlinearity when task competence is high.
* H6b: Contextual autonomous motivation is connected to infrequent nonlinearity when task competence is high.
* H6c: Task-relevant electronic experience is connected to infrequent nonlinearity when task competence is high.

## Setup

```{r, setup}
library(tidyverse)
library(dplyr)
library(psych)
library(gridExtra)
library(corrplot)
library(lme4)
library(lmerTest)
library(DHARMa)
library(performance)
library(car)
library(effects)
library(interactions)
library(jtools)
getwd() # working directory should be ~/Short_Story_Reading_Behaviour_Public
```

Save working directory so that this script can be used elsewhere, if required. The working directory should be "~/Short-Story-Reading-Behaviour-Public/". If the working directory is not correct, we save the correct path and use that in loading files. In our purposes, the foulder could be found from /Documents/GITHUB/Short_Story_Reading_Behaviour_Public.

The working directory is not changed with setwd() because this script is knit remotely in other scripts.

```{r 'working directory for my purposes'}
mypath_SSRBP <- getwd()
if (!grepl("Short_Story_Reading_Behaviour_Public", mypath_SSRBP, fixed = TRUE)) {
    # wrong working directory
    if (!grepl("GITHUB", dirname(mypath_SSRBP), fixed = TRUE)) {
        # directory name isn't GITHUB, unlike I would expect
        if (grepl("GITHUB", mypath_SSRBP, fixed = TRUE)) {
            # GITHUB is in the path
            ## use mypath_SSRBP instead of dirname()
            mypath_SSRBP <- paste0(
                mypath_SSRBP,
                "/Short_Story_Reading_Behaviour_Public"
            )
        }
    } else {
        # directory name is GITHUB
        # save correct working directory
        mypath_SSRBP <- paste0(
            dirname(mypath_SSRBP),
            "/Short_Story_Reading_Behaviour_Public"
        )
    }
}
```

## Load data and check variable types

```{r, load data}
# load data
linearity_measure_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/linearity_measure_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
linearity_measure_data <- dplyr::select(linearity_measure_data, -X) # remove row numbers
```

We then load in data from questionnaires:

```{r 'load predictors: questionnaires'}
# IMI
IMI_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/IMI_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
IMI_scores <- dplyr::select(IMI_scores, -X) # remove row numbers
# IMI-R
IMIR_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/IMIR_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
IMIR_scores <- dplyr::select(IMIR_scores, -X) # remove row numbers
# Electronic experience
eexp_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/eexp_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
eexp_scores <- dplyr::select(eexp_scores, -X) # remove row numbers
# demographic information
demographics_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/demographics_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
demographics_data <- dplyr::select(demographics_data, -X) # remove row numbers
```

Load in information about the stories that participants read:

```{r}
story_information_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/story_information_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
story_information_data <- dplyr::select(story_information_data, -X) # remove row numbers
```

We then merge these dataframes together:

```{r 'merge dfs'}
# measure and IMI
linearity_measure_data <- merge(
    linearity_measure_data,
    IMI_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and IMI-R
linearity_measure_data <- merge(
    linearity_measure_data,
    IMIR_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and EEXP
linearity_measure_data <- merge(
    linearity_measure_data,
    eexp_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and demographic information
linearity_measure_data <- merge(
    linearity_measure_data,
    demographics_data,
    all.x = TRUE,
    by = "UserId"
)
# measure and information about stories
linearity_measure_data <- merge(
    linearity_measure_data,
    story_information_data[, c(1:3, 10)],
    all.x = TRUE,
    by = "StoryId"
)
```

## Change variable types

```{r 'change variable types'}
str(linearity_measure_data)
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_VariableTypeConversion.R"
    )
)

## turn columns into factors that should be factors
linearity_measure_data[, c(
    "UserId",
    "StoryId"
)] <- convert.magic(
    linearity_measure_data[, c(
        "UserId",
        "StoryId"
    )],
    "factor"
)
```

## Reader characteristics model

The reader characteristics model of linearity aims to address the following hypotheses:

* H6a: Situational autonomous motivation is connected to infrequent nonlinearity when task competence is high.
* H6b: Contextual autonomous motivation is connected to infrequent nonlinearity when task competence is high.
* H6c: Task-relevant electronic experience is connected to infrequent nonlinearity when task competence is high.

The reader characteristics model for linearity of reading takes the following structure:

$Y_{i} = 
    \beta_{0} +
    DUD +
    Native +
    Cond +
    SComp +
    CMot +
    CComp +
    EExp1 +
    EExp2 +
    Cond x SComp +
    CMot x SComp +
    EExp1 x EExp2 +
    EExp1 x EExp2 x SComp +
    u_{Subject} +
    u_{Story} +
    u_{ScreenSize} +
    \varepsilon_{i}$ 

where
**$Y_{i}$** is the reading behaviour measure, in this case StartsNonlinearity for reading linearity.
**$\beta_{0}$** is the intercept
**DUD** is an acronym for 'DaysUntilDeadline' which tells us how much the participant has time left to read the short story. Days until deadline is used to control for variance in reading behaviour as a result of pressure to read the short story in time. For example, participants may read the text differently closer to the deadline compared to the beginning of the study.
**Native** is a binary variable indicating whether the participant is a native speaker of English or not (responses - Yes/No, yes indicating that the participant is a native speaker).
**Cond** represents autonomy condition (as a measure of situational motivation)
**SComp** is participants' situational competence (perception of competence to read the story), measured by the subcomponent of 'competence' from the IMI questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**CMot** is participants' contextual reading motivation, measured by the subcomponent of contextual interest from the IMI-R questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**CComp** is participants' contextual competence (perception of general reading ability), measured by the subcomponent of 'competence' from the IMI-R questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**EExp1** is frequency of using any electronic devices for long-form text reading purposes
**EExp2** is frequency of using task-relevant devices for any reading purpose
**$u_{Subject}$** indicates a random intercept of subject indicator
**$u_{Story}$** indicates a random intercept of story indicator
**$u_{ScreenSize}$** indicates a random intercept of screen size, measured by window width in pixels (which is affected by device and has an effect on how much text is visible)
**$\varepsilon_{i}$** indicates the residual variance of $Y_{i}$

Whereas linearity of reading and reading speed are analysed with this reader characteristics structure, the remaining models (task switching and reading persistence) are modelled without 'Cond x SComp', 'CMot x SComp', and 'EExp1 x EExp2 x SComp'. This is because linearity and speed are expected to be connected to motivation and electronic experience via interaction with situational competence, whereas task switching and reading persistence are expected to be connected to motivation and electronic experience directly (main effects).

### Visualise outcome variable and predictors

The linearity_measure_data has `r nrow(linearity_measure_data)` observations.

We create graphs to inspect linearity of reading and its connection to other variables.

The linearity measure, StartsNonlinearity, is a binary variable (whether each event initiates nonlinearity or not, TRUE/FALSE). To visualise it, we create counts:

```{r}
linearity_measure_count <- linearity_measure_data %>%
    group_by(
        UserId,
        StoryId,
        CharacterLength,
        Gender,
        Age,
        NativeEnglish,
        Condition,
        SCompetence,
        CInterest,
        CCompetence,
        Eexp1LongformEFreq,
        Eexp2DeviceRecFreq,
        StartsNonlinearity
    ) %>%
    summarise(
        Count = n()
    )
```

```{r}
ggplot(linearity_measure_count, aes(x = StartsNonlinearity, y = Count)) +
    geom_violin() +
    geom_jitter() +
    theme_classic()
```

Variation between participants:

```{r}
ggplot(linearity_measure_count, aes(x = UserId, y = Count, colour = StartsNonlinearity)) +
    geom_point() +
    theme_classic()
```

Variation between different stories read:

```{r}
ggplot(linearity_measure_count, aes(x = StoryId, y = Count, colour = StartsNonlinearity)) +
    geom_boxplot() +
    theme_classic()
```

We correct for story length to see if there are differences between different books read:

```{r}
ggplot(
    linearity_measure_count,
    aes(x = StoryId, y = (Count / CharacterLength), colour = StartsNonlinearity)
) +
    geom_boxplot() +
    geom_jitter() +
    theme_classic() +
    ylab("Count of StartsNonlinearity/Story length")
```

The impact of window width (size of device and amount of text visible):

```{r}
ggplot(
    linearity_measure_data,
    aes(x = StartsNonlinearity, y = WindowWidth, colour = UserId)
) +
    geom_jitter(size = 2) +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

We then visualise the effect of DaysUntilDeadline:

```{r}
ggplot(
    linearity_measure_data,
    aes(x = StartsNonlinearity, y = FirstTimeUntilDeadlineDays, colour = UserId)
) +
    geom_jitter(size = 2) +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```


### Linearity of Reading and Demographic Information

Gender:

```{r}
ggplot(linearity_measure_count, aes(x = Gender, y = Count, colour = StartsNonlinearity)) +
    geom_violin() +
    geom_jitter() +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

Age:

```{r}
ggplot(linearity_measure_count, aes(x = Age, y = Count, colour = StartsNonlinearity)) +
    geom_point() +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

Whether participants' native language is English or not:

```{r}
ggplot(linearity_measure_count, aes(x = NativeEnglish, y = Count, colour = StartsNonlinearity)) +
    geom_violin() +
    geom_jitter() +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

### Condition

```{r}
ggplot(linearity_measure_count, aes(x = Condition, y = Count, colour = StartsNonlinearity)) +
    geom_boxplot() +
    theme_classic()
```

### Motivation

Only situational competence is included in the model from the IMI variables, considering that condition is significantly connected to both situational autonomy and interest.

Situational competence:

```{r}
ggplot(linearity_measure_count, aes(x = SCompetence, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

We then create visualisations for contextual measures of motivation: interest, and competence.

Contextual interest:

```{r}
ggplot(linearity_measure_count, aes(x = CInterest, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

Contextual competence:

```{r}
ggplot(linearity_measure_count, aes(x = CCompetence, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

### Linearity of reading and electronic reading experience

We are interested in how task-relevant electronic experience is connected to linearity.
Electronic experience was measured by two variables: Eexp1LongFormEFreq - the frequency of reading long-form, narrative texts electronnically, and Eexp2DeviceRecFreq - the frequency of using task-relevant digital devices for recreational reading purposes.

Eexp1LongformEFreq (longfrom reading electronically):

```{r}
ggplot(linearity_measure_count, aes(x = Eexp1LongformEFreq, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

Eexp2DeviceRecFreq (task-relevant digital devices for recreational reading):

```{r}
ggplot(linearity_measure_count, aes(x = Eexp2DeviceRecFreq, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

### Set contrasts to predictors

Predictors in the model are given contrasts. Continuous variables are scaled and centered, whereas categorical variables are given helmert contrasts. Helmert contrasts are used to control for uneven levels in categorical variables.

Load a function to create helmert contrasts for categorical variables:

```{r 'load helmert contrasts functions'}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.weighted.R"
    )
)
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.unweighted.R"
    )
)
```

Contrasts for categorical variables:

```{r 'categorical contrasts'}
# whether native language is English or not
linearity_measure_data$NativeEnglish <- as.factor(linearity_measure_data$NativeEnglish)
contrasts(linearity_measure_data$NativeEnglish) <- contr.helmert.unweighted(linearity_measure_data$NativeEnglish)
contrasts(linearity_measure_data$NativeEnglish)
# condition
linearity_measure_data$Condition <- dplyr::recode(
    linearity_measure_data$Condition,
    "AutonomousCondition" = "HighAutonomyCondition",
    "NonAutonomousCondition" = "LowAutonomyCondition"
)
linearity_measure_data$Condition <- as.factor(linearity_measure_data$Condition)
contrasts(linearity_measure_data$Condition) <- contr.helmert.unweighted(
    linearity_measure_data$Condition
)
contrasts(linearity_measure_data$Condition)
```

Next, we scale and center continuous variables. These variables names are appended with ".cs" as a reminder that the variable is scaled and centered.

```{r 'scale continuous variables'}
# Situational competence
linearity_measure_data$SCompetence.sc <-
    scale(linearity_measure_data$SCompetence,
        center = TRUE,
        scale = TRUE
    )
# Contextual motivation
linearity_measure_data$CInterest.sc <-
    scale(linearity_measure_data$CInterest,
        center = TRUE,
        scale = TRUE
    )
linearity_measure_data$CCompetence.sc <-
    scale(linearity_measure_data$CCompetence,
        center = TRUE,
        scale = TRUE
    )
# Electronic reading experience
linearity_measure_data$Eexp1LongformEFreq.sc <-
    scale(linearity_measure_data$Eexp1LongformEFreq,
        center = TRUE,
        scale = TRUE
    )
linearity_measure_data$Eexp2DeviceRecFreq.sc <-
    scale(linearity_measure_data$Eexp2DeviceRecFreq,
        center = TRUE,
        scale = TRUE
    )
# Days until reading deadline
linearity_measure_data$DaysUntilDeadline.sc <-
    scale(linearity_measure_data$FirstTimeUntilDeadlineDays,
        center = TRUE,
        scale = TRUE
    )
```

### Modelling by Reader Characteristics

First, we build the full structure. Refer to the beginning of this script to see information on the full structure. The full model includes 20 parameters (including two random effects and categorical variable levels), and `r nrow(linearity_measure_data)` observations. Previous research has suggested that each parameter in a model has at least 20 observations. Our sample exceeds this threshold considering that `r nrow(linearity_measure_data)`/20 = `r round(nrow(linearity_measure_data)/20, 2)`

```{r}
FullStructure_RC <- (
    "StartsNonlinearity ~ DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | StoryId) + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Linearity_RC_FullModel <- glmer(
    FullStructure_RC,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The full model is singular, and so it needs to be simplified.

```{r}
summary(Linearity_RC_FullModel)
plot(Linearity_RC_FullModel)
```

Inspect model values:

```{r}
fixef(Linearity_RC_FullModel)
```

### Model backward selection

The fullmodel is singular, and thus its structure needs to be simplified. We remove predictors by backward stepwise selection, until convergence. The predictors with the lowest p-value is removed first.

Find lowest p-value:
```{r}
summary(Linearity_RC_FullModel)
```

'DaysUntilDeadline.sc' has the lowest p-value and thus contributes the least to the model. It is removed:

```{r}
SimplifiedStructure <- str_remove(FullStructure_RC, fixed("DaysUntilDeadline.sc + "))
```

re-fit the model with the simplified structure:

```{r}
Linearity_RC_ModelSelection1 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Linearity_RC_ModelSelection1)
plot(Linearity_RC_ModelSelection1)
```

'Condition:SCompetence.sc' has the lowest p-value and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("+  Condition : SCompetence.sc "))
```

Re-fit the model:

```{r}
Linearity_RC_ModelSelection2 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Linearity_RC_ModelSelection2)
plot(Linearity_RC_ModelSelection2)
```

'CCompetence.sc' has the lowest p-value and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("+ CCompetence.sc "))
```

Re-fit the model:

```{r}
Linearity_RC_ModelSelection3 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Linearity_RC_ModelSelection3)
plot(Linearity_RC_ModelSelection3)
```

'NativeEnglish' has the lowest p-value and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("NativeEnglish + "))
```

Re-fit the model:

```{r}
Linearity_RC_ModelSelection4 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Linearity_RC_ModelSelection4)
plot(Linearity_RC_ModelSelection4)
```

'Condition' has the lowest p-value and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("Condition + "))
```

Re-fit the model:

```{r}
Linearity_RC_ModelSelection5 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model converges, and it is no longer singular.
We then inspect the model coefficients:

```{r}
summary(Linearity_RC_ModelSelection5)
plot(Linearity_RC_ModelSelection5)
fixef(Linearity_RC_ModelSelection5)
```

### Model Assumptions

All generalised linear mixed models were tested for (1) multicollinearity, (2) under- and overdispersion, (3) heteroscedasticity, (4) influential observations, and (5) normality of random effects.

#### 1. Multicollinearity

We check for multicollinearity first with a correlation matrix:

```{r}
corrplot(cor(linearity_measure_data[, c(17, 19, 21:22)]), method = "number")
```

The two electronic experience measures are positively correlated, *r* = `r round(cor(linearity_measure_data$Eexp1LongformEFreq, linearity_measure_data$Eexp2DeviceRecFreq), 2)`. Other predictors in the model are not correlated with each other.

To check if these correlations are problematic, we calculate Variance Inflation Factors (VIF):

```{r}
check_collinearity(Linearity_RC_ModelSelection5)
plot(check_collinearity(Linearity_RC_ModelSelection5))
```

Despite of the correlations, the model predictors do not seem to be multicollinear. All VIF < 1.5, and usually VIF > 5 is used as an indicator of potential multicollinearity.

#### 2. Under- and Overdispersion

Under- and overdispersion occur when the data shows less or more variation, respectively, than what we would expect. We test dispersion with simulation-based tests from DHARMa:

```{r}
simulated_model_output <- simulateResiduals(fittedModel = Linearity_RC_ModelSelection5, plot = F)
```

```{r}
plotQQunif(simulated_model_output)
```

The model does not deviate from expected distribution according to the qqplot.

```{r}
testDispersion(simulated_model_output)
```

The dispersion test is not significant, indicated that the model is not affected by under- or overdispersion.

#### 3. Heteroscedasticity

We use the same simulated_model_output created in step 2 to test heteroscedasticity

```{r}
testQuantiles(simulated_model_output)
```

The p-value is not significant, and the lines lie flat and horizontal in the plot, close to the reference lines. This indicates that the residuals are homoscedastic.

#### 4. Influential Observations

We test influential observations with Cook's Distance:

```{r}
cooksd <-
    cooks.distance(Linearity_RC_ModelSelection5)
plot(cooksd,
    pch = "*",
    cex = 2,
    main = "Influential Obs by Cooks Distance"
)
abline(h = 4 * mean(cooksd, na.rm = TRUE), col = "red")
text(
    x = 1:length(cooksd) + 1,
    y = cooksd,
    labels = ifelse(cooksd > 4 * mean(cooksd, na.rm = TRUE), names(cooksd), ""),
    col = "red"
)
```

The plot indicates that some observations may be influential. However, the Cook's Distance threshold is very conservative at `r round((4 * mean(cooksd, na.rm = TRUE)), 2)`.

We further inspect outliers using the simulated_model_output and DHARMa's outlier test:

```{r}
testOutliers(simulated_model_output)
```

The test and the plot indicate that none of the observations are influential. We trust this judgement after off-script experimentation indicated that removal of the most influential observation flagged earlier does not affect results.

#### 5. Normality of Random Effects

Random effects should be normally distributed in multilevel models. We test this assumption by inspecting first the normality of 'UserId' and then 'StoryId', and finally 'WindowWidth' random intercepts.

```{r}
random_intercept_UserId <- ranef(Linearity_RC_ModelSelection5)$UserId$`(Intercept)`
qqnorm(random_intercept_UserId)
qqline(random_intercept_UserId)
shapiro.test(random_intercept_UserId)
```

The qqplot varies from the reference line, but considering that the variance is not extensive and the Shapiro Test is not significant, we assume that UserId random intercept is normally distributed.

```{r}
random_intercept_StoryId <- ranef(Linearity_RC_ModelSelection5)$StoryId$`(Intercept)`
qqnorm(random_intercept_StoryId)
qqline(random_intercept_StoryId)
shapiro.test(random_intercept_StoryId)
```

StoryId aligns much worse with the reference line, however, this is expected considering that StoryId only includes 9 different story groups. With the limited amount of information, achieving a visually normally distributed result is unlikely. Considering that the Shapiro Test is not significant, we can assume that StoryId random intercept is normally distributed.

```{r}
random_intercept_WindowWidth <- ranef(Linearity_RC_ModelSelection5)$WindowWidth$`(Intercept)`
qqnorm(random_intercept_WindowWidth)
qqline(random_intercept_WindowWidth)
shapiro.test(random_intercept_WindowWidth)
```

Again, the qqplot indicates that WindowWidth may vary from normality, however, considering that the variance is not extensive and the Shapiro Test is not significant, we assume that WindowWidth random intercept is normally distributed.

The model aligns with all assumptions.

### Interpret reader characteristics model

We then interpret model effects to interpret the results.

```{r}
summary(Linearity_RC_ModelSelection5)
```

Only an interaction effect between the two task-relevant electronic experience measures is a signficant predictor of linearity of reading.

#### Hypotheses

**H4a: We expect situational motivation (high-autonomy condition) to be a negative predictor of nonlinear navigation frequency when situational competence is high.**

This hypothesis was not supported as the interaction effect between Condition and situational competence was removed from the model during backward stepwise selection. Indeed, the main effect of condition was similarly removed, and therefore, participants in the high-autonomy and low-autonomy conditions did not differ significantly in their frequency of using nonlinear navigation during reading of the short story.

**H4b: We expect contextual motivation to be a negative predictor of nonlinear navigation frequency when situational competence is high.**

This hypothesis was not supported as the interaction between contextual motivation ('CInterest') and situational competence (SCompetence) was not a significant predictor of nonlinear navigation.

```{r}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_AddLine.R"
    )
)

linearity_effectplot_cmotscomp <- interact_plot(
    Linearity_RC_ModelSelection5,
    pred = "CInterest.sc",
    modx = "SCompetence.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Situation competence score,(centered and scaled)")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Contextual motivation score, measured by contextual reading interest,(centered and scaled)")
    )
```

Although the hypothesis was not supported, the trend of the results is in the same direction as we predicted: participants with a high contextual motivation score initiated nonlinear navigation slightly less often when they found the text easy to read (high situational competence score). In contrast, a low contextual motivation score and high situational competence is associated with more frequent nonlinear navigation.

However, the interaction was not significant and so the effect should not be interpreted further.

**H4c: We expect an interaction between task-relevant electronic experience measures to be a negative predictor of nonlinear navigation frequency when situational competence is high.**

This hypothesis was not supported as an interaction between the two electronic experience measures and situational competence was not a significant predictor of linearity of reading.

```{r}
linearity_effectplot_eexpscomp <- interact_plot(
    Linearity_RC_ModelSelection5,
    pred = "Eexp1LongformEFreq.sc",
    modx = "Eexp2DeviceRecFreq.sc",
    mod2 = "SCompetence.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Electronic experience measure 2: Device*"),
    mod2.labels = c("-1SD Situational competence", "Mean of Situational Competence", "+1SD Situational Competence")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Electronic experience measure 1, frequency of e-reading task-relevant text types,(centered and scaled)")
    )
```

Indeed, the effect of electronic experience was the same, despite of situational competence.

Although the three-way interaction between electronic experience measures and situational competence did not significantly predict linearity of reading, a two-way interaction between the electronic experience measures was a significant predictor in the model.

```{r}
linearity_effectplot_eexp <- interact_plot(
    Linearity_RC_ModelSelection5,
    pred = "Eexp1LongformEFreq.sc",
    modx = "Eexp2DeviceRecFreq.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Electronic experience measure 2: Device*")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Electronic experience measure 1, frequency of e-reading task-relevant text types,(centered and scaled)")
    )
```

The finding indicated that a combination of high scores in the two task-relevant electronic reading experience measures was associated with lower likelihood of nonlinear navigation. In contrast, a low score in either measure with a high level of the other, was associated with more frequent nonlinear navigation. This indicates that electronic experience affected participants' linearity of reading.

## Event properties model

The event properties model of linearity is used for explorative purposes, to study how frequency of nonlinear navigation varied during the study.

The model takes the following structure:

$Y_{i} = 
    \beta_{0} +
    # control variable
    DUD +
    # predictors
    Observation~k-1~ +
    Observation~k-2~ +
    ReadingSessionNumber +
    TimeInReadingSession +
    LocationInText +
    u_{Subject} +
    u_{Story} +
    u_{ScreenSize} +
    \varepsilon_{i}$ 

where <!-- fix below-->
**$Y_{i}$** is the reading behaviour measure, in this case StartsNonlinearity for reading linearity.
**$\beta_{0}$** is the intercept
**DUD** is an acronym for 'DaysUntilDeadline' which tells us how much the participant has time left to read the short story. Days until deadline is used to control for variance in reading behaviour as a result of pressure to read the short story in time. For example, participants may read the text differently closer to the deadline compared to the beginning of the study.
**Observation~k-1~**
**Observation~k-2~**
**ReadingSessionNumber**
**TimeInReadingSession**
**LocationInText**
**$u_{Subject}$** indicates a random intercept of subject indicator
**$u_{Story}$** indicates a random intercept of story indicator
**$u_{ScreenSize}$** indicates a random intercept of screen size, measured by window width in pixels (which is affected by device and has an effect on how much text is visible)
**$\varepsilon_{i}$** indicates the residual variance of $Y_{i}$

First, we select the fixed effects in the model backward stepwise until the model converges. We then add two-way interactions between all predictors remaining in the model, and select them backward stepwise until the model converges. This model selection is used to explore possible interactions that are meaningful to add without overfitting the model.

### Create predictor variables


```{r}
linearity_measure_data$StartsNonlinearity_lag1 <- lag(linearity_measure_data$StartsNonlinearity, 1)
linearity_measure_data$StartsNonlinearity_lag2 <- lag(linearity_measure_data$StartsNonlinearity, 2)
```


### Visualise outcome variable and predictors

The linearity_measure_data has `r nrow(linearity_measure_data)` observations.

We create graphs to inspect linearity of reading and its connection to other variables.
Graphs on the distribution of StartsNonlinearity and connection to random effects were created before the reader characteristics model, and so they are not repeated here.


### Connection to previous event

We visualise connection to previous events to see if initiating nonlinearity on the previous page, or on the page before the previous, is connected to initiating linearity on the current page-view.

Previous page-view:

```{r}
table(linearity_measure_data$StartsNonlinearity, linearity_measure_data$StartsNonlinearity_lag1)
ggplot(
    linearity_measure_data,
    aes(StartsNonlinearity, ..count..)
) +
    geom_bar(aes(fill = StartsNonlinearity_lag1), position = "dodge") +
    theme_classic()
```

Two pages before the current page-view:

```{r}
table(linearity_measure_data$StartsNonlinearity, linearity_measure_data$StartsNonlinearity_lag1)
ggplot(linearity_measure_data, aes(StartsNonlinearity, ..count..)) +
    geom_bar(aes(fill = StartsNonlinearity_lag2), position = "dodge") +
    theme_classic()
```

### Reading session

Reading session number:

```{r}
ggplot(linearity_measure_data, aes(x = StartsNonlinearity, y = ReadingSessionNumber, fill = StartsNonlinearity)) +
    geom_violin() +
    geom_jitter(alpha = .1) +
    theme_classic()
```

Nonlinearity may be more common in early reading sessions.

Time in a reading session:

```{r}
ggplot(linearity_measure_data, aes(
    x = StartsNonlinearity,
    y = FirstCumulativeRSTime,
    fill = StartsNonlinearity
)) +
    geom_violin() +
    geom_jitter(alpha = .1) +
    theme_classic() +
    labs(y = "Time in a reading session (min)")
```

Similarly, nonlinearity may be more frequent at the beginning of reading sessions.

### Location in text

```{r}
linearity_measure_data$PercentageLocation <- (
    (linearity_measure_data$StartLocation
        / linearity_measure_data$CharacterLength)
)
```

```{r}
ggplot(linearity_measure_data, aes(
    x = StartsNonlinearity,
    y = (PercentageLocation) * 100,
    fill = StartsNonlinearity
)) +
    geom_violin() +
    geom_jitter(alpha = .1) +
    theme_classic() +
    labs(y = "Location (%)")
```

Nonlinearity may also be connected to the beginning of the text.

### Set contrasts to predictors

Predictors in the model are given contrasts. Continuous variables are scaled and centered, whereas categorical variables are given helmert contrasts. Helmert contrasts are used to control for uneven levels in categorical variables.

Load a function to create helmert contrasts for categorical variables:

```{r 'load helmert contrasts functions'}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.weighted.R"
    )
)
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.unweighted.R"
    )
)
```

Contrasts for categorical variables:

```{r 'categorical contrasts'}
# previous event (lag of 1)
linearity_measure_data$StartsNonlinearity_lag1 <- as.factor(linearity_measure_data$StartsNonlinearity_lag1)
contrasts(linearity_measure_data$StartsNonlinearity_lag1) <- contr.helmert.unweighted(linearity_measure_data$StartsNonlinearity_lag1)
contrasts(linearity_measure_data$StartsNonlinearity_lag1)
# two events back (lag of 2)
linearity_measure_data$StartsNonlinearity_lag2 <- as.factor(linearity_measure_data$StartsNonlinearity_lag2)
contrasts(linearity_measure_data$StartsNonlinearity_lag2) <- contr.helmert.unweighted(linearity_measure_data$StartsNonlinearity_lag2)
contrasts(linearity_measure_data$StartsNonlinearity_lag2)
```

Next, we scale and center continuous variables. These variables names are appended with ".cs" as a reminder that the variable is scaled and centered. Note that ReadingSessionNumber is treated as a continuous variable because we do not expect to see categorical differences between reading sessions, on the basis of the visualisations.

```{r 'scale continuous variables'}
# Days until reading deadline (control variable)
linearity_measure_data$DaysUntilDeadline.sc <-
    scale(linearity_measure_data$FirstTimeUntilDeadlineDays,
        center = TRUE,
        scale = TRUE
    )
# Reading session number
linearity_measure_data$ReadingSessionNumber.sc <-
    scale(linearity_measure_data$ReadingSessionNumber,
        center = TRUE,
        scale = TRUE
    )
# Time in a reading session
linearity_measure_data$TimeInReadingSession.sc <-
    scale(linearity_measure_data$FirstCumulativeRSTime,
        center = TRUE,
        scale = TRUE
    )
# Location in text
linearity_measure_data$PercentageLocation.sc <-
    scale(linearity_measure_data$PercentageLocation,
        center = TRUE,
        scale = TRUE
    )
```

### Modelling by Event Properties

#### Main effects

First, we build the model structure consisting of only main effects and random intercepts. Refer to the beginning of the event properties section of this script to see information on how the model is built.

The model includes 10 parameters before adding interactions (including intercept and the three random effects), and `r nrow(linearity_measure_data)` observations. Previous research has suggested that each parameter in a model has at least 20 observations. Our sample exceeds this threshold considering that `r nrow(linearity_measure_data)`/10 = `r round(nrow(linearity_measure_data)/10, 2)`

```{r}
AdditiveStructure_EP <- (
    "StartsNonlinearity ~ DaysUntilDeadline.sc + StartsNonlinearity_lag1 + StartsNonlinearity_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 | StoryId) + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Linearity_EP_AdditiveModel <- glmer(
    AdditiveStructure_EP,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

```{r}
summary(Linearity_EP_AdditiveModel)
plot(Linearity_EP_AdditiveModel)
```

Inspect model values:

```{r}
fixef(Linearity_EP_AdditiveModel)
```

The model with all main effects converges, and so we can add all two-way interactions.

#### Interactions

We create a new model structure with interactions between all predictors of interest (all main effects apart from DaysUntilDeadline which is used as a control variable only). Only two-way interactions are included to avoid overfitting the model. The interactive model includes 32 parameters (including the intercept, main effects, interactions and random effects). Previous research has suggested that each parameter in a model has at least 20 observations. Our sample exceeds this threshold considering that `r nrow(linearity_measure_data)`/32 = `r round(nrow(linearity_measure_data)/32, 2)`.

```{r}
InteractiveStructure_EP <- (
    "StartsNonlinearity ~ DaysUntilDeadline.sc + StartsNonlinearity_lag1 + StartsNonlinearity_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + StartsNonlinearity_lag1 : StartsNonlinearity_lag2 + StartsNonlinearity_lag1 : ReadingSessionNumber.sc + StartsNonlinearity_lag1 : TimeInReadingSession.sc + StartsNonlinearity_lag1 : PercentageLocation.sc +  StartsNonlinearity_lag2 : ReadingSessionNumber.sc + StartsNonlinearity_lag2 : TimeInReadingSession.sc + StartsNonlinearity_lag2 : PercentageLocation.sc + ReadingSessionNumber.sc : TimeInReadingSession.sc +  ReadingSessionNumber.sc : TimeInReadingSession.sc + ReadingSessionNumber.sc : PercentageLocation.sc + TimeInReadingSession.sc : PercentageLocation.sc + (1 | StoryId) + (1 | UserId) + (1 | WindowWidth)"
)

InteractiveStructure_EP2 <- ( # possible way to prior set interactions
    "StartsNonlinearity ~ DaysUntilDeadline.sc + StartsNonlinearity_lag1 + StartsNonlinearity_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + StartsNonlinearity_lag1 : StartsNonlinearity_lag2 + ReadingSessionNumber.sc : TimeInReadingSession.sc +  ReadingSessionNumber.sc : TimeInReadingSession.sc + ReadingSessionNumber.sc : PercentageLocation.sc + TimeInReadingSession.sc : PercentageLocation.sc + (1 | StoryId) + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Linearity_EP_InteractiveModel <- glmer(
    InteractiveStructure_EP2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

```{r}
summary(Linearity_EP_InteractiveModel)
plot(Linearity_EP_InteractiveModel)
```

Inspect model values:

```{r}
fixef(Linearity_EP_InteractiveModel)
```

The interactive model converges, and so there is no need for simplification of the model if the model assumptions are met.

### Model Assumptions

All generalised linear mixed models were tested for (1) multicollinearity, (2) under- and overdispersion, (3) influential observations, (4) heteroscedasticity, and (5) normality of random effects.

#### 1. Multicollinearity

Multicollinearity is checked by first inspecting a correlation matrix, and then computing Variance Inflation Factors (VIF) for all predictors.
Any VIF > 5 is considered to be a strong indication of multicollinearity, and thus those predictors should be removed. Any VIFs that are in the moderate range 1 < VIF < 5 are inspected before removing them. If removing the variable with a moderate VIF influences the model interpretation of predictors (not including the one that is removed), then the predictor should be removed from the model. 

We check for multicollinearity first with a correlation matrix:

```{r}
corrplot(cor(linearity_measure_data[, c("ReadingSessionNumber.sc", "TimeInReadingSession.sc", "PercentageLocation.sc")]), method = "number")
```

Location in text and time in a reading session are positively correlated, *r* = `r round(cor(linearity_measure_data$PercentageLocation.sc, linearity_measure_data$TimeInReadingSession.sc), 2)`.

To check if these correlations are problematic, we calculate VIF:

```{r}
check_collinearity(Linearity_EP_InteractiveModel)
plot(check_collinearity(Linearity_EP_InteractiveModel))
```

The 'check_collinearity' test from the 'performance' -package indicates a slightly inflated VIF value of 'StartsNonlinearity_lag1:StartsNonlinearity_lag2' (*VIF* = 3.82). Considering that the VIF is in the moderate range, we inspect whether removing the interaction has a significant effect on the model results interpretation (other than the removed predictor):

```{r}
SimplifiedStructure_EP <- str_remove(InteractiveStructure_EP, fixed("+ StartsNonlinearity_lag1 : StartsNonlinearity_lag2 "))
```

```{r}
Linearity_EP_InteractiveModel_MulticollinearityTest <- glmer(
    SimplifiedStructure_EP,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

Compare model results to original interactive model:

```{r}
summary(Linearity_EP_InteractiveModel_MulticollinearityTest)
summary(Linearity_EP_InteractiveModel)
```

Inclusion of the interaction has a slight impact on model results, and therefore, the interaction effect should be removed.

Multicollinearity is tested again after removal of "StartsNonlinearity_lag1 : StartsNonlinearity_lag2":

```{r}
check_collinearity(Linearity_EP_InteractiveModel_MulticollinearityTest)
plot(check_collinearity(Linearity_EP_InteractiveModel_MulticollinearityTest))
```

The test indicates that an interaction effect between 'StartsNonlinearity_lag1' and 'ReadingSessionNumber.sc' has a moderate VIF (*VIF* = 2.86). Again, the impact of removal is inspected:

```{r}
SimplifiedStructure_EP <- str_remove(SimplifiedStructure_EP, fixed("+ StartsNonlinearity_lag1 : ReadingSessionNumber.sc "))
```

```{r}
Linearity_EP_InteractiveModel_MulticollinearityTest2 <- glmer(
    SimplifiedStructure_EP,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

Compare model results to original interactive model:

```{r}
summary(Linearity_EP_InteractiveModel_MulticollinearityTest2)
summary(Linearity_EP_InteractiveModel_MulticollinearityTest)
```

Inclusion of the interaction has a slight impact on model results, and therefore, the interaction effect should be removed.

Multicollinearity is tested again after removal of "StartsNonlinearity_lag1 : ReadingSessionNumber.sc":

```{r}
check_collinearity(Linearity_EP_InteractiveModel_MulticollinearityTest2)
plot(check_collinearity(Linearity_EP_InteractiveModel_MulticollinearityTest2))
```

The test indicates that an interaction effect between 'StartsNonlinearity_lag1' and 'TimeInReadingSession.sc' has a moderate VIF (*VIF* = 3.90). Again, the impact of removal is inspected:

```{r}
SimplifiedStructure_EP <- str_remove(SimplifiedStructure_EP, fixed("+ StartsNonlinearity_lag1 : TimeInReadingSession.sc "))
```

```{r}
Linearity_EP_InteractiveModel_MulticollinearityTest3 <- glmer(
    SimplifiedStructure_EP,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

Compare model results to original interactive model:

```{r}
summary(Linearity_EP_InteractiveModel_MulticollinearityTest3)
summary(Linearity_EP_InteractiveModel_MulticollinearityTest2)
```

Inclusion of the interaction has a slight impact on model results, and therefore, the interaction effect should be removed.
Multicollinearity is tested again after removal of "StartsNonlinearity_lag1 : TimeInReadingSession.sc":

```{r}
check_collinearity(Linearity_EP_InteractiveModel_MulticollinearityTest3)
plot(check_collinearity(Linearity_EP_InteractiveModel_MulticollinearityTest3))
```

The test indicates low correlation between predictors, and so the model is now expected to align to the assumption of not having multicollinearity. We use Linearity_EP_InteractiveModel_MulticollinearityTest3 in the following assumptions checks.

#### 2. Under- and Overdispersion

Under- and overdispersion occur when the data shows less or more variation, respectively, than what we would expect. We test dispersion with simulation-based tests from DHARMa:

```{r}
set.seed(1234) # for reproducibility
simulated_model_output <- simulateResiduals(fittedModel = Linearity_EP_InteractiveModel, plot = F)
```

```{r}
plotQQunif(simulated_model_output)
```

The model does not deviate from the expected distribution according to the qqplot.

```{r}
testDispersion(simulated_model_output)
```

The dispersion test is not significant, indicating that the model is not affected by under- or overdispersion.

#### 3. Influential Observations

We test influential observations with Cook's Distance:

```{r}
cooksd <-
    cooks.distance(Linearity_EP_InteractiveModel)
plot(cooksd,
    pch = "*",
    cex = 2,
    main = "Influential Obs by Cooks Distance"
)
abline(h = 4 * mean(cooksd, na.rm = TRUE), col = "red")
text(
    x = 1:length(cooksd) + 1,
    y = cooksd,
    labels = ifelse(cooksd > 4 * mean(cooksd, na.rm = TRUE), names(cooksd), ""),
    col = "red"
)
```

The plot indicates that the dataset may include some highly influential observations. In particular, the plot highlights observations 254 and 60.

We further inspect outliers using the simulated_model_output and DHARMa's outlier test:

```{r}
testOutliers(Linearity_EP_InteractiveModel)
```

Although Cook's Distance indicates that the model has influential observations, the outlier test does not produce the same results. In order to assess the impact of the influential points indicated by Cook's Distance, we re-fit the model twice  without each influential observation to see if the results are altered.

**Fit the model without observation #254:**

```{r}
subset_without_observation254 <- linearity_measure_data[c(1:253, 255:nrow(linearity_measure_data)), ]
```

```{r}
Linearity_EP_InteractiveModel_influential254 <- glmer(
    SimplifiedStructure_EP,
    data = subset_without_observation254,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

Inspect if model results are affected:

```{r}
summary(Linearity_EP_InteractiveModel_influential254)
summary(Linearity_EP_InteractiveModel_MulticollinearityTest3)
```

Removing observation #254 has an impact on model results. Removing the value results in a new significant interaction effect (ReadingSessionNumber and TimeInReadingSession). Therefore, the observation affects model interpretation and it should be removed from analyses.

Assess influential observations after removing point 254:

```{r}
cooksd <-
    cooks.distance(Linearity_EP_InteractiveModel_influential254)
plot(cooksd,
    pch = "*",
    cex = 2,
    main = "Influential Obs by Cooks Distance"
)
abline(h = 4 * mean(cooksd, na.rm = TRUE), col = "red")
text(
    x = 1:length(cooksd) + 1,
    y = cooksd,
    labels = ifelse(cooksd > 4 * mean(cooksd, na.rm = TRUE), names(cooksd), ""),
    col = "red"
)
```

The plot indicates that the dataset still includes some influential observations. In particular, the plot highlights observation 60.

**Fit the model without observation #60:**

```{r}
subset_without_observations254and60 <- subset_without_observation254[c(1:59, 61:nrow(subset_without_observation254)), ]
```

```{r}
Linearity_EP_InteractiveModel_influential60and254 <- glmer(
    SimplifiedStructure_EP,
    data = subset_without_observations254and60,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

Inspect if model results are affected:

```{r}
summary(Linearity_EP_InteractiveModel_influential60and254)
summary(Linearity_EP_InteractiveModel_influential254)
```

Removing observation #60 has an impact on model results. Removing the value results in a new significant interaction effect (StartsNonlinearity_lag2 and TimeInReadingSession). Therefore, the observation affects model interpretation and it should be removed from analyses.

Assess influential observations after removing points 254 and 60:

```{r}
cooksd <-
    cooks.distance(Linearity_EP_InteractiveModel_influential60and254)
plot(cooksd,
    pch = "*",
    cex = 2,
    main = "Influential Obs by Cooks Distance"
)
abline(h = 4 * mean(cooksd, na.rm = TRUE), col = "red")
text(
    x = 1:length(cooksd) + 1,
    y = cooksd,
    labels = ifelse(cooksd > 4 * mean(cooksd, na.rm = TRUE), names(cooksd), ""),
    col = "red"
)
```

The plot indicates that the dataset still includes some influential observations. In particular, the plot highlights observation 263.

**Fit the model without observation #263:**

```{r}
subset_without_3observations <- subset_without_observations254and60[c(1:262, 264:nrow(subset_without_observations254and60)), ]
```

```{r}
Linearity_EP_InteractiveModel_3InfluentialPoints <- glmer(
    SimplifiedStructure_EP,
    data = subset_without_3observations,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

Inspect if model results are affected:

```{r}
summary(Linearity_EP_InteractiveModel_3InfluentialPoints)
summary(Linearity_EP_InteractiveModel_influential60and254)
```

Removing observation #263 has no impact on model interpretation, and thus it is retained in the model and it is not considered to be influential.

In following assumption checks we use Linearity_EP_InteractiveModel_influential60and254 that has two influential observations removed.

#### 4. Heteroscedasticity

Create a new simulated_model_output to test for heteroscedasticity:

```{r}
set.seed(1234) # for reproducibility
simulated_model_output <- simulateResiduals(fittedModel = Linearity_EP_InteractiveModel_influential60and254, plot = F)
```

```{r}
testQuantiles(simulated_model_output)
```

The quantile test for heteroscedasticity is significant, indicating that the model needs to be adjusted.

#### 5. Normality of Random Effects

Random effects should be normally distributed in multilevel models. We test this assumption by inspecting first the normality of 'UserId' and then 'StoryId', and finally 'WindowWidth' random intercepts.

```{r}
random_intercept_UserId <- ranef(Linearity_EP_InteractiveModel)$UserId$`(Intercept)`
qqnorm(random_intercept_UserId)
qqline(random_intercept_UserId)
shapiro.test(random_intercept_UserId)
```

The qqplot varies from the reference line, but considering that the variance is not extensive and the Shapiro Test is not significant, we assume that UserId random intercept is normally distributed.

```{r}
random_intercept_StoryId <- ranef(Linearity_EP_InteractiveModel)$StoryId$`(Intercept)`
qqnorm(random_intercept_StoryId)
qqline(random_intercept_StoryId)
shapiro.test(random_intercept_StoryId)
```

StoryId aligns much worse with the reference line, however, this is expected considering that StoryId only includes 9 different story groups. With the limited amount of information, achieving a visually normally distributed result is unlikely. Considering that the Shapiro Test is not significant, we can assume that StoryId random intercept is normally distributed.

```{r}
random_intercept_WindowWidth <- ranef(Linearity_EP_InteractiveModel)$WindowWidth$`(Intercept)`
qqnorm(random_intercept_WindowWidth)
qqline(random_intercept_WindowWidth)
shapiro.test(random_intercept_WindowWidth)
```

Again, the qqplot indicates that WindowWidth may vary from normality, however, considering that the variance is not extensive and the Shapiro Test is not significant, we assume that WindowWidth random intercept is normally distributed.

**Assumptions conclusion**

Two influential observations were found in assumption testing. These observations are removed from the model dataset and the model is re-fit.

```{r}
subset_without_observation254and263 <- linearity_measure_data[c(1:253, 255:262, 264:nrow(linearity_measure_data)), ]
```

```{r}
Linearity_EP_InteractiveModel_influentialsremoved <- glmer(
    InteractiveStructure_EP,
    data = subset_without_observation254and263,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

**Re-test the model assumptions**

The new model is tested for assumptions.

1. Multicollinearity:

```{r}
check_collinearity(Linearity_EP_InteractiveModel_influentialsremoved)
plot(check_collinearity(Linearity_EP_InteractiveModel_influentialsremoved))
```

The collinearity test indicates that three different interaction effects may be collinear in the model.
We test the impact of removing each of these one-by-one:

```{r}
SimplifiedStructure_EP <- str_remove(InteractiveStructure_EP, fixed("+ StartsNonlinearity_lag1 : StartsNonlinearity_lag2 "))
```

```{r}
Linearity_EP_InteractiveModel_influentialsremoved_multicollinearity1 <- glmer(
    SimplifiedStructure_EP,
    data = subset_without_observation254and263,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

```{r}
anova(Linearity_EP_InteractiveModel_influentialsremoved, Linearity_EP_InteractiveModel_influentialsremoved_multicollinearity1)
```

Removing the interaction effect between 'StartsNonlinearity_lag1' and 'StartsNonlinearity_lag2' does not significantly change model results, and so the interaction is retained in the model.

```{r}
SimplifiedStructure_EP <- str_remove(InteractiveStructure_EP, fixed("+ StartsNonlinearity_lag1 : ReadingSessionNumber.sc "))
```

```{r}
Linearity_EP_InteractiveModel_influentialsremoved_multicollinearity2 <- glmer(
    SimplifiedStructure_EP,
    data = subset_without_observation254and263,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

```{r}
anova(Linearity_EP_InteractiveModel_influentialsremoved, Linearity_EP_InteractiveModel_influentialsremoved_multicollinearity2)
```

Removing the interaction effect between 'StartsNonlinearity_lag1' and 'ReadingSessionNumber.sc' does not significantly change model results, and so the interaction is retained in the model.

```{r}
SimplifiedStructure_EP <- str_remove(InteractiveStructure_EP, fixed("+ StartsNonlinearity_lag2 : TimeInReadingSession.sc "))
```

```{r}
Linearity_EP_InteractiveModel_influentialsremoved_multicollinearity3 <- glmer(
    SimplifiedStructure_EP,
    data = subset_without_observation254and263,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

```{r}
anova(Linearity_EP_InteractiveModel_influentialsremoved, Linearity_EP_InteractiveModel_influentialsremoved_multicollinearity3)
```

Removing the interaction effect between 'StartsNonlinearity_lag1' and 'TimeInReadingSession.sc' does have a significant effect on the model results. As a result, 

As a result, 

lag1 : per affects model results

```{r}
anova(Linearity_EP_InteractiveModel_influentialsremoved, Linearity_EP_InteractiveModel_influentialsremoved2)
```

sig in anova too

```{r}

```

```{r 'all assumptions with updated model'}
plot(check_collinearity(Linearity_EP_InteractiveModel_influentialsremoved2))
set.seed(1234)
simulated_model_output <- simulateResiduals(fittedModel = Linearity_EP_InteractiveModel_influentialsremoved, plot = F)
plotQQunif(simulated_model_output)
testDispersion(simulated_model_output)
testQuantiles(simulated_model_output)
testOutliers(simulated_model_output)
random_intercept_UserId <- ranef(Linearity_EP_InteractiveModel_influentialsremoved)$UserId$`(Intercept)`
shapiro.test(random_intercept_UserId)
random_intercept_StoryId <- ranef(Linearity_EP_InteractiveModel_influentialsremoved)$StoryId$`(Intercept)`
shapiro.test(random_intercept_StoryId)
random_intercept_WindowWidth <- ranef(Linearity_EP_InteractiveModel_influentialsremoved)$WindowWidth$`(Intercept)`
shapiro.test(random_intercept_WindowWidth)
```

### Interpret reader characteristics model

We then interpret model effects to interpret the results.

```{r}
summary(Linearity_EP_InteractiveModel_influential254)
```

Linearity of reading is significantly predicted by
1. A main effect of previous event (lag 1)
2. A main effect of time in a reading session
3. An interaction between previous event (lag 1) and location in text
4. An interaction between event preceeding previous (lag 2) and time in a reading session
5. An interaction between reading session number and location in text
6. An interaction between time in a reading session and location in text

These findings are explored below by visualisations.
First, we discuss the effect of previous events (1, 3, and 4). Secondly, we discuss the influence of reading session and location (2, 5, and 6)

Note that no hypotheses were set for the event properties models and they are used for explorative purposes only.

#### Connection between subsequent events

The results of the model inidcated that previous event (lag 1) was a significant negative predictor of the odds of a nonlinear navigation being initiated.

```{r}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_AddLine.R"
    )
)
linearity_effectplot_previousevent <- effect_plot(
    Linearity_EP_InteractiveModel_influential254,
    pred = "StartsNonlinearity_lag1",
    data =  subset_without_observation254,
    interval = TRUE
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Whether previous event initiated,nonlinear navigation or not")
    )
```
 
The result indicates that participants were more likely to initiate nonlinear navigation if the previous event did not initiate nonlinearity. In other words, participants were unlikely to move between different types of nonlinear navigation (e.g. from a regression to a forward leap) in consecutive page-views.

This main effect is affected by a significant interaction between the event on page(k-1) and location in text.

```{r}
subset_without_observation254$StartsNonlinearity_lag1 <- as.factor(subset_without_observation254$StartsNonlinearity_lag1)

linearity_effectplot_previouseventlocation <- interact_plot(
    Linearity_EP_InteractiveModel_influential254,
    pred = "PercentageLocation.sc",
    modx = "StartsNonlinearity_lag1",
    data =  subset_without_observation254,
    interval = TRUE,
    legend.main = addline_format("Whether previous event initiated,nonlinear navigation or not")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Location in text,(centered and scaled)")
    )
```

The interaction effect indicates that the odds of a participant initiating a nonlinear navigation event on page(k) following an event on page(k-1) that did not initiate nonlinearity remained roughly the same regardless of text location. However, when the event on page(k-1) initiated nonlinear navigation, the odds of nonlinear navigation being initiated again on page(k) increased towards the end of the text. If consecutive page-views initiate nonlinearity, the participant is likely to be using both regressions and forward leaps to move in the text. Therefore, participants alternated more in their navigation habits towards the end of the text, compared to the beginning.

In addition to the event on page(k-1), the event in page(k-2) was a significant predictor of nonlinearity in interaction with time in a reading session.

```{r}
linearity_effectplot_lag2time <- interact_plot(
    Linearity_EP_InteractiveModel_influential254,
    pred = "TimeInReadingSession.sc",
    modx = "StartsNonlinearity_lag2",
    interval = TRUE,
    legend.main = addline_format("Whether nonlinear navigation was initiated,in the event preceeding the previous or not")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Location in text,(centered and scaled)")
    )
```

The interaction effect indicates that the odds of a participant initiating nonlinear navigation in page(k) following a similar event in page(k-2) remained constant across the text. However, if the event in page(k-2) did not start a new nonlinear navigation event, the odds of nonlinear navigation being initiated in page(k) decreased towards the end of the text. This indicates that 




a nonlinear navigation event that was initiated in the page-view preceeding the previous page-view (lag 2) remained constant regardless of participants' location in text. However, if the event two page-views apart from the current did not include a initiation of nonlinear navigation, the odds of initiating nonlinear navigation in the current event decreased towards the end of the text. In other words, participants were equally likely to initiate nonlinearity at the beginning of the text despite of previous events' nonlinearity, however, the likelihood of initiating nonlinearity became increasingly unlikely if the event


**H4a: We expect situational motivation (high-autonomy condition) to be a negative predictor of nonlinear navigation frequency when situational competence is high.**

This hypothesis was not supported as the interaction effect between Condition and situational competence was removed from the model during backward stepwise selection. Indeed, the main effect of condition was similarly removed, and therefore, participants in the high-autonomy and low-autonomy conditions did not differ significantly in their frequency of using nonlinear navigation during reading of the short story.

**H4b: We expect contextual motivation to be a negative predictor of nonlinear navigation frequency when situational competence is high.**

This hypothesis was not supported as the interaction between contextual motivation ('CInterest') and situational competence (SCompetence) was not a significant predictor of nonlinear navigation.

```{r}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_AddLine.R"
    )
)

linearity_effectplot_cmotscomp <- interact_plot(
    Linearity_ModelSelection5,
    pred = "CInterest.sc",
    modx = "SCompetence.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Situation competence score,(centered and scaled)")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Contextual motivation score, measured by contextual reading interest,(centered and scaled)")
    )
```

Although the hypothesis was not supported, the trend of the results is in the same direction as we predicted: participants with a high contextual motivation score initiated nonlinear navigation slightly less often when they found the text easy to read (high situational competence score). In contrast, a low contextual motivation score and high situational competence is associated with more frequent nonlinear navigation.

However, the interaction was not significant and so the effect should not be interpreted further.

**H4c: We expect an interaction between task-relevant electronic experience measures to be a negative predictor of nonlinear navigation frequency when situational competence is high.**

This hypothesis was not supported as an interaction between the two electronic experience measures and situational competence was not a significant predictor of linearity of reading.

```{r}
linearity_effectplot_eexpscomp <- interact_plot(
    Linearity_ModelSelection5,
    pred = "Eexp1LongformEFreq.sc",
    modx = "Eexp2DeviceRecFreq.sc",
    mod2 = "SCompetence.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Electronic experience measure 2: Device*"),
    mod2.labels = c("-1SD Situational competence", "Mean of Situational Competence", "+1SD Situational Competence")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Electronic experience measure 1, frequency of e-reading task-relevant text types,(centered and scaled)")
    )
```

Indeed, the effect of electronic experience was the same, despite of situational competence.

Although the three-way interaction between electronic experience measures and situational competence did not significantly predict linearity of reading, a two-way interaction between the electronic experience measures was a significant predictor in the model.

```{r}
linearity_effectplot_eexp <- interact_plot(
    Linearity_ModelSelection5,
    pred = "Eexp1LongformEFreq.sc",
    modx = "Eexp2DeviceRecFreq.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Electronic experience measure 2: Device*")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Electronic experience measure 1, frequency of e-reading task-relevant text types,(centered and scaled)")
    )
```

The finding indicated that a combination of high scores in the two task-relevant electronic reading experience measures was associated with lower likelihood of nonlinear navigation. In contrast, a low score in either measure with a high level of the other, was associated with more frequent nonlinear navigation. This indicates that electronic experience affected participants' linearity of reading.

