---
title: "Analysis_ReadingLinearity"
author: "Pauliina Vuorinen"
date: "27/06/2022-"
output: html_document
library: "~/Extra/RPackages.bib"
---

## Introduction

The purpose of this script is to analyse how linearly participants' read the short story during the study.
We aim to assess how frequently participants' used nonlinearity during the study, how nonlinearity occurred, and whether autonomy condition, contextual reading motivation, and electronic reading experience are connected to this nonlinearity. See the full article for details.

To answer these research questions, we use 'StartsNonlinearity' (saved in Data as linearity_measure_data.csv) as a measure of the frequency of nonlinearity. This measure was computed in Prep_ReadingLinearityMeasure.rmd, and it tells us whether nonlinear navigation was initiated on a page-view or not. Therefore, StartsNonlinearity is TRUE when a participant starts a regression or a forward leap on a page-view and the previous page-view did not include similar nonlinearity (see Prep_TrackingDataWrangling.rmd in Prep for more detail).

In this analysis script, StartsNonlinearity is used as an outcome variable in **two different multilevel models**. In the first model, we study whether reader charactersistics such as condition, contextual motivation, and electronic reading experience predict StartsNonlinearity. In the second model, the dependent variables include information on the timing and location of StartsNonlinearity. With the latter model our intention is to study in which situations nonlinearity occurs. Both model structures are based on selection outlined in the full article. The models are constructed using glm() in the lme4-package by @bates_fitting_2015.

**Information on the hypotheses**

* H6a: Situational autonomous motivation is connected to infrequent nonlinearity when task competence is high.
* H6b: Contextual autonomous motivation is connected to infrequent nonlinearity when task competence is high.
* H6c: Task-relevant electronic experience is connected to infrequent nonlinearity when task competence is high.

## Setup

```{r, setup}
library(tidyverse)
library(dplyr)
library(psych)
library(gridExtra)
library(corrplot)
library(lme4)
library(lmerTest)
library(DHARMa)
library(performance)
library(car)
library(effects)
library(interactions)
library(jtools)
getwd() # working directory should be ~/Short_Story_Reading_Behaviour_Public
```

Save working directory so that this script can be used elsewhere, if required. The working directory should be "~/Short-Story-Reading-Behaviour-Public/". If the working directory is not correct, we save the correct path and use that in loading files. In our purposes, the foulder could be found from /Documents/GITHUB/Short_Story_Reading_Behaviour_Public.

The working directory is not changed with setwd() because this script is knit remotely in other scripts.

```{r 'working directory for my purposes'}
mypath_SSRBP <- getwd()
if (!grepl("Short_Story_Reading_Behaviour_Public", mypath_SSRBP, fixed = TRUE)) {
    # wrong working directory
    if (!grepl("GITHUB", dirname(mypath_SSRBP), fixed = TRUE)) {
        # directory name isn't GITHUB, unlike I would expect
        if (grepl("GITHUB", mypath_SSRBP, fixed = TRUE)) {
            # GITHUB is in the path
            ## use mypath_SSRBP instead of dirname()
            mypath_SSRBP <- paste0(
                mypath_SSRBP,
                "/Short_Story_Reading_Behaviour_Public"
            )
        }
    } else {
        # directory name is GITHUB
        # save correct working directory
        mypath_SSRBP <- paste0(
            dirname(mypath_SSRBP),
            "/Short_Story_Reading_Behaviour_Public"
        )
    }
}
```

## Load data and check variable types

```{r, load data}
# load data
linearity_measure_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/linearity_measure_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
linearity_measure_data <- dplyr::select(linearity_measure_data, -X) # remove row numbers
```

We then load in data from questionnaires:

```{r 'load predictors: questionnaires'}
# IMI
IMI_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/IMI_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
IMI_scores <- dplyr::select(IMI_scores, -X) # remove row numbers
# IMI-R
IMIR_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/IMIR_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
IMIR_scores <- dplyr::select(IMIR_scores, -X) # remove row numbers
# Electronic experience
eexp_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/eexp_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
eexp_scores <- dplyr::select(eexp_scores, -X) # remove row numbers
# demographic information
demographics_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/demographics_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
demographics_data <- dplyr::select(demographics_data, -X) # remove row numbers
```

Load in information about the stories that participants read:

```{r}
story_information_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/story_information_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
story_information_data <- dplyr::select(story_information_data, -X) # remove row numbers
```

We then merge these dataframes together:

```{r 'merge dfs'}
# measure and IMI
linearity_measure_data <- merge(
    linearity_measure_data,
    IMI_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and IMI-R
linearity_measure_data <- merge(
    linearity_measure_data,
    IMIR_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and EEXP
linearity_measure_data <- merge(
    linearity_measure_data,
    eexp_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and demographic information
linearity_measure_data <- merge(
    linearity_measure_data,
    demographics_data,
    all.x = TRUE,
    by = "UserId"
)
# measure and information about stories
linearity_measure_data <- merge(
    linearity_measure_data,
    story_information_data[, c(1:3, 10)],
    all.x = TRUE,
    by = "StoryId"
)
```

## Change variable types

```{r 'change variable types'}
str(linearity_measure_data)
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_VariableTypeConversion.R"
    )
)

## turn columns into factors that should be factors
linearity_measure_data[, c(
    "UserId",
    "StoryId"
)] <- convert.magic(
    linearity_measure_data[, c(
        "UserId",
        "StoryId"
    )],
    "factor"
)
```

## Reader characteristics model

The reader characteristics model of linearity aims to address the following hypotheses:

* H6a: Situational autonomous motivation is connected to infrequent nonlinearity when task competence is high.
* H6b: Contextual autonomous motivation is connected to infrequent nonlinearity when task competence is high.
* H6c: Task-relevant electronic experience is connected to infrequent nonlinearity when task competence is high.

The reader characteristics model for linearity of reading takes the following structure:

$Y_{i} = 
    \beta_{0} +
    DUD +
    Native +
    Cond +
    SComp +
    CMot +
    CComp +
    EExp1 +
    EExp2 +
    Cond x SComp +
    CMot x SComp +
    EExp1 x EExp2 +
    EExp1 x EExp2 x SComp +
    u_{Subject} +
    u_{Story} +
    u_{ScreenSize} +
    \varepsilon_{i}$ 

where
**$Y_{i}$** is the reading behaviour measure, in this case StartsNonlinearity for reading linearity.
**$\beta_{0}$** is the intercept
**DUD** is an acronym for 'DaysUntilDeadline' which tells us how much the participant has time left to read the short story. Days until deadline is used to control for variance in reading behaviour as a result of pressure to read the short story in time. For example, participants may read the text differently closer to the deadline compared to the beginning of the study.
**Native** is a binary variable indicating whether the participant is a native speaker of English or not (responses - Yes/No, yes indicating that the participant is a native speaker).
**Cond** represents autonomy condition (as a measure of situational motivation)
**SComp** is participants' situational competence (perception of competence to read the story), measured by the subcomponent of 'competence' from the IMI questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**CMot** is participants' contextual reading motivation, measured by the subcomponent of contextual interest from the IMI-R questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**CComp** is participants' contextual competence (perception of general reading ability), measured by the subcomponent of 'competence' from the IMI-R questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**EExp1** is frequency of using any electronic devices for long-form text reading purposes
**EExp2** is frequency of using task-relevant devices for any reading purpose
**$u_{Subject}$** indicates a random intercept of subject indicator
**$u_{Story}$** indicates a random intercept of story indicator
**$u_{ScreenSize}$** indicates a random intercept of screen size, measured by window width in pixels (which is affected by device and has an effect on how much text is visible)
**$\varepsilon_{i}$** indicates the residual variance of $Y_{i}$

Whereas linearity of reading and reading speed are analysed with this reader characteristics structure, the remaining models (task switching and reading persistence) are modelled without 'Cond x SComp', 'CMot x SComp', and 'EExp1 x EExp2 x SComp'. This is because linearity and speed are expected to be connected to motivation and electronic experience via interaction with situational competence, whereas task switching and reading persistence are expected to be connected to motivation and electronic experience directly (main effects).

### Visualise outcome variable and predictors

The linearity_measure_data has `r nrow(linearity_measure_data)` observations.

We create graphs to inspect linearity of reading and its connection to other variables.

The linearity measure, StartsNonlinearity, is a binary variable (whether each event initiates nonlinearity or not, TRUE/FALSE). To visualise it, we create counts:

```{r}
linearity_measure_count <- linearity_measure_data %>%
    group_by(
        UserId,
        StoryId,
        CharacterLength,
        Gender,
        Age,
        NativeEnglish,
        Condition,
        SCompetence,
        CInterest,
        CCompetence,
        Eexp1LongformEFreq,
        Eexp2DeviceRecFreq,
        StartsNonlinearity
    ) %>%
    summarise(
        Count = n()
    )
```

```{r}
ggplot(linearity_measure_count, aes(x = StartsNonlinearity, y = Count)) +
    geom_violin() +
    geom_jitter() +
    theme_classic()
```

Variation between participants:

```{r}
ggplot(linearity_measure_count, aes(x = UserId, y = Count, colour = StartsNonlinearity)) +
    geom_point() +
    theme_classic()
```

Variation between different stories read:

```{r}
ggplot(linearity_measure_count, aes(x = StoryId, y = Count, colour = StartsNonlinearity)) +
    geom_boxplot() +
    theme_classic()
```

We correct for story length to see if there are differences between different books read:

```{r}
ggplot(
    linearity_measure_count,
    aes(x = StoryId, y = (Count / CharacterLength), colour = StartsNonlinearity)
) +
    geom_boxplot() +
    geom_jitter() +
    theme_classic() +
    ylab("Count of StartsNonlinearity/Story length")
```

The impact of window width (size of device and amount of text visible):

```{r}
ggplot(
    linearity_measure_data,
    aes(x = StartsNonlinearity, y = WindowWidth, colour = UserId)
) +
    geom_jitter(size = 2) +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

We then visualise the effect of DaysUntilDeadline:

```{r}
ggplot(
    linearity_measure_data,
    aes(x = StartsNonlinearity, y = FirstTimeUntilDeadlineDays, colour = UserId)
) +
    geom_jitter(size = 2) +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```


### Linearity of Reading and Demographic Information

Gender:

```{r}
ggplot(linearity_measure_count, aes(x = Gender, y = Count, colour = StartsNonlinearity)) +
    geom_violin() +
    geom_jitter() +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

Age:

```{r}
ggplot(linearity_measure_count, aes(x = Age, y = Count, colour = StartsNonlinearity)) +
    geom_point() +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

Whether participants' native language is English or not:

```{r}
ggplot(linearity_measure_count, aes(x = NativeEnglish, y = Count, colour = StartsNonlinearity)) +
    geom_violin() +
    geom_jitter() +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

### Task switching and condition

```{r}
ggplot(linearity_measure_count, aes(x = Condition, y = Count, colour = StartsNonlinearity)) +
    geom_boxplot() +
    theme_classic()
```

### Motivation

Only situational competence is included in the model from the IMI variables, considering that condition is significantly connected to both situational autonomy and interest.

Situational competence:

```{r}
ggplot(linearity_measure_count, aes(x = SCompetence, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

We then create visualisations for contextual measures of motivation: interest, competence, and effort.

Contextual interest:

```{r}
ggplot(linearity_measure_count, aes(x = CInterest, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

Contextual competence:

```{r}
ggplot(linearity_measure_count, aes(x = CCompetence, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

### Linearity of reading and electronic reading experience

We are interested in how task-relevant electronic experience is connected to linearity.
Electronic experience was measured by two variables: Eexp1LongFormEFreq - the frequency of reading long-form, narrative texts electronnically, and Eexp2DeviceRecFreq - the frequency of using task-relevant digital devices for recreational reading purposes.

Eexp1LongformEFreq (longfrom reading electronically):

```{r}
ggplot(linearity_measure_count, aes(x = Eexp1LongformEFreq, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

Eexp2DeviceRecFreq (task-relevant digital devices for recreational reading):

```{r}
ggplot(linearity_measure_count, aes(x = Eexp2DeviceRecFreq, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

### Set contrasts to predictors

Predictors in the model are given contrasts. Continuous variables are scaled and centered, whereas categorical variables are given helmert contrasts. Helmert contrasts are used to control for uneven levels in categorical variables.

Load a function to create helmert contrasts for categorical variables:

```{r 'load helmert contrasts functions'}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.weighted.R"
    )
)
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.unweighted.R"
    )
)
```

Contrasts for categorical variables:

```{r 'categorical contrasts'}
# whether native language is English or not
linearity_measure_data$NativeEnglish <- as.factor(linearity_measure_data$NativeEnglish)
contrasts(linearity_measure_data$NativeEnglish) <- contr.helmert.unweighted(linearity_measure_data$NativeEnglish)
contrasts(linearity_measure_data$NativeEnglish)
# condition
linearity_measure_data$Condition <- dplyr::recode(
    linearity_measure_data$Condition,
    "AutonomousCondition" = "HighAutonomyCondition",
    "NonAutonomousCondition" = "LowAutonomyCondition"
)
linearity_measure_data$Condition <- as.factor(linearity_measure_data$Condition)
contrasts(linearity_measure_data$Condition) <- contr.helmert.unweighted(
    linearity_measure_data$Condition
)
contrasts(linearity_measure_data$Condition)
```

Next, we scale and center continuous variables. These variables names are appended with ".cs" as a reminder that the variable is scaled and centered.

```{r 'scale continuous variables'}
# Situational competence
linearity_measure_data$SCompetence.sc <-
    scale(linearity_measure_data$SCompetence,
        center = TRUE,
        scale = TRUE
    )
# Contextual motivation
linearity_measure_data$CInterest.sc <-
    scale(linearity_measure_data$CInterest,
        center = TRUE,
        scale = TRUE
    )
linearity_measure_data$CCompetence.sc <-
    scale(linearity_measure_data$CCompetence,
        center = TRUE,
        scale = TRUE
    )
# Electronic reading experience
linearity_measure_data$Eexp1LongformEFreq.sc <-
    scale(linearity_measure_data$Eexp1LongformEFreq,
        center = TRUE,
        scale = TRUE
    )
linearity_measure_data$Eexp2DeviceRecFreq.sc <-
    scale(linearity_measure_data$Eexp2DeviceRecFreq,
        center = TRUE,
        scale = TRUE
    )
# Days until reading deadline
linearity_measure_data$DaysUntilDeadline.sc <-
    scale(linearity_measure_data$FirstTimeUntilDeadlineDays,
        center = TRUE,
        scale = TRUE
    )
```

### Modelling by Reader Characteristics

First, we build the full structure. Refer to the beginning of this script to see information on the full structure. The full model includes 20 parameters (including two random effects and categorical variable levels), and `r nrow(linearity_measure_data)` observations. Previous research has suggested that each parameter in a model has at least 20 observations. Our sample exceeds this threshold considering that `r nrow(linearity_measure_data)`/20 = `r round(nrow(linearity_measure_data)/20, 2)`

```{r}
FullStructure <- (
    "StartsNonlinearity ~ DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | StoryId) + (1 | UserId) + (1 | WindowWidth)"
)
```

```{r}
Linearity_FullModel <- glmer(
    FullStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The full model is singular, and so it needs to be simplified.

```{r}
summary(Linearity_FullModel)
plot(Linearity_FullModel)
```

Inspect model values:

```{r}
fixef(Linearity_FullModel)
```

### Model backward selection

The fullmodel is singular, and thus its structure needs to be simplified. We remove predictors by backward stepwise selection, until convergence. The predictors with the lowest p-value is removed first.

Find lowest p-value:
```{r}
summary(Linearity_FullModel)
```

'DaysUntilDeadline.sc' has the lowest p-value and thus contributes the least to the model. It is removed:

```{r}
SimplifiedStructure <- str_remove(FullStructure, fixed("DaysUntilDeadline.sc + "))
```

re-fit the model with the simplified structure:

```{r}
Linearity_ModelSelection1 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Linearity_ModelSelection1)
plot(Linearity_ModelSelection1)
```

'Condition:SCompetence.sc' has the lowest p-value and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("+  Condition : SCompetence.sc "))
```

Re-fit the model:

```{r}
Linearity_ModelSelection2 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Linearity_ModelSelection2)
plot(Linearity_ModelSelection2)
```

'CCompetence.sc' has the lowest p-value and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("+ CCompetence.sc "))
```

Re-fit the model:

```{r}
Linearity_ModelSelection3 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Linearity_ModelSelection3)
plot(Linearity_ModelSelection3)
```

'NativeEnglish' has the lowest p-value and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("NativeEnglish + "))
```

Re-fit the model:

```{r}
Linearity_ModelSelection4 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model remains singular, and so we identify another predictor to be removed:

```{r}
summary(Linearity_ModelSelection4)
plot(Linearity_ModelSelection4)
```

'Condition' has the lowest p-value and so it is removed:

```{r}
SimplifiedStructure <- str_remove(SimplifiedStructure, fixed("Condition + "))
```

Re-fit the model:

```{r}
Linearity_ModelSelection5 <- glmer(
    SimplifiedStructure,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model converges, and it is no longer singular.
We then inspect the model coefficients:

```{r}
summary(Linearity_ModelSelection5)
plot(Linearity_ModelSelection5)
fixef(Linearity_ModelSelection5)
```

### Model Assumptions

All generalised linear mixed models were tested for (1) multicollinearity, (2) under- and overdispersion, (3) heteroscedasticity, (4) influential observations, and (5) normality of random effects.

#### 1. Multicollinearity

We check for multicollinearity first with a correlation matrix:

```{r}
corrplot(cor(linearity_measure_data[, c(17, 19, 21:22)]), method = "number")
```

The two electronic experience measures are positively correlated, *r* = `r round(cor(linearity_measure_data$Eexp1LongformEFreq, linearity_measure_data$Eexp2DeviceRecFreq), 2)`. Other predictors in the model are not correlated with each other.

To check if these correlations are problematic, we calculate Variance Inflation Factors (VIF):

```{r}
check_collinearity(Linearity_ModelSelection5)
plot(check_collinearity(Linearity_ModelSelection5))
```

Despite of the correlations, the model predictors do not seem to be multicollinear. All VIF < 1.5, and usually VIF > 5 is used as an indicator of potential multicollinearity.

#### 2. Under- and Overdispersion

Under- and overdispersion occur when the data shows less or more variation, respectively, than what we would expect. We test dispersion with simulation-based tests from DHARMa:

```{r}
simulated_model_output <- simulateResiduals(fittedModel = Linearity_ModelSelection5, plot = F)
```

```{r}
plotQQunif(simulated_model_output)
```

The model does not deviate from expected distribution according to the qqplot.

```{r}
testDispersion(simulated_model_output)
```

The dispersion test is not significant, indicated that the model is not affected by under- or overdispersion.

#### 3. Heteroscedasticity

We use the same simulated_model_output created in step 2 to test heteroscedasticity

```{r}
testQuantiles(simulated_model_output)
```

The p-value is not significant, and the lines lie flat and horizontal in the plot, close to the reference lines. This indicates that the residuals are homoscedastic.

#### 4. Influential Observations

We test influential observations with Cook's Distance:

```{r}
cooksd <-
    cooks.distance(Linearity_ModelSelection5)
plot(cooksd,
    pch = "*",
    cex = 2,
    main = "Influential Obs by Cooks Distance"
)
abline(h = 4 * mean(cooksd, na.rm = TRUE), col = "red")
text(
    x = 1:length(cooksd) + 1,
    y = cooksd,
    labels = ifelse(cooksd > 4 * mean(cooksd, na.rm = TRUE), names(cooksd), ""),
    col = "red"
)
```

The plot indicates that some observations may be influential. However, the Cook's Distance threshold is very conservative at `r round((4 * mean(cooksd, na.rm = TRUE)), 2)`.

We further inspect outliers using the simulated_model_output and DHARMa's outlier test:

```{r}
testOutliers(simulated_model_output)
```

The test and the plot indicate that none of the observations are influential. We trust this judgement after off-script experimentation indicated that removal of the most influential observation flagged earlier does not affect results.

#### 5. Normality of Random Effects

Random effects should be normally distributed in multilevel models. We test this assumption by inspecting first the normality of 'UserId' and then 'StoryId', and finally 'WindowWidth' random intercepts.

```{r}
random_intercept_UserId <- ranef(Linearity_ModelSelection5)$UserId$`(Intercept)`
qqnorm(random_intercept_UserId)
qqline(random_intercept_UserId)
shapiro.test(random_intercept_UserId)
```

The qqplot varies from the reference line, but considering that the variance is not extensive and the Shapiro Test is not significant, we assume that UserId random intercept is normally distributed.

```{r}
random_intercept_StoryId <- ranef(Linearity_ModelSelection5)$StoryId$`(Intercept)`
qqnorm(random_intercept_StoryId)
qqline(random_intercept_StoryId)
shapiro.test(random_intercept_StoryId)
```

StoryId aligns much worse with the reference line, however, this is expected considering that StoryId only includes 9 different story groups. With the limited amount of information, achieving a visually normally distributed result is unlikely. Considering that the Shapiro Test is not significant, we can assume that StoryId random intercept is normally distributed.

```{r}
random_intercept_WindowWidth <- ranef(Linearity_ModelSelection5)$WindowWidth$`(Intercept)`
qqnorm(random_intercept_WindowWidth)
qqline(random_intercept_WindowWidth)
shapiro.test(random_intercept_WindowWidth)
```

Again, the qqplot indicates that WindowWidth may vary from normality, however, considering that the variance is not extensive and the Shapiro Test is not significant, we assume that WindowWidth random intercept is normally distributed.

The model aligns with all assumptions.

### Interpret reader characteristics model

We then interpret model effects to interpret the results.

```{r}
summary(Linearity_ModelSelection5)
```

Only an interaction effect between the two task-relevant electronic experience measures is a signficant predictor of linearity of reading.

#### Hypotheses

**H4a: We expect situational motivation (high-autonomy condition) to be a negative predictor of nonlinear navigation frequency when situational competence is high.**

This hypothesis was not supported as the interaction effect between Condition and situational competence was removed from the model during backward stepwise selection. Indeed, the main effect of condition was similarly removed, and therefore, participants in the high-autonomy and low-autonomy conditions did not differ significantly in their frequency of using nonlinear navigation during reading of the short story.

**H4b: We expect contextual motivation to be a negative predictor of nonlinear navigation frequency when situational competence is high.**

This hypothesis was not supported as the interaction between contextual motivation ('CInterest') and situational competence (SCompetence) was not a significant predictor of nonlinear navigation.

```{r}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_AddLine.R"
    )
)

linearity_effectplot_cmotscomp <- interact_plot(
    Linearity_ModelSelection5,
    pred = "CInterest.sc",
    modx = "SCompetence.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Situation competence score,(centered and scaled)")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Contextual motivation score, measured by contextual reading interest,(centered and scaled)")
    )
```

Although the hypothesis was not supported, the trend of the results is in the same direction as we predicted: participants with a high contextual motivation score initiated nonlinear navigation slightly less often when they found the text easy to read (high situational competence score). In contrast, a low contextual motivation score and high situational competence is associated with more frequent nonlinear navigation.

However, the interaction was not significant and so the effect should not be interpreted further.

**H4c: We expect an interaction between task-relevant electronic experience measures to be a negative predictor of nonlinear navigation frequency when situational competence is high.**

This hypothesis was not supported as an interaction between the two electronic experience measures and situational competence was not a significant predictor of linearity of reading.

```{r}
linearity_effectplot_eexpscomp <- interact_plot(
    Linearity_ModelSelection5,
    pred = "Eexp1LongformEFreq.sc",
    modx = "Eexp2DeviceRecFreq.sc",
    mod2 = "SCompetence.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Electronic experience measure 2: Device*"),
    mod2.labels = c("-1SD Situational competence", "Mean of Situational Competence", "+1SD Situational Competence")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Electronic experience measure 1, frequency of e-reading task-relevant text types,(centered and scaled)")
    )
```

Indeed, the effect of electronic experience was the same, despite of situational competence.

Although the three-way interaction between electronic experience measures and situational competence did not significantly predict linearity of reading, a two-way interaction between the electronic experience measures was a significant predictor in the model.

```{r}
linearity_effectplot_eexp <- interact_plot(
    Linearity_ModelSelection5,
    pred = "Eexp1LongformEFreq.sc",
    modx = "Eexp2DeviceRecFreq.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Electronic experience measure 2: Device*")
) +
    theme_bw() +
    theme(
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.ticks = element_blank(),
        axis.title.x = element_text(size = 20, vjust = 0.5),
        axis.title.y = element_text(size = 20, angle = 90, vjust = 0.5),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 18),
        legend.key = element_blank(),
        legend.position = "top",
        legend.direction = "horizontal",
        plot.caption = element_text(size = 16),
        strip.text.x = element_text(size = 16),
        legend.background = element_rect(color = "black", size = .5, linetype = "solid")
    ) +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Electronic experience measure 1, frequency of e-reading task-relevant text types,(centered and scaled)")
    )
```

The finding indicated that a combination of high scores in the two task-relevant electronic reading experience measures was associated with lower likelihood of nonlinear navigation. In contrast, a low score in either measure with a high level of the other, was associated with more frequent nonlinear navigation. This indicates that electronic experience affected participants' linearity of reading.