---
title: "Analysis_ReadingLinearity"
author: "Pauliina Vuorinen"
date: "27/06/2022-24/04/2023"
output: html_document
library: ["~/Extra/RPackages.bib", "~/Extra/InCodeCitations.bib"]
csl : ~/Extra/apa-7th-ed.csl
---

## Introduction

The purpose of this script is to analyse how linearly participants read the short story during the study.
We aim to assess how frequently participants used nonlinearity during the study, how nonlinearity occurred, and whether autonomy condition, contextual reading motivation, and electronic reading experience are connected to this nonlinearity. See the full article for details.

To answer these research questions, we use 'StartsNonlinearity' (saved in Data as linearity_measure_data.csv) as a measure of the frequency of nonlinearity. This measure was computed in Prep_ReadingLinearityMeasure.rmd, and it tells us whether nonlinear navigation was initiated on a page-view or not. Therefore, StartsNonlinearity is TRUE when a participant starts a regression or a forward leap on a page-view and the previous page-view did not include similar nonlinearity (see Prep_TrackingDataWrangling.rmd in Prep for more detail).

In this analysis script, StartsNonlinearity is used as an outcome variable in **two different multilevel models**. In the first model, we study whether reader charactersistics such as condition, contextual motivation, and electronic reading experience predict StartsNonlinearity. In the second model, the independent variables include information on the timing and location of StartsNonlinearity. With the latter model our intention is to study in which situations nonlinearity occurs. Both model structures are based on selection outlined in the full article. The models are constructed using glmer() in the lme4-package by @bates_fitting_2015.

**Information on the hypotheses**

* H4a: Situational autonomous motivation is connected to frequent nonlinearity when task competence is low, and less frequent nonlinearity when task competence is high.
* H4b: Contextual autonomous motivation is connected to frequent nonlinearity when task competence is low, and less frequent nonlinearity when task competence is high.
* H4c: Task-relevant electronic experience is connected to frequent nonlinearity when task competence is low, and less frequent nonlinearity when task competence is high.

## Setup

```{r 'setup'}
library(tidyverse)
library(dplyr)
library(psych)
library(gridExtra)
library(corrplot)
library(lme4)
library(lmerTest)
library(DHARMa)
library(performance)
library(car)
library(effects)
library(interactions)
library(jtools)
```

Save working directory so that this script can be used elsewhere, if required. The working directory should be "~/Short-Story-Reading-Behaviour-Public/". If the working directory is not correct, we save the correct path and use that in loading files.

The working directory is not changed with setwd() because this script is knit remotely in other scripts. If using remotely, define 'AnalysisFilePath'. AnalysisFilePath refers to this exact file location, and so path to the main folder "Short_Story_Reading_Behaviour_Public" is two folders lower.

```{r 'setwd'}
getwd() # working directory should be ~/Short_Story_Reading_Behaviour_Public
# If AnalysisFilePath is not defined, use setwd() to refer to the main folder
# setwd()
if (exists("AnalysisFilePath")) {
    mypath_SSRBP <- dirname(dirname(AnalysisFilePath))
} else if (grepl("Short_Story_Reading_Behaviour_Public/Analysis", getwd())) {
    mypath_SSRBP <- dirname(getwd())
} else if (grepl("Short_Story_Reading_Behaviour_Public", getwd())) {
    mypath_SSRBP <- getwd()
} else {
    print("mypath_SSRBP should be defined manually")
    # mypath_SSRBP <- XX
}
```

```{r 'load-figure-theme-for-consistency', echo=FALSE, warning="hide"}
source(
    paste0(
        mypath_SSRBP,
        "/Fig/Fig_FigureTheme.R"
    )
)
```

```{r 'load-helmert-contrasts-functions'}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.weighted.R"
    )
)
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_contr.helmert.unweighted.R"
    )
)
```

```{r 'source-functions-for-line-adding'}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_AddLine.R"
    )
)
```

```{r 'source-function-for-variabletype-conversion'}
source(
    paste0(
        mypath_SSRBP,
        "/Functions/Functions_VariableTypeConversion.R"
    )
)
```

## Load data and check variable types

```{r 'load-data'}
# load data
linearity_measure_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/linearity_measure_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
linearity_measure_data <- dplyr::select(linearity_measure_data, -X) # remove row numbers
```

We then load in data from questionnaires:

```{r 'load-predictors:-questionnaires'}
# IMI
IMI_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/IMI_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
IMI_scores <- dplyr::select(IMI_scores, -X) # remove row numbers
# IMI-R
IMIR_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/IMIR_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
IMIR_scores <- dplyr::select(IMIR_scores, -X) # remove row numbers
# Electronic experience
eexp_scores <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/eexp_scores.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
eexp_scores <- dplyr::select(eexp_scores, -X) # remove row numbers
# demographic information
demographics_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/demographics_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
demographics_data <- dplyr::select(demographics_data, -X) # remove row numbers
```

Load in information about the stories that participants read:

```{r 'load-information-about-stories'}
story_information_data <-
    read.csv(
        paste0(
            mypath_SSRBP,
            "/Data/story_information_data.csv"
        ),
        header = TRUE,
        sep = ";",
        dec = ","
    )
story_information_data <- dplyr::select(story_information_data, -X) # remove row numbers
```

We then merge these dataframes together:

```{r 'merge-dfs'}
# measure and IMI
linearity_measure_data <- merge(
    linearity_measure_data,
    IMI_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and IMI-R
linearity_measure_data <- merge(
    linearity_measure_data,
    IMIR_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and EEXP
linearity_measure_data <- merge(
    linearity_measure_data,
    eexp_scores,
    all.x = TRUE,
    by = "UserId"
)
# measure and demographic information
linearity_measure_data <- merge(
    linearity_measure_data,
    demographics_data,
    all.x = TRUE,
    by = "UserId"
)
# measure and information about stories
linearity_measure_data <- merge(
    linearity_measure_data,
    story_information_data[, c(1:3, 8)],
    all.x = TRUE,
    by = "StoryId"
)
```

## Change variable types

```{r 'change-variable-types'}
str(linearity_measure_data)

## turn columns into factors that should be factors
linearity_measure_data[, c(
    "UserId",
    "StoryId"
)] <- convert.magic(
    linearity_measure_data[, c(
        "UserId",
        "StoryId"
    )],
    "factor"
)
```

## Reader characteristics model

The reader characteristics model of linearity aims to address the following hypotheses:

* H4a: Situational autonomous motivation is connected to frequent nonlinearity when task competence is low, and less frequent nonlinearity when task competence is high.
* H4b: Contextual autonomous motivation is connected to frequent nonlinearity when task competence is low, and less frequent nonlinearity when task competence is high.
* H4c: Task-relevant electronic experience is connected to frequent nonlinearity when task competence is low, and less frequent nonlinearity when task competence is high.

The reader characteristics model was pre-specified to best address our hypotheses. We use a maximal interactive model structure, which includes random intercepts for story and participant indicator. Random slopes were included for all key variables in the model which are used to address our hypotheses. These random slopes could only be included in the random effect of story indicator, considering that all key variables had only one observation from each participant.

The maximal interactive model takes the following structure:

$Y_{i} = 
    \beta_{0} +
    WindowWidth +
    DUD +
    Native +
    Cond +
    SComp +
    CMot +
    CComp +
    TR-EExp1 +
    TR-EExp2 +
    Cond x SComp +
    CMot x SComp +
    TR-EExp1 x TR-EExp2 +
    TR-EExp1 x TR-EExp2 x SComp +
    (1  + Cond x SComp + CMot x SComp + TR-EEXP1 x TR-EEXP2 | Story indicator) +
    (1 | Participant indicator) +
    \varepsilon_{i}$ 

where
**$Y_{i}$** is the reading behaviour measure, in this case StartsNonlinearity for reading linearity.
**$\beta_{0}$** is the intercept
**WindowWidth** indicates of screen size, measured by window width in pixels (which is affected by device and has an effect on how much text is visible)
**DUD** is an acronym for 'DaysUntilDeadline' which tells us how much the participant has time left to read the short story. Days until deadline is used to control for variance in reading behaviour as a result of pressure to read the short story in time. For example, participants may read the text differently closer to the deadline compared to the beginning of the study.
**Native** is a binary variable indicating whether the participant is a native speaker of English or not (responses - Yes/No, yes indicating that the participant is a native speaker).
**Cond** represents autonomy condition (as a measure of situational motivation)
**SComp** is participants' situational competence (perception of competence to read the story), measured by the subcomponent of 'competence' from the IMI questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**CMot** is participants' contextual reading motivation, measured by the subcomponent of contextual interest from the IMI-R questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**CComp** is participants' contextual competence (perception of general reading ability), measured by the subcomponent of 'competence' from the IMI-R questionnaire (see Prep_Questionnaires.Rmd in Prep folder)
**TR-EExp1** is frequency of using any electronic devices for long-form text reading purposes
**TR-EExp2** is frequency of using task-relevant devices for any reading purpose
**(1  + Cond x SComp + CMot x SComp + TR-EEXP1 x TR-EEXP2 | Story indicator)** indicates a random effect of story indicator. The random effect includes an intercept for each story indicator, and a random slope for all key variables used to address our hypotheses.
**(1 | Participant indicator)** indicates a random effect of participant indicator. The random effect only includes the random intercept for each participant considering that key variables of interest only had one observation for each participant, and so usage of random slopes was not meaningful. Note that participant indicator is explicitly nested within story indicator as participants only read one of 9 stories during the study. Due to the explicit nesting, notation as nested vs crossed does not make a difference in the model specification. To allow us to use different random slopes, we use crossed random effect notation here.
**$\varepsilon_{i}$** indicates the residual variance of $Y_{i}$

Whereas linearity of reading and reading speed are analysed with this reader characteristics structure, the remaining models (task switching and reading persistence) are modelled without 'Cond x SComp', 'CMot x SComp', and 'TR-EExp1 x TR-EExp2 x SComp'. This is because linearity and speed are expected to be connected to motivation and electronic experience via interaction with situational competence, whereas task switching and reading persistence are expected to be connected to motivation and electronic experience directly (main effects).

In case of nonconvergence or singularity, the model is selected backward stepwise. This is done by first removing random slopes from the model starting with the slope that accounts for the least variance. If the model remains nonconvergent or singular, fixed effects are removed one-by-one starting with the predictors that account for the least variance according to p-values.

### Visualise outcome variable and predictors

The linearity_measure_data has `r nrow(linearity_measure_data)` observations.

We create graphs to inspect linearity of reading and its connection to other variables.

The linearity measure, StartsNonlinearity, is a binary variable (whether each event initiates nonlinearity or not, TRUE/FALSE). To visualise it, we create counts:

```{r 'count-summary-df', echo=FALSE}
linearity_measure_count <- linearity_measure_data %>%
    group_by(
        UserId,
        StoryId,
        CharacterLength,
        Gender,
        Age,
        NativeEnglish,
        Condition,
        SCompetence,
        CInterest,
        CCompetence,
        Eexp1LongformEFreq,
        Eexp2DeviceRecFreq,
        StartsNonlinearity
    ) %>%
    summarise(
        Count = n()
    )
```

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = StartsNonlinearity, y = Count)) +
    geom_violin() +
    geom_jitter() +
    theme_classic()
```

Variation between participants:

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = UserId, y = Count, colour = StartsNonlinearity)) +
    geom_point() +
    theme_classic()
```

Variation between different stories read:

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = StoryId, y = Count, colour = StartsNonlinearity)) +
    geom_boxplot() +
    theme_classic()
```

We correct for story length to see if there are differences between different books read:

```{r, include=FALSE}
ggplot(
    linearity_measure_count,
    aes(x = StoryId, y = (Count / CharacterLength), colour = StartsNonlinearity)
) +
    geom_boxplot() +
    geom_jitter() +
    theme_classic() +
    ylab("Count of StartsNonlinearity/Story length")
```

The impact of window width (size of device and amount of text visible):

```{r, include=FALSE}
ggplot(
    linearity_measure_data,
    aes(x = StartsNonlinearity, y = WindowWidth)
) +
    geom_jitter(size = 2) +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

We then visualise the effect of DaysUntilDeadline:

```{r, include=FALSE}
ggplot(
    linearity_measure_data,
    aes(x = StartsNonlinearity, y = FirstTimeUntilDeadlineDays)
) +
    geom_jitter(size = 2) +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```


### Linearity of Reading and Demographic Information

Gender:

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = Gender, y = Count, colour = StartsNonlinearity)) +
    geom_violin() +
    geom_jitter() +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

Age:

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = Age, y = Count, colour = StartsNonlinearity)) +
    geom_point() +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

Whether participants' native language is English or not:

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = NativeEnglish, y = Count, colour = StartsNonlinearity)) +
    geom_violin() +
    geom_jitter() +
    theme_classic() +
    theme(
        legend.position = "none"
    )
```

### Condition

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = Condition, y = Count, colour = StartsNonlinearity)) +
    geom_boxplot() +
    theme_classic()
```

### Motivation

Only situational competence is included in the model from the IMI variables, considering that condition is significantly connected to both situational autonomy and interest.

Situational competence:

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = SCompetence, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

We then create visualisations for contextual measures of motivation: interest, and competence.

Contextual interest:

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = CInterest, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

Contextual competence:

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = CCompetence, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

### Linearity of reading and electronic reading experience

We are interested in how task-relevant electronic experience is connected to linearity.
Electronic experience was measured by two variables: Eexp1LongFormEFreq - the frequency of reading long-form, narrative texts electronnically, and Eexp2DeviceRecFreq - the frequency of using task-relevant digital devices for recreational reading purposes.

Eexp1LongformEFreq (longfrom reading electronically):

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = Eexp1LongformEFreq, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

Eexp2DeviceRecFreq (task-relevant digital devices for recreational reading):

```{r, include=FALSE}
ggplot(linearity_measure_count, aes(x = Eexp2DeviceRecFreq, y = Count, colour = StartsNonlinearity)) +
    geom_smooth() +
    geom_point() +
    theme_classic()
```

### Set contrasts to predictors

Predictors in the model are given contrasts. Continuous variables are scaled and centered, whereas categorical variables are given helmert contrasts. Helmert contrasts are used to control for uneven levels in categorical variables.

Contrasts for categorical variables:

```{r 'categorical-contrasts-rc', echo=FALSE}
# whether native language is English or not
linearity_measure_data$NativeEnglish <- as.factor(linearity_measure_data$NativeEnglish)
contrasts(linearity_measure_data$NativeEnglish) <- contr.helmert.unweighted(linearity_measure_data$NativeEnglish)
contrasts(linearity_measure_data$NativeEnglish)
# condition
linearity_measure_data$Condition <- dplyr::recode(
    linearity_measure_data$Condition,
    "AutonomousCondition" = "HighAutonomyCondition",
    "NonAutonomousCondition" = "LowAutonomyCondition"
)
linearity_measure_data$Condition <- as.factor(linearity_measure_data$Condition)
contrasts(linearity_measure_data$Condition) <- contr.helmert.unweighted(
    linearity_measure_data$Condition
)
contrasts(linearity_measure_data$Condition)
```

Next, we scale and center continuous variables. These variables names are appended with ".cs" as a reminder that the variable is scaled and centered.

```{r 'scale-continuous-variables-rc', echo=FALSE}
# Situational competence
linearity_measure_data$SCompetence.sc <-
    scale(linearity_measure_data$SCompetence,
        center = TRUE,
        scale = TRUE
    )
# Contextual motivation
linearity_measure_data$CInterest.sc <-
    scale(linearity_measure_data$CInterest,
        center = TRUE,
        scale = TRUE
    )
linearity_measure_data$CCompetence.sc <-
    scale(linearity_measure_data$CCompetence,
        center = TRUE,
        scale = TRUE
    )
# Electronic reading experience
linearity_measure_data$Eexp1LongformEFreq.sc <-
    scale(linearity_measure_data$Eexp1LongformEFreq,
        center = TRUE,
        scale = TRUE
    )
linearity_measure_data$Eexp2DeviceRecFreq.sc <-
    scale(linearity_measure_data$Eexp2DeviceRecFreq,
        center = TRUE,
        scale = TRUE
    )
# Days until reading deadline
linearity_measure_data$DaysUntilDeadline.sc <-
    scale(linearity_measure_data$FirstTimeUntilDeadlineDays,
        center = TRUE,
        scale = TRUE
    )
# Window Width
linearity_measure_data$WindowWidth.sc <-
    scale(linearity_measure_data$WindowWidth,
        center = TRUE,
        scale = TRUE
    )
```

### Modelling by Reader Characteristics

First, we build the maximal, interactive model structure. Refer to the beginning of this script to see information on the maximal structure. The model includes XX parameters, and `r nrow(linearity_measure_data)` observations. Previous research has suggested that each parameter in a model has at least 15 observations. Our sample exceeds this threshold considering that `r nrow(linearity_measure_data)`/19 = `r round(nrow(linearity_measure_data)/19, 2)`

Define and then fit the maximal interactive model:

```{r 'define-maximal-structure'}
Structure_RC_nested_maximal <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 + Condition : SCompetence.sc + CInterest.sc : SCompetence.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc | StoryId) + (1 | UserId)"
)
```

```{r 'fit-model', warning='hide'}
Linearity_RC_nested_maximal <- glmer(
    Structure_RC_nested_maximal,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The maximal model is singular.
In an attempt to resolve this, we use backward selection of the random slopes by removing the slope that accounts for the lowest amount of variance in the model.

```{r 'backward-select-slopes-in-model', warning='hide'}
# Identify slope that accounts for the lowest amount of variance
summary(Linearity_RC_nested_maximal)
# CInterest.sc:SCompetence.sc accounts for the lowest amount of variance
## re-fit model without it
Structure_RC_nested_slopes2a <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 + Condition : SCompetence.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc | StoryId) + (1 | UserId)"
)
Linearity_RC_nested_slopes2a <- glmer( # remains singular
    Structure_RC_nested_slopes2a,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
# Identify next slope to remove
summary(Linearity_RC_nested_slopes2a)
# Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc is removed
## re-fit model without it
Structure_RC_nested_slopes2b <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 + Condition : SCompetence.sc | StoryId) + (1 | UserId)"
)
Linearity_RC_nested_slopes2b <- glmer( # remains singular
    Structure_RC_nested_slopes2b,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
summary(Structure_RC_nested_slopes2b)
# We remove the final random slope
## re-fit the model without it
Structure_RC_nested_slopes2c <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | StoryId) + (1 | UserId)"
)
Linearity_RC_nested_slopes2c <- glmer( # remains singular
    Structure_RC_nested_slopes2c,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The full model is singular, even after removal of all random slopes. Off-script, we also noted that the singularity is not resolved by backward selection of fixed effects.

```{r, include=FALSE}
summary(Linearity_RC_nested_slopes2c)
```

Inspection of the model values indicates that the random intercept of StoryId is accounting for zero variance in the model. This can occur when the variance accounted for by the random intercept is minimal and thus the model fails to estimate variance, resulting in singularity. To address the issue, we attempt to fit the model with StoryId as a fixed effect rather than a random intercept. Note that inclusion of StoryId as a fixed effect means that no random slopes can be included in the model:

```{r 'define-structure-without-storyid'}
Structure_RC_AdjustedStoryId <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | UserId)"
)
```

Including StoryId as a fixed effect rather than a random effect increases the amount of parameters in the model from 16 to 23. This isn't an issue, however, as the observations per parameter value still exceeds the threshold of 20 considering that `r nrow(linearity_measure_data)`/23 = `r round(nrow(linearity_measure_data)/23, 2)`

Add contrasts to StoryId:

```{r 'add-contrasts-to-storyidRC', include=FALSE}
contrasts(linearity_measure_data$StoryId) <- contr.helmert.unweighted(
    linearity_measure_data$StoryId
)
contrasts(linearity_measure_data$StoryId)
```

Re-fit the model:

```{r 'fit-adjusted-model', warning='hide'}
Linearity_RC_AdjustedModel <- glmer(
    Structure_RC_AdjustedStoryId,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model is no longer singular. However, it is possible that StoryId is collinear with other predictors in the model, and thus we assess multicollinearity before proceeding:

```{r, include=FALSE}
car::vif(Linearity_RC_AdjustedModel)
```

Indeed, the Variance Inflation Factor (VIF) indicates that StoryId causes multicollinearity in the model. Therefore, it cannot be included as a fixed effect in the model and we remove it. This means that variation between different stories read cannot be estimated in the reader characteristics model.

```{r 'define-structure-wo-storyid'}
Structure_RC_RemovedStoryId <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + NativeEnglish + Condition + SCompetence.sc + CInterest.sc + CCompetence.sc + Eexp1LongformEFreq.sc + Eexp2DeviceRecFreq.sc +  Condition : SCompetence.sc + CInterest.sc : SCompetence.sc + Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc +  Eexp1LongformEFreq.sc : Eexp2DeviceRecFreq.sc : SCompetence.sc + (1 | UserId)"
)
```

Re-fit the model:

```{r 'fit-adjusted-model-2', warning='hide'}
Linearity_RC_RemovedStoryId <- glmer(
    Structure_RC_RemovedStoryId,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The model converges and it is not singular. We re-assess multicollinearity:

```{r, include=FALSE}
car::vif(Linearity_RC_RemovedStoryId)
```

All predictors VIF is below 2.5, indicating that the model is no longer affected by multicollinearity. We assess this in more detail below in the assumptions.

### Model Assumptions

All generalised linear mixed models were tested for (1) multicollinearity, (2) heteroscedasticity, (3) influential observations, and (4) normality of random effects.

Under- and overdispersion were not tested considering that the observations cannot be effectively grouped by predictors (see https://stats.stackexchange.com/a/200848).

#### 1. Multicollinearity

We check for multicollinearity first with a correlation matrix:

```{r, eval=FALSE, include=FALSE}
# corrplot(cor(linearity_measure_data[, c(30:36)]), method = "number")
```

Some of the predictors are highly correlated. For example, the two electronic experience measures are positively correlated, *r* = `r round(cor(linearity_measure_data$Eexp1LongformEFreq, linearity_measure_data$Eexp2DeviceRecFreq), 2)`. Similarly, contextual motivation is positively correlated with contextual competence scores, *r* = `r round(cor(linearity_measure_data$CInterest, linearity_measure_data$CCompetence), 2)`

To check if these correlations are problematic, we assess Variance Inflation Factors (VIF) with check_collinearity from the performance package:

```{r, eval=FALSE, include=FALSE}
check_collinearity(Linearity_RC_RemovedStoryId)
plot(check_collinearity(Linearity_RC_RemovedStoryId))
```

Despite of the correlations, the model predictors do not seem to be multicollinear. All VIF < 2.5, and usually VIF > 5 is used as an indicator of potential multicollinearity.

#### 2. Heteroscedasticity

We use the same simulated_model_output created in step 2 to test heteroscedasticity

```{r, eval=FALSE, include=FALSE}
# testQuantiles(simulated_model_output)
```

The p-value is not significant, and the lines lie flat and horizontal in the plot, close to the reference lines. This indicates that the residuals are homoscedastic.

#### 3. Influential Observations

We test influential observations with Cook's Distance:

```{r, eval=FALSE, include=FALSE}
# cooksd <-
#     cooks.distance(Linearity_RC_RemovedStoryId)
# plot(cooksd,
#     pch = "*",
#     cex = 2,
#     main = "Influential Obs by Cooks Distance"
# )
# abline(h = 4 * mean(cooksd, na.rm = TRUE), col = "red")
# text(
#     x = 1:length(cooksd) + 1,
#     y = cooksd,
#     labels = ifelse(cooksd > 4 * mean(cooksd, na.rm = TRUE), names(cooksd), ""),
#     col = "red"
# )
```

The plot indicates that some observations may be influential. However, the Cook's Distance threshold is very conservative at `r round((4 * mean(cooksd, na.rm = TRUE)), 2)`.

We further inspect outliers using the simulated_model_output and DHARMa's outlier test:

```{r, eval=FALSE, include=FALSE}
set.seed(1234) # for reproducibility
simulated_model_output <- DHARMa::simulateResiduals(Linearity_RC_RemovedStoryId)
# testOutliers(simulated_model_output)
```

The test and the plot indicate that none of the observations are influential. We trust this judgement after off-script experimentation indicated that removal of the most influential observation flagged earlier does not affect results.

#### 4. Normality of Random Effects

Random effects should be normally distributed in multilevel models. We test this assumption by inspecting first the normality of 'UserId'. The random intercept of 'StoryId' was removed from the selected model.

```{r, eval=FALSE, include=FALSE}
random_intercept_UserId <- ranef(Linearity_RC_RemovedStoryId)$UserId$`(Intercept)`
# qqnorm(random_intercept_UserId)
# qqline(random_intercept_UserId)
shapiro.test(random_intercept_UserId)
```

The qqplot varies from the reference line, but considering that the variance is not extensive and the Shapiro Test is not significant, we assume that UserId random intercept is normally distributed.

**Assumptions conclusion**

To reach convergence and non-singularity, the random intercept of StoryId was removed from the model as it accounted for zero variance. We attempted to fit the model with StoryId as a fixed effect, however, the variable was ultimately removed due to high collinearity with other predictors in the model.

The final, adjusted model aligns with all assumptions.

### Interpret reader characteristics model

We then interpret model effects to interpret the results.

```{r, include=FALSE}
summary(Linearity_RC_RemovedStoryId)
```

Only an interaction effect between the two task-relevant electronic experience measures is a signficant predictor of linearity of reading.

#### Hypotheses

**H4a: Situational autonomous motivation is connected to frequent nonlinearity when task competence is low, and less frequent nonlinearity when task competence is high.**

This hypothesis was not supported as the interaction effect between Condition and situational competence was not a significant predictor of linearity.

```{r, warning='hide'}
linearity_rc_condscomp <- interact_plot(
    Linearity_RC_RemovedStoryId,
    pred = SCompetence.sc,
    interval = TRUE,
    modx = Condition,
    data = linearity_measure_data
) +
    figure_theme_with_top_legend +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Situational competence score,(centered and scaled)")
    )
```

As the interaction effect was not significant, the combination of situational competence and condition manipulation was not connected to participants' frequency of using nonlinear navigation. Therefore, our hypothesis H4a was not supported.

**H4b: Contextual autonomous motivation is connected to frequent nonlinearity when task competence is low, and less frequent nonlinearity when task competence is high.**

This hypothesis was not supported as the interaction between contextual motivation ('CInterest') and situational competence (SCompetence) was not a significant predictor of nonlinear navigation.

```{r, warning='hide'}
linearity_rc_cmotscomp <- interact_plot(
    Linearity_RC_RemovedStoryId,
    pred = "SCompetence.sc",
    modx = "CInterest.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Contextual,motivation score")
) +
    figure_theme_with_top_legend +
    labs(
        y = addline_format("Odds of initiating,nonlinear navigation"),
        x = addline_format("Situation competence score,(centered and scaled)")
    ) +
    coord_cartesian(ylim = c(0, 0.4), expand = FALSE) +
    scale_y_continuous(breaks = c(0, 0.1, 0.2, 0.3, 0.4))
```

Although the hypothesis was not supported, the trend of the results is in the same direction as we predicted: participants with a high contextual motivation score initiated nonlinear navigation more often when they found the text difficult to read than when they reported that the story was easy to comprehend (in line with H4b). In contrast, participants with a low contextual motivation score used on average less nonlinear navigation when the text was difficult to read, and more when the text was seen as easy. However, the effect is not significant and thus it should not be interpreted further.

**H4c: Task-relevant electronic experience is connected to frequent nonlinearity when task competence is low, and less frequent nonlinearity when task competence is high.**

This hypothesis was not supported as an interaction between the two electronic experience measures and situational competence was not a significant predictor of linearity of reading.

```{r, warning='hide'}
linearity_rc_eexpscomp <- interact_plot(
    Linearity_RC_RemovedStoryId,
    pred = "Eexp1LongformEFreq.sc",
    modx = "Eexp2DeviceRecFreq.sc",
    mod2 = "SCompetence.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Electronic experience measure 2: Device*"),
    mod2.labels = c("-1SD Situational competence", "Mean of Situational Competence", "+1SD Situational Competence")
) +
    figure_theme_with_top_legend +
    labs(
        y = "Odds of initiating nonlinear navigation",
        x = addline_format("Electronic experience measure 1, frequency of e-reading task-relevant text types,(centered and scaled)")) +
    coord_cartesian(ylim = c(0, 0.4), expand = FALSE) +
    scale_y_continuous(breaks = c(0, 0.1, 0.2, 0.3, 0.4))
```

Indeed, the effect of electronic experience was the same, despite of situational competence.

Although the three-way interaction between electronic experience measures and situational competence did not significantly predict linearity of reading, a two-way interaction between the electronic experience measures was a significant predictor in the model.

```{r, warning='hide'}
linearity_effectplot_eexp <- interact_plot(
    Linearity_RC_RemovedStoryId,
    pred = "Eexp1LongformEFreq.sc",
    modx = "Eexp2DeviceRecFreq.sc",
    data =  linearity_measure_data,
    interval = TRUE,
    legend.main = addline_format("Electronic,Experience 2")) +
    figure_theme_with_top_legend +
    labs(
        y = addline_format("Odds of initiating,nonlinear navigation"),
        x = addline_format("Electronic Experience 1,(centered and scaled)")) +
    coord_cartesian(ylim = c(0, 0.4), expand = FALSE) +
    scale_y_continuous(breaks = c(0, 0.1, 0.2, 0.3, 0.4))
```

The finding indicated that a combination of high scores in the two task-relevant electronic reading experience measures was associated with a lower likelihood of nonlinear navigation. In contrast, a low score in either measure with a high level of the other, was associated with more frequent nonlinear navigation. This indicates that electronic experience affected participants' linearity of reading.

## Event properties model

The event properties model of linearity is used for explorative purposes, to study how frequency of nonlinear navigation varied during the study. As a result, the model specification is done differently from the reader characteristics model.

To explore meaningful random slope variables and interactions, we use data-driven selection. 

1. Additive intercepts-only model
2. Selection of random slopes with best-path algorithm
3. Backward selection if necessary
4. Interactive model structure with two-way interactions
5. Backward selection if necessary

1. We first specify an additive intercepts-only model with the following structure:

$Y_{i} = 
    \beta_{0} +
    # control variables
    WindowWidth +
    DUD +
    # predictors of interest
    Event~k-1~ +
    Event~k-2~ +
    ReadingSessionNumber +
    TimeInReadingSession +
    LocationInText +
    (1 | Story indicator / Participant indicator) +
    \varepsilon_{i}$ 

where
**$Y_{i}$** is the reading behaviour measure, in this case StartsNonlinearity for reading linearity.
**$\beta_{0}$** is the intercept
**WindowWidth** indicates of screen size, measured by window width in pixels (which is affected by device and has an effect on how much text is visible)
**DUD** is an acronym for 'DaysUntilDeadline' which tells us how much the participant has time left to read the short story. Days until deadline is used to control for variance in reading behaviour as a result of pressure to read the short story in time. For example, participants may read the text differently closer to the deadline compared to the beginning of the study.
**Event~k-1~** reflects the previous StartsNonlinearity event at k-1 if the current event is k
**Event~k-2~** reflects the event preceeding the previous StartsNonlinearity event at k-2 if the current event is k.
**ReadingSessionNumber** reflects the reading session number at event~k~ (outcome variable)
**TimeInReadingSession** indicates time passed since the beginning of the reading session at event~k~
**LocationInText** indicates location in text (in percentages) at event~k~
**(1 | Story indicator / Participant indicator)** indicates a nested random intercept of story and subject indicator. Each participant only read one short story during the study, and participant indicator is explicitly nested.
**$\varepsilon_{i}$** indicates the residual variance of $Y_{i}$

2. After specifying the additive intercepts-only model, we test inclusion of random slopes using the 'best-path' algorithm recommended by @barrRandomEffectsStructure2013a. With this method, we test inclusion of each predictor of interest as a random slope variable against the intercepts-only model. A liberal alpha-level of .20 is used. If multiple predictors contribute to the model with p < .2, we first accept the random slope with the lowest p-value. The testing is then resumed by comparing the inclusion of each of the remaining predictors as a random slope variable against the model with the previously accepted slope. This is continued until none of the predictors contribute to the model, and so all p > .2.

3. If the model does not converge or if it is singular after random slope selection, the model is simplified by backward stepwise selection. This is done by first removing random slopes from the model starting with the slope that accounts for the least variance. If the model remains nonconvergent or singular, fixed effects are removed one-by-one starting with the predictors that account for the least variance according to p-values. Significant fixed effects are not removed.

4-5. Once the additive model converges, we add two-way interactions in the model. Two-way interactions could be added between predictors measuring previous navigation events (Event~k-1~ and Event~k-2~) and between reading session number, time in reading session, and location in text. To avoid multicollinearity, interactions were not included between between previous navigation events and the other predictors in the model (e.g. Event~k-1~ was not allowed to interact with reading session number, time in reading session, or location in text). If the interactive model structure does not converge, it is simplified with backward stepwise selection.

This model selection is used to explore possible interactions that are meaningful to add without overfitting the model.

### Create missing predictor variables

```{r 'create-preceeding-event-variables'}
# order by User, reading session number and time since the beginning of a reading session
linearity_measure_data <-
    linearity_measure_data[
        with(
            linearity_measure_data,
            order(UserId, ReadingSessionNumber, FirstCumulativeRSTime)
        ),
    ]
# find previous events
linearity_measure_data$Observation_lag1 <- lag(linearity_measure_data$StartsNonlinearity, 1)
linearity_measure_data$Observation_lag2 <- lag(linearity_measure_data$StartsNonlinearity, 2)
#  create 'IsNewUser' test
linearity_measure_data$IsNewUser <- (linearity_measure_data$UserId != lag(linearity_measure_data$UserId, 1))
# confirm ordering
linearity_measure_data <-
    linearity_measure_data[
        with(
            linearity_measure_data,
            order(UserId, ReadingSessionNumber, FirstCumulativeRSTime)
        ),
    ]
# remove across participants comparisons
linearity_measure_data$Observation_lag1 <- ifelse(
    linearity_measure_data$IsNewUser, # current row is new user, cannot compare to previous
    NA,
    linearity_measure_data$Observation_lag1
)
linearity_measure_data$Observation_lag2 <- ifelse(
    lag(linearity_measure_data$IsNewUser, 1) |
    linearity_measure_data$IsNewUser, # current or previous row is new user, cannot compare to event k-2
    NA,
    linearity_measure_data$Observation_lag2
)
```

```{r 'create-location-in-%'}
linearity_measure_data$PercentageLocation <- (
    (linearity_measure_data$StartLocation
        / linearity_measure_data$CharacterLength)
)
```

### Visualise outcome variable and predictors

The linearity_measure_data has `r nrow(linearity_measure_data)` observations.

We create graphs to inspect linearity of reading and its connection to other variables.
Graphs on the distribution of StartsNonlinearity and connection to random effects and control variables were created before the reader characteristics model, and so they are not repeated here.

### Connection to previous event

We visualise connection to previous events to see if initiating nonlinearity on the previous page, or on the page before the previous, is connected to initiating linearity on the current page-view.

Previous page-view:

```{r, include=FALSE}
ggplot(
    linearity_measure_data,
    aes(StartsNonlinearity, after_stat(count))
) +
    geom_bar(aes(fill = Observation_lag1), position = "dodge") +
    theme_classic()
```

Two pages before the current page-view:

```{r, include=FALSE}
ggplot(linearity_measure_data, aes(StartsNonlinearity, after_stat(count))) +
    geom_bar(aes(fill = Observation_lag2), position = "dodge") +
    theme_classic()
```

### Reading session

Reading session number:

```{r, include=FALSE}
ggplot(linearity_measure_data, aes(x = StartsNonlinearity, y = ReadingSessionNumber, fill = StartsNonlinearity)) +
    geom_violin() +
    geom_jitter(alpha = .1) +
    theme_classic()
```

Nonlinearity may be more common in early reading sessions.

Time in a reading session:

```{r, include=FALSE}
ggplot(linearity_measure_data, aes(
    x = StartsNonlinearity,
    y = FirstCumulativeRSTime,
    fill = StartsNonlinearity
)) +
    geom_violin() +
    geom_jitter(alpha = .1) +
    theme_classic() +
    labs(y = "Time in a reading session (min)")
```

Similarly, nonlinearity may be more frequent at the beginning of reading sessions.

### Location in text

```{r, include=FALSE}
ggplot(linearity_measure_data, aes(
    x = StartsNonlinearity,
    y = (PercentageLocation) * 100,
    fill = StartsNonlinearity
)) +
    geom_violin() +
    geom_jitter(alpha = .1) +
    theme_classic() +
    labs(y = "Location (%)")
```

Nonlinearity may also be connected to the beginning of the text.

### Set contrasts to predictors

Predictors in the model are given contrasts. Continuous variables are scaled and centered, whereas categorical variables are given helmert contrasts. Helmert contrasts are used to control for uneven levels in categorical variables.

Contrasts for categorical variables:

```{r 'categorical-contrasts-ep', echo=FALSE}
# previous event (lag of 1)
linearity_measure_data$Observation_lag1 <- as.factor(linearity_measure_data$Observation_lag1)
contrasts(linearity_measure_data$Observation_lag1) <- contr.helmert.unweighted(linearity_measure_data$Observation_lag1)
contrasts(linearity_measure_data$Observation_lag1)
# two events back (lag of 2)
linearity_measure_data$Observation_lag2 <- as.factor(linearity_measure_data$Observation_lag2)
contrasts(linearity_measure_data$Observation_lag2) <- contr.helmert.unweighted(linearity_measure_data$Observation_lag2)
contrasts(linearity_measure_data$Observation_lag2)
```

Next, we scale and center continuous variables. These variables names are appended with ".cs" as a reminder that the variable is scaled and centered. Note that ReadingSessionNumber is treated as a continuous variable because we do not expect to see categorical differences between reading sessions, on the basis of the visualisations.

```{r 'scale-continuous-variables-ep', echo=FALSE}
# Days until reading deadline (control variable)
linearity_measure_data$DaysUntilDeadline.sc <-
    scale(linearity_measure_data$FirstTimeUntilDeadlineDays,
        center = TRUE,
        scale = TRUE
    )
# Window Width
linearity_measure_data$WindowWidth.sc <-
    scale(linearity_measure_data$WindowWidth,
        center = TRUE,
        scale = TRUE
    )
# Reading session number
linearity_measure_data$ReadingSessionNumber.sc <-
    scale(linearity_measure_data$ReadingSessionNumber,
        center = TRUE,
        scale = TRUE
    )
# Time in a reading session
linearity_measure_data$TimeInReadingSession.sc <-
    scale(linearity_measure_data$FirstCumulativeRSTime,
        center = TRUE,
        scale = TRUE
    )
# Location in text
linearity_measure_data$PercentageLocation.sc <-
    scale(linearity_measure_data$PercentageLocation,
        center = TRUE,
        scale = TRUE
    )
```

### Modelling by Event Properties

#### Additive model

First, we build an additive model structure consisting of only main effects and random intercepts. Refer to the beginning of the event properties section of this script to see information on how the model is built.

The model includes 10 parameters before adding interactions, and `r nrow(linearity_measure_data)` observations. Previous research has suggested that each parameter in a model has at least 15 observations. Our sample exceeds this threshold considering that `r nrow(linearity_measure_data)`/10 = `r round(nrow(linearity_measure_data)/10, 2)`

```{r 'define-intercept-only-additive-structure'}
AdditiveStructure_EP <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 | StoryId / UserId)"
)
```

Fit the intercept-only additive model:

```{r 'fit-the-intercept-only-additive-model', warning='hide'}
Linearity_EP_AdditiveModel <- glmer(
    AdditiveStructure_EP,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

Next, we add random slopes to the additive model. Slopes are selected in a data-driven way to explore meaningful slopes to add to the model. We follow the 'best path' algorithm recommended by @barrRandomEffectsStructure2013a

In the best path algorithm, we test inclusion of each random slope against the intercept-only model. A liberal alpha-level is used, in accordance with @barrRandomEffectsStructure2013a recommendations, and so, any random slope which differs from the intercepts-only model with p < .2 is considered for inclusion. If multiple slopes cross this threshold, we include the slope with the lowest p-value in the model. Then, we re-test the remaining slopes in the model against this model. This is continued until none of the random slopes contribute to the model, and so all remaining slope p > .2.

```{r 'select-slopes1EP', eval=FALSE, echo=FALSE}
# slope test 1
    ## 1_1 WW
    ## 1_2 DUD
    ## 1_3 Event k-1
    ## 1_4 Event k-2
    ## 1_5 RSN
    ## 1_6 TRS
    ## 1_7 Loc
###################
# 1_1
AdditiveStructure_EP_nested_slope1_1 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + WindowWidth.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested1_1 <- glmer( #sing
    AdditiveStructure_EP_nested_slope1_1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel, Linearity_EP_AdditiveModel_nested1_1) # contributes, p = .119
# 1_2
AdditiveStructure_EP_nested_slope1_2 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + DaysUntilDeadline.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested1_2 <- glmer( # sing
    AdditiveStructure_EP_nested_slope1_2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested1_2, Linearity_EP_AdditiveModel) # does not contribute, p > .2
# 1_3
AdditiveStructure_EP_nested_slope1_3 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + Observation_lag1 | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested1_3 <- glmer( # sing
    AdditiveStructure_EP_nested_slope1_3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested1_3, Linearity_EP_AdditiveModel) # contributes, p = .0051
# 1_4
AdditiveStructure_EP_nested_slope1_4 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + Observation_lag2 | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested1_4 <- glmer( # sing
    AdditiveStructure_EP_nested_slope1_4,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested1_4, Linearity_EP_AdditiveModel) # contributes, p = .047
# 1_5
AdditiveStructure_EP_nested_slope1_5 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + ReadingSessionNumber.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested1_5 <- glmer( # sing
    AdditiveStructure_EP_nested_slope1_5,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested1_5, Linearity_EP_AdditiveModel) # contributes, p = .139
# 1_6
AdditiveStructure_EP_nested_slope1_6 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested1_6 <- glmer( # sing
    AdditiveStructure_EP_nested_slope1_6,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested1_6, Linearity_EP_AdditiveModel) # contributes, p < .0001
# 1_7
AdditiveStructure_EP_nested_slope1_7 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + PercentageLocation.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested1_7 <- glmer( # sing
    AdditiveStructure_EP_nested_slope1_7,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested1_7, Linearity_EP_AdditiveModel) # does not contribute, p > .20
```

A random slope of TimeInReadingSession contributes the most to the model. We continue testing slopes by comparing their addition to a model with TRS.

```{r 'select-slopes2EP', eval=FALSE, echo=FALSE}
# slope test 2
## Start adding all slopes to a model with TRS
    ## 2_1: WW
    ## 2_2: DUD
    ## 2_3: Event k-1
    ## 2_4: Event k-2
    ## 2_5: RSN
    ## 2_6: Loc
###################
AdditiveStructure_EP_nested_slope2 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested2 <- glmer( # sing
    AdditiveStructure_EP_nested_slope2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
# 2_1
AdditiveStructure_EP_nested_slope2_1 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested2_1 <- glmer( # sing
    AdditiveStructure_EP_nested_slope2_1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2_1) # contributes, p = .128
# 2_2
AdditiveStructure_EP_nested_slope2_2 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + DaysUntilDeadline.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested2_2 <- glmer( # sing
    AdditiveStructure_EP_nested_slope2_2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2_2) # does not contribute, p > .2
# 2_3
AdditiveStructure_EP_nested_slope2_3 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + Observation_lag1 | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested2_3 <- glmer( # sing
    AdditiveStructure_EP_nested_slope2_3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2_3) # does not contribute, p > .2
# 2_4
AdditiveStructure_EP_nested_slope2_4 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + Observation_lag2 | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested2_4 <- glmer( # sing
    AdditiveStructure_EP_nested_slope2_4,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2_4) # does not contribute, p > .2
# 2_5
AdditiveStructure_EP_nested_slope2_5 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested2_5 <- glmer( # sing
    AdditiveStructure_EP_nested_slope2_5,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2_5) # contributes, p = .087
# 2_6
AdditiveStructure_EP_nested_slope2_6 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + PercentageLocation.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested2_6 <- glmer( # sing
    AdditiveStructure_EP_nested_slope2_6,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2_6) # does not contribute, p > .20
```

A random slope of ReadingSessionNumber contributes most to the model. We continue testing possible random slopes by comparing to a model with a random slope for TimeInReadingSession and ReadingSessionNumber

```{r 'select-slopes3EP', eval=FALSE, echo=FALSE}
# slope test 3
## Start adding all slopes to a model with TRS + RSN
    ## 3_1: WW
    ## 3_2: DUD
    ## 3_3: Event k-1
    ## 3_4: Event k-2
    ## 3_5: Loc
###################
AdditiveStructure_EP_nested_slope3 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested3 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
# 3_1
AdditiveStructure_EP_nested_slope3_1 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc + WindowWidth.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested3_1 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3_1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3_1) # does not contribute, p > .2
# 3_2
AdditiveStructure_EP_nested_slope3_2 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc + DaysUntilDeadline.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested3_2 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3_2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3_2) # does not contribute, p > .2
# 3_3
AdditiveStructure_EP_nested_slope3_3 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc + Observation_lag1 | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested3_3 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3_3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3_3) # does not contribute, p > .2
# 3_4
AdditiveStructure_EP_nested_slope3_4 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc + Observation_lag2 | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested3_4 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3_4,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3_4) # does not contribute, p > .2
# 3_5
AdditiveStructure_EP_nested_slope3_5 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc + PercentageLocation.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested3_5 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3_5,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3_5) # does not contribute, p > .2
```

None of the remaining random slopes contributes to the model, and so the additive model has slopes for time in a reading session and reading session number only.

Inspect the model:

```{r 'additive-model-with-slopesEP', eval=FALSE}
AdditiveStructure_EP_nested_slope3 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested3 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
summary(Linearity_EP_AdditiveModel_nested3)
```

The additive model is singular, and so it is necessary to select it backward stepwise.
Similarly to the reader characteristics model, we first attempt to make the model non-singular by removing random slopes that least contribute to the model.

Inspection of the model shows that TimeInReadingSession contributes less to the model, and so it is removed first.

```{r 'backward-select-slopes', eval=FALSE, echo=FALSE}
# Model without slope for TRS
AdditiveStructure_EP_nested_slope_selection1 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + ReadingSessionNumber.sc | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested_selection1 <- glmer( # remains singular
    AdditiveStructure_EP_nested_slope_selection1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
summary(Linearity_EP_AdditiveModel_nested_selection1)
# Model without slope for reading session number
AdditiveStructure_EP_nested_slope_selection2 <- (
    "StartsNonlinearity ~ WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 | StoryId / UserId)"
)
Linearity_EP_AdditiveModel_nested_selection2 <- glmer( # remains singular
    AdditiveStructure_EP_nested_slope_selection2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The additive model remains singular after removal of all slopes in the model.
Similarly to the reader characteristics model, we attempted to remove fixed effects in the model without being able to resolve the singularity.

```{r, eval=FALSE}
summary(Linearity_EP_AdditiveModel_nested_selection2)
```

Again, inspection of model values shows that StoryId is accounting for zero variance in the model. We attempt to resolve this by fitting StoryId as a fixed effect in the model.

Add contrasts to StoryId:

```{r 'add-contrasts-to-storyidEP'}
linearity_measure_data$StoryId <- as.factor(linearity_measure_data$StoryId)
contrasts(linearity_measure_data$StoryId) <- contr.helmert.unweighted(linearity_measure_data$StoryId)
contrasts(linearity_measure_data$StoryId)
```

Re-fit the intercepts-only additive model with StoryId as a fixed effect:

```{r 'adjusted-additive-model', warning='hide'}
AdditiveStructure_Adjusted_EP <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 | UserId)"
)
Linearity_EP_AdditiveModel_Adjusted <- glmer(
    AdditiveStructure_Adjusted_EP,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

When the StoryId is fit as a fixed effect rather than a random effect, the model converges and is no longer singular. We inspect if this significantly inflates covariances in the model:

```{r, echo=FALSE}
car::vif(Linearity_EP_AdditiveModel_Adjusted)
```

All VIF values are < 2, indicating that including StoryId as a fixed effect does not significantly inflate covariances.

We then attempt to select random slopes a new in the model, this time only for the random effect of UserId. A random slope of StoryId is not tested considering that the number of groups in this variable would inflate the number of parameters in the model and result in overfitting.

```{r 'select-slopes-adjusted1EP', eval=FALSE, echo=FALSE}
# adjusted model slope test 1
    ## 1b_1 WW
    ## 1b_2 DUD
    ## 1b_3 Event k-1
    ## 1b_4 Event k-2
    ## 1b_5 RSN
    ## 1b_6 TRS
    ## 1b_7 Loc
###################
# 1b_1
AdditiveStructure_EP_nested_slope1b_1 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + WindowWidth.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested1b_1 <- glmer( #sing
    AdditiveStructure_EP_nested_slope1b_1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_Adjusted, Linearity_EP_AdditiveModel_nested1b_1) # contributes, p = .0076
# 1b_2
AdditiveStructure_EP_nested_slope1b_2 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + DaysUntilDeadline.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested1b_2 <- glmer( # sing
    AdditiveStructure_EP_nested_slope1b_2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_Adjusted, Linearity_EP_AdditiveModel_nested1b_2) # does not contribute, p > .2
# 1b_3
AdditiveStructure_EP_nested_slope1b_3 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_nested1b_3 <- glmer(
    AdditiveStructure_EP_nested_slope1b_3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_Adjusted, Linearity_EP_AdditiveModel_nested1b_3) # contributes, p = .00038
# 1b_4
AdditiveStructure_EP_nested_slope1b_4 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + Observation_lag2 | UserId)"
)
Linearity_EP_AdditiveModel_nested1b_4 <- glmer(
    AdditiveStructure_EP_nested_slope1b_4,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_Adjusted, Linearity_EP_AdditiveModel_nested1b_4) # contributes, p = .017
# 1b_5
AdditiveStructure_EP_nested_slope1b_5 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + ReadingSessionNumber.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested1b_5 <- glmer( # sing
    AdditiveStructure_EP_nested_slope1b_5,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_Adjusted, Linearity_EP_AdditiveModel_nested1b_5) # contributes, p = .074
# 1b_6
AdditiveStructure_EP_nested_slope1b_6 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested1b_6 <- glmer(
    AdditiveStructure_EP_nested_slope1b_6,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_Adjusted, Linearity_EP_AdditiveModel_nested1b_6) # contributes, p < .0001
# 1b_7
AdditiveStructure_EP_nested_slope1b_7 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + PercentageLocation.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested1b_7 <- glmer(
    AdditiveStructure_EP_nested_slope1b_7,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_Adjusted, Linearity_EP_AdditiveModel_nested1b_7) # contributes, p = .065
```

TimeInReadingSession contributes most to the model, and so it is added as a random slope. We test other options against a model with TRS

```{r 'select-slopes-adjusted2', eval=FALSE, echo=FALSE}
# adjusted model slope test 2
## Compare to model with a random slope of TRS
    ## 2b_1 WW
    ## 2b_2 DUD
    ## 2b_3 Event k-1
    ## 2b_4 Event k-2
    ## 2b_5 RSN
    ## 2b_6 Loc
###################
AdditiveStructure_EP_nested_slope2 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested2 <- glmer(
    AdditiveStructure_EP_nested_slope2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
# 2b_1
AdditiveStructure_EP_nested_slope2b_1 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested2b_1 <- glmer( #sing
    AdditiveStructure_EP_nested_slope2b_1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2b_1) # contributes, p = .017
# 2b_2
AdditiveStructure_EP_nested_slope2b_2 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + DaysUntilDeadline.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested2b_2 <- glmer( # sing
    AdditiveStructure_EP_nested_slope2b_2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2b_2) # does not contribute, p > .2
# 2b_3
AdditiveStructure_EP_nested_slope2b_3 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_nested2b_3 <- glmer(
    AdditiveStructure_EP_nested_slope2b_3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2b_3) # contributes, p = .047
# 2b_4
AdditiveStructure_EP_nested_slope2b_4 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + Observation_lag2 | UserId)"
)
Linearity_EP_AdditiveModel_nested2b_4 <- glmer(
    AdditiveStructure_EP_nested_slope2b_4,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2b_4) # does not contribute, p > .2
# 2b_5
AdditiveStructure_EP_nested_slope2b_5 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested2b_5 <- glmer( # sing
    AdditiveStructure_EP_nested_slope2b_5,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2b_5) # contributes, p = .018
# 2b_6
AdditiveStructure_EP_nested_slope2b_6 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + PercentageLocation.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested2b_6 <- glmer(
    AdditiveStructure_EP_nested_slope2b_6,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested2, Linearity_EP_AdditiveModel_nested2b_6) # contributes, p = .031
```

WindowWidth contributes the most, and so it is added as a random slope in the model. Next, we test the remaining variables as slopes against a model with TRS and WW

```{r 'select-slopes-adjusted3', eval=FALSE, echo=FALSE}
# adjusted model slope test 3
## Compare to model with a random slope of TRS and WW
    ## 3b_1 DUD
    ## 3b_2 Event k-1
    ## 3b_3 Event k-2
    ## 3b_4 RSN
    ## 3b_5 Loc
###################
AdditiveStructure_EP_nested_slope3 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested3 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
# 3b_1
AdditiveStructure_EP_nested_slope3b_1 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + DaysUntilDeadline.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested3b_1 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3b_1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3b_1) # does not contribute, p > .2
# 3b_2
AdditiveStructure_EP_nested_slope3b_2 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_nested3b_2 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3b_2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3b_2) # contributes, p = .114
# 3b_3
AdditiveStructure_EP_nested_slope3b_3 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + Observation_lag2 | UserId)"
)
Linearity_EP_AdditiveModel_nested3b_3 <- glmer(
    AdditiveStructure_EP_nested_slope3b_3, # sing
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3b_3) # does not contribute, p > .2
# 3b_4
AdditiveStructure_EP_nested_slope3b_4 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + ReadingSessionNumber.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested3b_4 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3b_4,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3b_4) # does not contribute, p > .2
# 3b_5
AdditiveStructure_EP_nested_slope3b_5 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested3b_5 <- glmer( # sing
    AdditiveStructure_EP_nested_slope3b_5,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested3, Linearity_EP_AdditiveModel_nested3b_5) # contributes, p = .05
```

PercentageLocation contributes the most, and so it is added as a random slope in the model. Next, we test the remaining variables as slopes against a model with TRS, WW and Loc

```{r 'select-slopes-adjusted4', eval=FALSE, echo=FALSE}
# adjusted model slope test 4
## Compare to model with a random slope of TRS, WW and Loc
    ## 4b_1 DUD
    ## 4b_2 Event k-1
    ## 4b_3 Event k-2
    ## 4b_4 RSN
###################
AdditiveStructure_EP_nested_slope4 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation | UserId)"
)
Linearity_EP_AdditiveModel_nested4 <- glmer( # sing
    AdditiveStructure_EP_nested_slope4,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
# 4b_1
AdditiveStructure_EP_nested_slope4b_1 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation + DaysUntilDeadline.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested4b_1 <- glmer( #sing
    AdditiveStructure_EP_nested_slope4b_1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested4, Linearity_EP_AdditiveModel_nested4b_1) # does not contribute, p > .2
# 4b_2
AdditiveStructure_EP_nested_slope4b_2 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_nested4b_2 <- glmer( # sing
    AdditiveStructure_EP_nested_slope4b_2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested4, Linearity_EP_AdditiveModel_nested4b_2) # contributes, p = .169
# 4b_3
AdditiveStructure_EP_nested_slope4b_3 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + Observation_lag2 | UserId)"
)
Linearity_EP_AdditiveModel_nested4b_3 <- glmer( # sing
    AdditiveStructure_EP_nested_slope4b_3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested4, Linearity_EP_AdditiveModel_nested4b_3) # does not contribute, p > .2
# 4b_4
AdditiveStructure_EP_nested_slope4b_4 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + ReadingSessionNumber.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested4b_4 <- glmer(
    AdditiveStructure_EP_nested_slope4b_4, # sing
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested4, Linearity_EP_AdditiveModel_nested4b_4) # contributes, p = .069
```

ReadingSessionNumber contributes the most, and so it is added as a random slope in the model. Next, we test the remaining variables as slopes against a model with TRS, WW, Loc, and RSN

```{r 'select-slopes-adjusted5', eval=FALSE, echo=FALSE}
# adjusted model slope test 5
## Compare to model with a random slope of TRS, WW, Loc, and RSN
    ## 5b_1 DUD
    ## 5b_2 Event k-1
    ## 5b_3 Event k-2
###################
AdditiveStructure_EP_nested_slope5 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + ReadingSessionNumber.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested5 <- glmer( # sing
    AdditiveStructure_EP_nested_slope5,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
# 5b_1
AdditiveStructure_EP_nested_slope5b_1 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1  + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + ReadingSessionNumber.sc + DaysUntilDeadline.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested5b_1 <- glmer( #sing, nc
    AdditiveStructure_EP_nested_slope5b_1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested5, Linearity_EP_AdditiveModel_nested5b_1) # does not contribute, p > .2
# 5b_2
AdditiveStructure_EP_nested_slope5b_2 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + ReadingSessionNumber.sc + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_nested5b_2 <- glmer( # sing, nc
    AdditiveStructure_EP_nested_slope5b_2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested5, Linearity_EP_AdditiveModel_nested5b_2) # contributes, p = .194
# 5b_3
AdditiveStructure_EP_nested_slope5b_3 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + ReadingSessionNumber.sc + Observation_lag2 | UserId)"
)
Linearity_EP_AdditiveModel_nested5b_3 <- glmer( # sing, nc
    AdditiveStructure_EP_nested_slope5b_3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested5, Linearity_EP_AdditiveModel_nested5b_3) # does not contribute, p > 2
```

Observation_lag1 contributes to the model, and so it is added as a random slope. The model includes slopes for TRS, WW, Loc, RSN and Event~k-1~. We then resume slope testing.

```{r, eval=FALSE, echo=FALSE}
# adjusted model slope test 6
## Compare to model with a random slope of TRS, WW, Loc, RSN, and OBS-1
    ## 6b_1 DUD
    ## 6b_2 Event k-2
###################
AdditiveStructure_EP_nested_slope6 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + ReadingSessionNumber.sc + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_nested6 <- glmer( # sing
    AdditiveStructure_EP_nested_slope6,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
# 6b_1
AdditiveStructure_EP_nested_slope6b_1 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + ReadingSessionNumber.sc + Observation_lag1 + DaysUntilDeadline.sc | UserId)"
)
Linearity_EP_AdditiveModel_nested6b_1 <- glmer( #sing, nc
    AdditiveStructure_EP_nested_slope6b_1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_AdditiveModel_nested6, Linearity_EP_AdditiveModel_nested6b_1) # does not contribute, p > .2
# 5b_2
AdditiveStructure_EP_nested_slope6b_2 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + ReadingSessionNumber.sc + Observation_lag1 + Observation_lag2 | UserId)"
)
Linearity_EP_AdditiveModel_nested6b_2 <- glmer( #unidentifiable
    AdditiveStructure_EP_nested_slope6b_2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

None of the remaining variables contribute to the model as random effects.
We then inspect the adjusted additive model with random slopes:

```{r}
AdditiveStructure_EP_nested_slope6 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + PercentageLocation.sc + ReadingSessionNumber.sc + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_nested6 <- glmer( # sing
    AdditiveStructure_EP_nested_slope6,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
summary(Linearity_EP_AdditiveModel_nested6)
```

The model is nonconvergent and singular. To address this, we simplify the model backward-stepwise, starting with the random slopes.
The model summary shows that the random slope of PercentageLocation accounts for the least variance in the model, and so it is removed first. 

```{r 'adjusted-model-backward-select-slopes', eval=FALSE, echo=FALSE}
# fit the model without PercentageLocation
AdditiveStructure_EP_slope_selected1 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + WindowWidth.sc + ReadingSessionNumber.sc + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_slope_selected1 <- glmer( # remains singular
    AdditiveStructure_EP_slope_selected1,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
summary(Linearity_EP_AdditiveModel_slope_selected1) # WindowWidth accounts for the least variance
# fit the model without WindowWidth
AdditiveStructure_EP_Adjusted_slope_selected2 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + ReadingSessionNumber.sc + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_slope_selected2 <- glmer( # remains singular
    AdditiveStructure_EP_Adjusted_slope_selected2,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
summary(Linearity_EP_AdditiveModel_slope_selected2) # Reading session number accounts for the least variance
# fit the model without ReadingSessionNumber
AdditiveStructure_EP_Adjusted_slope_selected3 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_slope_selected3 <- glmer(
    AdditiveStructure_EP_Adjusted_slope_selected3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

After removal of three random slopes, PercentageLocation, WindowWidth and ReadingSessionNumber, the model converges and is no longer singular.
Inspect the model:

```{r, warning='hide', echo=FALSE}
AdditiveStructure_EP_Adjusted_slope_selected3 <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + (1 + TimeInReadingSession.sc + Observation_lag1 | UserId)"
)
Linearity_EP_AdditiveModel_slope_selected3 <- glmer(
    AdditiveStructure_EP_Adjusted_slope_selected3,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
summary(Linearity_EP_AdditiveModel_slope_selected3)
plot(Linearity_EP_AdditiveModel_slope_selected3)
fixef(Linearity_EP_AdditiveModel_slope_selected3)
```

Now that the model converges, we can add two-way interactions.

#### Interactive model

We create a new model structure with interactions between all predictors of interest (see information at the beginning of the event properties section for an explanation of model selection). Only two-way interactions are included to avoid overfitting the model. Note that StoryId is fit as a fixed rather than a random effect due to singularity in the additive model structure. The interactive model includes XX parameters. Previous research has suggested that each parameter in a model has at least 15 observations. Our sample exceeds this threshold considering that `r nrow(linearity_measure_data)`/21 = `r round(nrow(linearity_measure_data)/21, 2)`.

```{r 'define-interactive-structure'}
InteractiveStructure_EP <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + Observation_lag1 : Observation_lag2 + ReadingSessionNumber.sc : TimeInReadingSession.sc + ReadingSessionNumber.sc : PercentageLocation.sc + TimeInReadingSession.sc : PercentageLocation.sc + (1 + TimeInReadingSession.sc + Observation_lag1 | UserId)"
)
```

```{r 'fit-interactive-model', warning='hide'}
Linearity_EP_InteractiveModel <- glmer(
    InteractiveStructure_EP,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

The full interactive model structure converges and is not singular.
Inspect model values:

```{r, include=FALSE}
summary(Linearity_EP_InteractiveModel)
plot(Linearity_EP_InteractiveModel)
fixef(Linearity_EP_InteractiveModel)
```

### Model Assumptions

All generalised linear mixed models were tested for (1) multicollinearity, (2) influential observations, (3) heteroscedasticity, and (4) normality of random effects.

Under- and overdispersion was not tested considering that the observations cannot be effectively grouped by predictors (see https://stats.stackexchange.com/a/200848).

#### 1. Multicollinearity

Multicollinearity is checked by first inspecting a correlation matrix, and then computing Variance Inflation Factors (VIF) for all predictors.
Any VIF > 5 is considered to be a strong indication of multicollinearity, and thus those predictors should be removed. Any VIFs that are in the moderate range 1 < VIF < 5 are inspected before removing them. If removing the variable with a moderate VIF influences the model interpretation of predictors (not including the one that is removed), then the predictor should be removed from the model. 

We check for multicollinearity first with a correlation matrix:

```{r, eval=FALSE, include=FALSE}
corrplot(cor(linearity_measure_data[, c("WindowWidth.sc", "DaysUntilDeadline.sc", "ReadingSessionNumber.sc", "TimeInReadingSession.sc", "PercentageLocation.sc")]), method = "number")
```

Location in text and time in a reading session are positively correlated, *r* = `r round(cor(linearity_measure_data$PercentageLocation.sc, linearity_measure_data$TimeInReadingSession.sc), 2)`.

To check if these correlations are problematic, we calculate VIF:

```{r, eval=FALSE, include=FALSE}
check_collinearity(Linearity_EP_InteractiveModel)
plot(check_collinearity(Linearity_EP_InteractiveModel))
```

An interaction effect between the previous events (Event k-1 and k-2) have a moderately high VIF score. We further assess whether the multicollinearity influences results by re-fitting the model without it.

```{r}
# Specify model without an interaction between previous events
InteractiveStructure_EP_adjusted <- (
    "StartsNonlinearity ~ StoryId + WindowWidth.sc + DaysUntilDeadline.sc + Observation_lag1 + Observation_lag2 + ReadingSessionNumber.sc + TimeInReadingSession.sc + PercentageLocation.sc + ReadingSessionNumber.sc : TimeInReadingSession.sc + ReadingSessionNumber.sc : PercentageLocation.sc + TimeInReadingSession.sc : PercentageLocation.sc + (1 + TimeInReadingSession.sc + Observation_lag1 | UserId)"
)
# Re-fit the model
Linearity_EP_InteractiveModel_adjusted <- glmer(
    InteractiveStructure_EP_adjusted,
    data = linearity_measure_data,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
anova(Linearity_EP_InteractiveModel_adjusted, Linearity_EP_InteractiveModel)
summary(Linearity_EP_InteractiveModel)
summary(Linearity_EP_InteractiveModel_adjusted)
```

Inspection of the model without the interaction shows that the adjusted model is marginally different from the originally selected interactive model. Furthermore, summary of the model result shows that removal of the interaction has an impact on result interpretation. We assess whether the adjusted model adequately addresses the multicollinearity:

```{r}
check_collinearity(Linearity_EP_InteractiveModel_adjusted)
plot(check_collinearity(Linearity_EP_InteractiveModel_adjusted))
```

After removal of the interaction, the model multicollinearity reduces to acceptable levels. Accordingly, we decide to proceed with the adjusted model.

#### 2. Influential Observations

We test influential observations with Cook's Distance:

```{r}
cooksd <-
     cooks.distance(Linearity_EP_InteractiveModel_adjusted)
```

```{r, eval=FALSE, include=FALSE}
plot(cooksd,
    pch = "*",
     cex = 2,
    main = "Influential Obs by Cooks Distance"
)
abline(h = 4 * mean(cooksd, na.rm = TRUE), col = "red")
text(
    x = 1:length(cooksd) + 1,
    y = cooksd,
     labels = ifelse(cooksd > 4 * mean(cooksd, na.rm = TRUE), names(cooksd), ""),
    col = "red"
)
```

The plot indicates that the dataset may include some highly influential observations. In particular, the plot highlights observations #9 and #24.

We further inspect outliers using the simulated_model_output and DHARMa's outlier test:

```{r, eval=FALSE, include=FALSE}
set.seed(1234) # for reproducibility
simulated_model_output <- DHARMa::simulateResiduals(Linearity_EP_InteractiveModel_adjusted)
testOutliers(simulated_model_output)
```

Although Cook's Distance indicates that the model has influential observations, the outlier test does not produce the same results. We resolve this contradiction by testing the removal of the most influential observation (#9) from the model to see if its inclusion has an impact on model interpretations.

**Fit the model without observation #9:**

```{r, eval=FALSE}
subset_without_observation9 <- linearity_measure_data[c(1:8, 10:nrow(linearity_measure_data)), ]
```

```{r 'fit-model-wo-obs-3462', eval=FALSE}
Linearity_EP_InteractiveModel_adjusted_influential <- glmer(
    InteractiveStructure_EP_adjusted,
    data = subset_without_observation9,
    family = binomial(),
    control = glmerControl(optimizer = "bobyqa")
)
```

Inspect if model results are affected:

```{r, eval=FALSE}
summary(Linearity_EP_InteractiveModel_adjusted_influential)
summary(Linearity_EP_InteractiveModel_adjusted)
```

The results are not influenced by removing observation #9, and so we retain it in the sample. Considering that the most influential observation does not appear to affect model results, we believe that the sample does not have any influential observations, in accordance with results from DHARMa::testOutliers().

#### 3. Heteroscedasticity

Test for heteroscedasticity with DHARMa::testQuantiles() using simulated_model_output created earlier

```{r, eval=FALSE, include=FALSE}
testQuantiles(simulated_model_output)
```

The p-value is not significant, and the lines lie fairly flat and horizontal in the plot, close to the reference lines. This indicates that the assumption of homoscedasticity is fulfilled.

#### 4. Normality of Random Effects

Random effects should be normally distributed in multilevel models. We test this assumption by inspecting the normality of 'UserId'. 'StoryId' was fit in the model as a fixed effect and so it is not inspected.

```{r, eval=FALSE, include=FALSE}
random_intercept_UserId <- ranef(Linearity_EP_InteractiveModel_adjusted)$UserId$`(Intercept)`
qqnorm(random_intercept_UserId)
qqline(random_intercept_UserId)
shapiro.test(random_intercept_UserId)
```

The qqplot and the Shapiro Wilk test show that the random effect of UserId significantly deviates from a normal distribution. The plot shows that the random effect variances have long tails, and thus higher and lower values are skewed.
This nonnormality is likely to bias variance estimations from the random effect of UserId. However, it does not influence interpretation of the fixed effect coefficients [@Shielzeth2020]. Considering that our main aim is to interpret the results from the model rather than estimate UserId variance, we accept this limitation and interpret results with caution.

**Assumptions conclusion**

The model was fit with StoryId as a fixed rather than a random effect to resolve singularity in the model that was due to zero variance accounted for by the StoryId random intercept. The model also showed an elevated VIF score for a two-way interaction between previous events (Event~k-1~ and Event~k-2~), indicative of multicollinearity. This was resolved by re-fitting the model without this interaction after inspection of the model showed that the potential multicollinearity may have an impact on the model results.

Finally, inspection of the normality of the random effect of UserId showed that the distribution significantly deviates from normal. It is not possible to exclude or transform the variable without significantly altering the model. Instead, we accept this limitation to the model considering that it does not bias interpretation of fixed effect coefficients, according to a simulation study by @Shielzeth2020.

### Interpret reader characteristics model

We then interpret model effects to interpret the results.

```{r, include=FALSE}
summary(Linearity_EP_InteractiveModel_adjusted)
```

Linearity of reading is significantly predicted by
1. A main effect of previous event (navigation event k-1, 'Observation_lag1')
2. A main effect of time since the beginning of a reading session ('TimeInReadingSession')
3. An interaction effect between time in a reading session and location in text

These findings are explored below by visualisations.
First, we discuss the effect of previous event (1). Second, we discuss the influence of reading session and location (2 and 3)

Note that no hypotheses were set for the event properties models and they are used for explorative purposes only.

#### Connection between the outcome variable and previous events

The results of the model indicated that previous event (lag 1) was a significant predictor of nonlinear navigation.

```{r, warning='hide'}
linearity_ep_previousevent <- effect_plot(
    Linearity_EP_InteractiveModel_adjusted,
    pred = "Observation_lag1",
    data =  linearity_measure_data,
    interval = TRUE
) +
    figure_theme_without_legend +
    labs(
        y = addline_format("Odds of initiating,nonlinear navigation (Event k)"),
        x = addline_format("Whether nonlinearity was,initiated in Event k-1")
    ) +
    coord_cartesian(ylim = c(0, 0.3), expand = FALSE) +
    scale_y_continuous(breaks = c(0, 0.1, 0.2, 0.3))
```
 
The result indicates that participants were more likely to initiate nonlinear navigation if the previous event did not initiate nonlinearity. In other words, participants were unlikely to move between different types of nonlinear navigation (e.g. from a regression to a forward leap) in consecutive page-views.

In contrast, event~k-2~ was not a significant predictor of nonlinearity:

```{r, warning='hide'}
linearity_ep_lag2 <- effect_plot(
    Linearity_EP_InteractiveModel_adjusted,
    pred = "Observation_lag2",
    interval = TRUE,
) +
    figure_theme_without_legend +
    labs(
        y = addline_format("Odds of initiating,nonlinear navigation (Event k)"),
        x = addline_format("Whether nonlinear navigation was,initiated in Event k-2")
    ) +
    coord_cartesian(ylim = c(0, 0.3), expand = FALSE) +
    scale_y_continuous(breaks = c(0, 0.1, 0.2, 0.3))
```

Therefore, nonlinear navigation could not be predicted on the basis of the event preceeding the previous.

#### Effect of Time in a Reading session and Location in Text

Linearity of reading was significant predicted by the time since the beginning of a reading session.

```{r, warning='hide'}
linearity_ep_time <- effect_plot(
    Linearity_EP_InteractiveModel_adjusted,
    pred = "TimeInReadingSession.sc",
    interval = TRUE
) +
    figure_theme_without_legend +
    labs(
        y = addline_format("Odds of initiating nonlinear navigation"),
        x = addline_format("Time since the beginning of the reading session,(centered and scaled)")
    )
```

The main effect indicates that the odds of initiating nonlinear navigation were highest at the beginning of a reading session, and the likelihood of nonlinearity decreased further into the reading session. This indicates that participants may have used nonlinearity primarily at the beginning of reading sessions, potentially to assess the story structure or look-back on what they had read previously.

The main effect was qualified by a significant interaction between time in a reading session and location in text.

```{r, warning='hide'}
linearity_ep_timelocation <- interact_plot(
    Linearity_EP_InteractiveModel_adjusted,
    pred = "TimeInReadingSession.sc",
    modx = "PercentageLocation.sc",
    legend.main = addline_format("Location in text,(centered and scaled)"),
    interval = TRUE
) +
    figure_theme_with_top_legend +
    labs(
        y = addline_format("Odds of initiating nonlinear navigation"),
        x = addline_format("Time since the beginning of the reading session,(centered and scaled)")
    )
```

The interaction effect indicates that the odds of nonlinear navigation are the highest if the participant is at the beginning of the story at the beginning of a reading session. The odds of nonlinear navigation decrease towards the end of the reading session.